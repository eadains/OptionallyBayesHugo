[{"content":"One of the problems with insurance data is that you only get one go at observing losses for a given policy in a given year. You can debate this epistemically, but in my mind the \u0026ldquo;true\u0026rdquo; claim frequency or claim severity for a policy in a given year is a latent, unobserved quantity. The only thing you actually observe as an insurer is the policy characteristics, total claim count, and total claim amount. Inferring the true underlying frequency or severity is then a statistical exercise. A useful question to ask is, given the claims we saw, what is a reasonable range of claims we could have seen? This is the question I\u0026rsquo;ll provide at least an introduction to here.\nThe Inference Model import arviz as az import lightgbm as lgb import matplotlib.pyplot as plt import numpy as np import polars as pl import pymc as pm from scipy import stats from sklearn.compose import make_column_transformer from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder I\u0026rsquo;ll be using a freely available French auto liability dataset for this. There\u0026rsquo;s some preprocessing necessary to get the data together:\ndf_freq = pl.read_csv(\u0026#34;./data/insurance/freMTPL2freq.csv\u0026#34;) df_sev = pl.read_csv(\u0026#34;./data/insurance/freMTPL2sev.csv\u0026#34;, infer_schema_length=None).group_by(\u0026#34;IDpol\u0026#34;).sum() df = df_freq.join(df_sev, on=\u0026#34;IDpol\u0026#34;, how=\u0026#34;left\u0026#34;, coalesce=True).with_columns(pl.col(\u0026#34;ClaimAmount\u0026#34;).fill_null(0)) df_sample = df.sample(5000, seed=42) Now, to infer latent quantities we have to assume a probabilistic model for what we do actually observe. I think this model is about as simple as you can get. The general idea is to first model claim count and claim severity separately. For claim counts we assume they follow a Poisson distribution with a mean equal to the policy exposure times the claim frequency (claims per exposure). We assume claim severity (loss dollars per claim) follows an Exponential distribution. You could easily assume other functional forms here, like log-normal, but for a strictly positive variable with a specified mean, the Exponential distribution is the maximum entropy distribution.\nThe critical piece here is that we estimate the frequency and severity parameters individually for each policy, where we have a hyperprior for both to reduce the total effective number of parameters. This is the notation for the model: $$ \\begin{align*} C_i \u0026amp;\\sim \\text{Poisson}(\\mu=\\lambda_i \\text{E}_i) \\newline S_i \u0026amp;\\sim \\text{Exponential}\\left(\\lambda=\\frac{1}{\\mu_i C_i} \\right) \\newline \\lambda_i \u0026amp;\\sim \\text{Exponential}(\\lambda=\\phi) \\newline \\mu_i \u0026amp;\\sim \\text{Exponential}({\\lambda=\\omega}) \\newline \\phi \u0026amp;\\sim \\text{Exponential}(\\lambda=1) \\newline \\omega \u0026amp;\\sim \\text{Exponential}({\\lambda=1}) \\end{align*} $$\nWhere $i$ indexes each policy, $C$ and $S$ represent the observed claim count and total loss amount, respectively, and $\\text{E}$ represents the policy exposure. By multiplying $\\lambda$ and $\\text{E}$, we assume that the total number of claims is the policy exposure times the claim frequency, defined as the number of claims per unit of exposure, so we can interpret $\\lambda$ as the policy\u0026rsquo;s estimated claim frequency. We set the mean of the Exponential distribution for $S$ to be the observed claim amount $C$ times $\\mu$, so we can similarly interpret $\\mu$ as the policy\u0026rsquo;s estimated claim severity, the amount of loss dollars incurred per claim.\nWe can express this in pymc as follows. Note that we only apply the likelihood for the loss amount when it is above 0, because a total loss amount of 0 doesn\u0026rsquo;t tell us anything about the severity for the policy, as that happens when we have 0 claims. So, a 0 claim count is informative for our estimate of the claim frequency, but gives us no information about that policy\u0026rsquo;s claim severity, since we didn\u0026rsquo;t actually observe any losses. This becomes a key aspect of how this model works: for claims where we haven\u0026rsquo;t observed any losses we can still take a guess at its claim severity because of the hyperprior.\nN = len(df_sample) E = df_sample[\u0026#34;Exposure\u0026#34;].to_numpy() C = df_sample[\u0026#34;ClaimNb\u0026#34;].to_numpy() S = df_sample[\u0026#34;ClaimAmount\u0026#34;].to_numpy() with pm.Model() as model: phi = pm.Exponential(\u0026#34;phi\u0026#34;, lam=1.0) omega = pm.Exponential(\u0026#34;omega\u0026#34;, lam=1.0) lam = pm.Exponential(\u0026#34;lambda\u0026#34;, lam=phi, shape=N) mu = pm.Exponential(\u0026#34;mu\u0026#34;, lam=omega, shape=N) c_obs = pm.Poisson(\u0026#34;C\u0026#34;, mu=E * lam, observed=C) mask = S \u0026gt; 0 severity_rate = 1.0 / (mu[mask] * C[mask]) s_obs = pm.Exponential(\u0026#34;S\u0026#34;, lam=severity_rate, observed=S[mask]) trace = pm.sample(draws=1000, tune=1000, chains=4) trace.extend(pm.sample_posterior_predictive(trace)) trace.extend(pm.compute_log_likelihood(trace)) We can then look at some model diagnostics to see if our MCMC converged well. First, we can look at the chain samples and posterior distribution for our hyperprior variables:\naz.plot_trace(trace, var_names=[\u0026#34;phi\u0026#34;, \u0026#34;omega\u0026#34;]) $\\omega$ is not sampling as well as $\\phi$ but it looks okay.\nNext we can then visualize the energy transition distribution to see if it looks okay:\naz.plot_energy(trace) And the two distributions match, meaning we expect it to have adequately explored the posterior space.\nNow, let\u0026rsquo;s look at some diagnostics related to the posterior predictive distributions of $C$ and $S$. First, we expect, a priori, higher exposure policies to contain more information. This is because of a natural assumption about insurance policies that they can be \u0026ldquo;decomposed\u0026rdquo; in a certain sense. Here, our exposure is car-years, meaning that if a policy has an exposure of 2, that could be 2 cars for 1 year, or 1 car for 2 years, or some combination. So, one can think of a policy with exposure 2 as being equivalent, from a risk standpoint, as 2 policies both with exposure 1. Holding policy characteristics constant, and glossing over details of deductibles and whatnot, there\u0026rsquo;s no difference between a single policy covering 2 vehicles and 2 separate policies each covering one of them, the expected losses are the same. You can think through the same logic for home insurance and the like.\nSo, if we think of exposure in this way, it means that policies can be thought of as sums of random variables where each random variable represents a risk with unit exposure. Focusing on claim counts, imagine each of these unit exposure random variables follows a Poisson distribution with mean $\\mu$. Then, the total expected claim count for a policy with exposure $N$ is simply $\\mu * N$, and, more importantly, the variance of the mean is $\\frac{\\mu}{\\sqrt{N}}$ (from the fact that the variance and mean of a Poisson distribution are equal). Therefore, as $N$, total exposure, increases, we expect the variance of our estimate of the expected total claim count to go down. With some mathematical rearrangement you can see that this same idea applies to an estimate of the claim frequency rather than just total number of claims. Regardless, we can just see if this is true from our posterior samples:\nstats.pearsonr(df_sample[\u0026#34;Exposure\u0026#34;], az.extract(trace, var_names=[\u0026#34;lambda\u0026#34;]).var(axis=1)) PearsonRResult(statistic=np.float64(-0.2568976199074987), pvalue=np.float64(3.488038665730237e-76)) Here we are measuring the degree of correlation between the exposure values and the variance of the samples for the claim frequency for each policy. The negative correlation we observe is exactly as we\u0026rsquo;d expect: larger exposure policies have a lower variance in their estimated claim frequency.\nNext, we can compare the total number of claims in the dataset to our posterior distribution of the same. We expect the observed number of claims to fall in some high probability region of our posterior, which tells us that what we observe matches up with what the model is telling us:\nplt.hist(az.extract(trace.posterior_predictive, var_names=[\u0026#34;C\u0026#34;]).sum(axis=0), bins=25, color=\u0026#34;grey\u0026#34;) plt.axvline(df_sample[\u0026#34;ClaimNb\u0026#34;].sum(), color=\u0026#34;red\u0026#34;) plt.xlabel(\u0026#34;Total Number of Claims\u0026#34;) plt.ylabel(\u0026#34;Density\u0026#34;) plt.title(\u0026#34;Histogram of Sampled Claim Counts\u0026#34;) plt.legend([\u0026#34;Observed Total\u0026#34;, \u0026#34;Sampled Data\u0026#34;]) We can see that things look quite reasonable.\nWe can do the same analysis for the total loss:\nplt.hist(az.extract(trace.posterior_predictive, var_names=[\u0026#34;S\u0026#34;]).sum(axis=0), bins=25, color=\u0026#34;grey\u0026#34;) plt.axvline(df_sample[\u0026#34;ClaimAmount\u0026#34;].sum(), color=\u0026#34;red\u0026#34;) plt.xlabel(\u0026#34;Total Loss Amount\u0026#34;) plt.ylabel(\u0026#34;Density\u0026#34;) plt.title(\u0026#34;Histogram of Sampled Loss Amounts\u0026#34;) plt.legend([\u0026#34;Observed Total\u0026#34;, \u0026#34;Sampled Data\u0026#34;]) Our observed total loss amount lies a little to the right of the mode of our posterior distribution, but still lies in a reasonable area.\nNext, let\u0026rsquo;s compare the posterior claim frequency of a policy where we actually have observed claims to one where we observe zero claims:\n# Comparing the posterior distributions of policies with high/low observed frequency lam_sample = az.extract(trace, var_names=\u0026#34;lambda\u0026#34;) high_claim = lam_sample[df_sample[\u0026#34;ClaimNb\u0026#34;].to_numpy().argmax()] low_claim = lam_sample[df_sample[\u0026#34;ClaimNb\u0026#34;].to_numpy().argmin()] plt.hist(high_claim, bins=25, alpha=0.5, color=\u0026#34;black\u0026#34;) plt.hist(low_claim, bins=25, alpha=0.5, color=\u0026#34;red\u0026#34;) plt.axvline(high_claim.mean(), color=\u0026#34;black\u0026#34;) plt.axvline(low_claim.mean(), color=\u0026#34;red\u0026#34;) plt.xlabel(\u0026#34;Sampled Frequency\u0026#34;) plt.ylabel(\u0026#34;Density\u0026#34;) plt.legend([\u0026#34;High Obs Claims\u0026#34;, \u0026#34;Low Obs Claims\u0026#34;]) You can see that, as we\u0026rsquo;d expect, when we have actually observed claims for a policy, our estimate of its claim frequency goes up.\nFrom these basic diagnostics, we can see that our model reflects the stylized facts of insurance data that we expect to see. Note that this is a very simple model, you can increase the complexity from here. For example, this model assumes that our estimate of the true latent frequency and severity depends only upon our observed values for each, and no other policy characteristics. You can easily see how other specific information about the claim would affect your guess of its true frequency or severity.\nPredictive Model and Lift Charts I want to demonstrate one potentially useful application of a model like this, which is putting error bounds on lift charts. Typically, in insurance modeling applications, you create a lift chart that compares your modeled predictions of total claims or losses to the actually observed quantities. However, as we said at the beginning, the observed quantities only represent a single observation per policy. By using the posterior we computed above, we can now compute an expected range of observed claims and losses, so we can add some error bars to our lift chart, which lets us see probabilistically how our predictions stack up.\nModel predictions and actual observations will never perfectly match up on a lift chart, so a natural question to ask is: how far away can my predicted quantity be from the observed before it\u0026rsquo;s unreasonable? And that question is what the lift chart I develop below is able to give some clue to.\nFirst, we develop a very basic lightgbm model to predict claim counts given the policy-specific characteristics in the dataset:\ntransformer = make_column_transformer( ( OrdinalEncoder(handle_unknown=\u0026#34;use_encoded_value\u0026#34;, unknown_value=np.nan), [\u0026#34;Area\u0026#34;, \u0026#34;VehBrand\u0026#34;, \u0026#34;VehGas\u0026#34;, \u0026#34;Region\u0026#34;], ), (\u0026#34;passthrough\u0026#34;, [\u0026#34;Exposure\u0026#34;, \u0026#34;VehPower\u0026#34;, \u0026#34;VehAge\u0026#34;, \u0026#34;DrivAge\u0026#34;, \u0026#34;BonusMalus\u0026#34;, \u0026#34;Density\u0026#34;]), remainder=\u0026#34;drop\u0026#34;, ) X = transformer.fit_transform(df) y = df[\u0026#34;ClaimNb\u0026#34;].to_numpy() train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.20) claims_model = lgb.LGBMRegressor( learning_rate=0.1, n_estimators=10000, objective=\u0026#34;poisson\u0026#34;, subsample=0.50, subsample_freq=1, colsample_bytree=0.5, max_depth=6, num_leaves=2**6 - 1, extra_trees=True, ) claims_model.fit( train_X, train_y, eval_set=(val_X, val_y), callbacks=[lgb.early_stopping(stopping_rounds=50)], categorical_feature=[0, 1, 2, 3], ) We can look at some SHAP scatter charts to get a sense of how our input variables affect our predictions:\nimport shap explainer = shap.TreeExplainer(claims_model.booster_, feature_names=transformer.get_feature_names_out()) shap_values = explainer(val_X[: int(len(val_X) * 0.10)]) shap.plots.scatter( shap_values[:, [\u0026#34;passthrough__Exposure\u0026#34;, \u0026#34;passthrough__DrivAge\u0026#34;, \u0026#34;passthrough__BonusMalus\u0026#34;]], shap_values ) Then, we can assemble our values for the lift chart. First we compute our quantile bin cutoffs, and then for each bin we compute the total predicted claim count plus some estimated quantiles of observed claim counts using the posterior distribution from the inference model:\n(Note that the searchsorted trick here is quite nice. The list of quantiles is sorted by definition, and searchsorted returns the index of the quantiles array for each value of the predictions array that value would have to be placed to maintain its order. So, for instance if quantiles is [1, 2], then any prediction \u0026lt;= 1 would be assigned index 0, anything \u0026gt;1 but \u0026lt;= 2 would be assigned index 1, and anything greater than 2 would be assigned index 2. So, to find all values in the predictions array that are in the first quantile bin, i.e. \u0026lt;1, we just have to find which elements of the array returned by searchsorted are equal to 0, as so on for the other bins.)\npredictions = claims_model.predict(transformer.transform(df_sample)) qs = np.linspace(1 / 5, 1, 4, endpoint=False) quantiles = np.quantile(predictions, qs) sampled_freqs = az.extract(trace, group=\u0026#34;posterior\u0026#34;, var_names=\u0026#34;lambda\u0026#34;) exposures = df_sample[\u0026#34;Exposure\u0026#34;].to_numpy() actual_claims = df_sample[\u0026#34;ClaimNb\u0026#34;].to_numpy() results = {} for idx in range(len(quantiles) + 1): # This searchsorted trick is quite clever: subset_idx = np.searchsorted(quantiles, predictions) == idx claims_subset = actual_claims[subset_idx] freqs_subset = sampled_freqs[subset_idx] exposures_subset = exposures[subset_idx] results[idx] = { \u0026#34;predicted\u0026#34;: claims_subset.sum(), # Note that these calculations are the same as multiplying all of the sampled freqs and exposures together and then taking the quantile \u0026#34;q25\u0026#34;: (np.quantile(freqs_subset, 0.25, axis=1) * exposures_subset).sum(), \u0026#34;median\u0026#34;: (np.quantile(freqs_subset, 0.50, axis=1) * exposures_subset).sum(), \u0026#34;q75\u0026#34;: (np.quantile(freqs_subset, 0.75, axis=1) * exposures_subset).sum(), } fig, ax = plt.subplots(figsize=(10, 6)) bins = sorted(list(results.keys())) x_positions = np.arange(len(bins)) medians = [results[bin_idx][\u0026#34;median\u0026#34;] for bin_idx in bins] q25s = [results[bin_idx][\u0026#34;q25\u0026#34;] for bin_idx in bins] q75s = [results[bin_idx][\u0026#34;q75\u0026#34;] for bin_idx in bins] predictions = [results[bin_idx][\u0026#34;predicted\u0026#34;] for bin_idx in bins] ranges = ax.vlines( x_positions, q25s, q75s, color=\u0026#34;cornflowerblue\u0026#34;, alpha=0.7, linewidth=3, label=\u0026#34;25th-75th Percentile Range\u0026#34; ) median_dots = ax.scatter(x_positions, medians, color=\u0026#34;navy\u0026#34;, s=80, zorder=3, label=\u0026#34;Median\u0026#34;) pred_dots = ax.scatter(x_positions, predictions, color=\u0026#34;crimson\u0026#34;, marker=\u0026#34;X\u0026#34;, s=100, zorder=3, label=\u0026#34;Predicted\u0026#34;) ax.set_xticks(x_positions) ax.set_xticklabels([f\u0026#34;Bin {bin_idx}\u0026#34; for bin_idx in bins]) ax.set_xlabel(\u0026#34;Bin\u0026#34;, fontsize=12) ax.set_ylabel(\u0026#34;Total Claims\u0026#34;, fontsize=12) ax.set_title(\u0026#34;Predicted Values vs. Quantile Ranges by Bin\u0026#34;, fontsize=14) ax.legend(handles=[ranges, median_dots, pred_dots], loc=\u0026#34;upper left\u0026#34;) ax.grid(True, linestyle=\u0026#34;--\u0026#34;, alpha=0.7) ax.set_axisbelow(True) plt.tight_layout() Here we can see that for each bin, we marked the model predicted total claim count, as well as the 25th, median, and 75th percentile total claim counts computed from our inference model. We can see that for bins 0 through 3, our predictions sit nicely towards the middle of the expected claims distribution. However, for bin 4, our predicted values lies past the quantile boundaries, meaning that we over-predict claim counts for policies in that bin.\nThis kind of visualization gives you more information about how reasonable your predicted values are compared to the entire posterior distribution of observed claims. Of course, you can do the same kind of thing for total losses, severity, etc. See, for instance, that in bin 1 our predicted value does get away from the median moreso than some of the other bins, but the quantile boundaries tell you that it\u0026rsquo;s not really that off.\nConclusion Hopefully this is illustrative, but I think a model like this is very useful for placing your expectations about your historical policy experience in a nice framework. I don\u0026rsquo;t think what we actually observed in the past is the end of the story for insurance, even putting aside development issues, which is another thing you could expand this model to incorporate. The past only gives us a single pass of experience, but we can still use that information to make pretty good guesses about what could have happened.\n","permalink":"http://optionallybayes.com/posts/insurance_resampling/","summary":"Using pymc to build a model to provide posterior samples of policy-level claim and severity estimates","title":"Bayesian Method for Insurance Policy Resampling"},{"content":"In one of my previous posts I implemented a trend filtering model in the univariate case. This is useful on its own but I want to extend it to the multivariate additive case to make it more useful for real-world modeling. Here I\u0026rsquo;ll consider this model form: $$ y_i = \\alpha + f_1(x_{i, 1}) + f_2(x_{i, 2}) + \\ldots + f_k(x_{i, k}) + \\epsilon $$ So, we\u0026rsquo;re assuming that the value of $y$ is a linear function of functions of each of our input variables $x$. In this case each of the smoothing functions, $f_j$ will be fit using the trend filtering method. So, this is a traditional GAM where we\u0026rsquo;re changing the form of the smoothing functions. I\u0026rsquo;ll be again using a least squares fit for simplicity, so we\u0026rsquo;re assuming $\\epsilon$ is a standard normal random variable, but this probabilistic interpretation won\u0026rsquo;t matter much here because I\u0026rsquo;ll be focusing more on implementation.\nSynthetic Data I\u0026rsquo;ll be using synthetic data with a known additive functional form to make sure that the model fitting procedure can properly recover the true underlying function. The function is simple: $$ \\begin{align*} y_i \u0026amp;= 5 + \\sin(x_{i, 1}) + \\exp(x_{i, 2}) + \\epsilon \\newline \\epsilon \u0026amp;\\sim \\text{Normal}(\\mu = 0, \\sigma^2 = 0.5^2) \\end{align*} $$\nI\u0026rsquo;m also simulating the $x$ values in a specific way to ensure some behavior in the values to make sure I can handle common practical issues. Firstly, the total number of values I\u0026rsquo;m simulating is 10,000, but I\u0026rsquo;ll be sampling integers bounded in such a way that the total number of unique possibilities is smaller than the total number of values I\u0026rsquo;m sampling. By the pigeonhole principle, this guarantees that I will sample multiple of the same value which ensures that my fitting procedure can deal with data where there are repeated $x$ values. For example, I\u0026rsquo;m sampling $x_{i, 1}$ as integers from -100 to +100 and then dividing by 10 to get rational values between -10 and 10. There are only 200 unique values possible, but again, I\u0026rsquo;m sampling 10,000 points, and this guarantees repeated values. In other words, the number of unique values will always be less than 10,000.\nSecondly, I\u0026rsquo;m also making sure that the number of unique values differs between each of the $x$\u0026rsquo;s. For $x_{i, 1}$ there will be at most 200 unique values, and for $x_{i, 2}$ there will be at most 500. Technically, by random chance they could be the same, but this is exceedingly unlikely to happen given 10,000 sample points, so I\u0026rsquo;m ignoring this possibility. This behavior ensures that my fitting procedure can deal with each input feature having different numbers of unique values, necessitating individual treatment which we\u0026rsquo;ll see later.\nimport numpy as np import plotly.graph_objects as go import plotly.io as pio from plotly.subplots import make_subplots import scipy import cvxpy as cp rng = np.random.default_rng() pio.renderers.default = \u0026#34;iframe\u0026#34; n = 10000 X = np.hstack( [ rng.integers(-100, 100, size=(n, 1)) / 10, rng.integers(-250, 250, size=(n, 1)) / 250, ] ) true_y = 5 + np.sin(X[:, 0]) + np.exp(X[:, 1]) obs_y = true_y + 0.5 * rng.standard_normal(n) We can then plot our true function and some sampled values:\nplot_x = np.linspace(X[:, 0].min(), X[:, 0].max(), 100) plot_y = np.linspace(X[:, 1].min(), X[:, 1].max(), 100) x_grid, y_grid = np.meshgrid(plot_x, plot_y) Z = scipy.interpolate.griddata( (X[:, 0], X[:, 1]), true_y, (x_grid, y_grid), method=\u0026#34;linear\u0026#34; ) fig = go.Figure( data=[ go.Surface(x=plot_x, y=plot_y, z=Z), go.Scatter3d( x=X[:, 0], y=X[:, 1], z=obs_y, opacity=0.15, mode=\u0026#34;markers\u0026#34;, marker={\u0026#34;size\u0026#34;: 3, \u0026#34;color\u0026#34;: \u0026#34;black\u0026#34;}, ), ] ) fig.update_layout(title=\u0026#34;True Function with Sampled Values\u0026#34;) fig.show() We have our data now, so we can move on to fitted our trend filtering model. What we want to do is fit a separate smoothing function to each variable, so the below code creates a dictionary of dictionary that contains all of the information we need to keep track of for each variable to then construct our model.\nKeeping track of each variable separately immediately solves one of the problems outline above, namely that of differing numbers of unique values per input variable. The next challenge is dealing with the presence of duplicate values in our input data. The trend filtering fitting procedure will only work if our input data points are sorted and do not have any duplicate values. To handle this I construct two arrays for each variable: first is a sorted array containing all of the unique values, and the second is a reconstruction array containing indices that can reconstruct the entire original array of observations from the array of unique values.\nThis way we can then create our $D$ matrix which is used for applying to penalty to the parameters, as well as a fitted parameter vector, $\\beta$, the entires of which correspond to each unique observation value. In the model, then, we can reconstruct an array of equal length to our original observation vector by indexing $\\beta$ using the reconstruction indices we\u0026rsquo;ve precomputed.\ndef make_D_matrix(n): ones = np.ones(n) return scipy.sparse.spdiags(np.vstack([-ones, ones]), range(2), m=n - 1, n=n) params = {} for i in range(X.shape[1]): unique_vals, recon_idx = np.unique(X[:, i], return_inverse=True) params[i] = { \u0026#34;sort_idx\u0026#34;: np.argsort(X[:, i]), # These are guaranteed to be sorted \u0026#34;unique_vals\u0026#34;: unique_vals, \u0026#34;recon_idx\u0026#34;: recon_idx, \u0026#34;D_mat\u0026#34;: make_D_matrix(len(unique_vals)), \u0026#34;beta_vec\u0026#34;: cp.Variable(len(unique_vals), name=f\u0026#34;X_{i}\u0026#34;), } So we have now precomputed the things we need to assemble our model. First, we create a variable for the intercept. Next, we can get our model predicted values by taking the intercept plus the values from the fitted $\\beta$ vector reassembled by using the reconstruction array for each input variable. Then, we can compute the penalty for each input variable by taking each of their $D$ matrices and matrix multiplying it with the corresponding $\\beta$ vector, norming, and summing.\nIn notation: $$ \\hat{y_i} = \\alpha + \\sum_{j=1}^k \\beta_{i,j} $$\nWhere $k$ is the total number of input variables, and $\\beta_{i,j}$ is the fitted value corresponding to data point $i$ for variable $j$. The array indexing I do below in the code is needed to fetch the correct $\\beta$ vector value for each data point, given that we have duplicated values and our original data is not sorted.\nOur total penalty term looks like this: $$ P = \\sum_{j=1}^k \\Vert D_j \\beta_j \\Vert_1 $$\nThis is just the sum of the $\\ell_1$ norm of the difference matrix applied to the parameter vector for each variable. See my last post on the univariate trend filtering case for more details about how this works. Here we are simply applying the univariate trend filtering penalty to each variable individually and combining them.\nSo, now that we have our model predicted values for each input as well as the penalty term, we can assemble it all together into our objective function, which is a simple least squares objective with a penalty term: $$ \\underset{\\alpha, \\beta}{\\text{argmin}} \\frac{1}{2} \\Vert y - \\hat{y} \\Vert^2_2 + \\lambda P $$\nwhere $\\lambda$ is a free regularization parameter to be selected. From these equations you can start to see the appeal of this method: there is only 1 hyperparameter to be dealt with. Unlike splines, you don\u0026rsquo;t have to worry about selecting knots, because the trend filtering process does this implicitly for us.\nThere is only one more detail to be dealt with which is identifiability. This issue is already well known in the larger GAM literature, and the solution is simple, although not necessarily complete, as I\u0026rsquo;ll discuss later. The problem is that the space spanned by our smoothing functions includes a constant function, which means that each smoothing function also implicitly includes its own intercept term, along with the one we\u0026rsquo;ve explicitly added. This results in a problem where the intercept is not identifiable because the model is equivalent from a loss perspective whether the necessary constant terms get added to the intercept we\u0026rsquo;ve specified or whether it gets added to any of the implicit intercepts of any of the smoothing functions. For example, our synthetic data has a true intercept of 5, but our model may end up setting the intercept term to 6.5, and then offset this by moving one of the smoothing functions down by 1.5 everywhere. There are an infinite number of these possibilities, which means our model is not identifiable.\nTo fix this we add a constraint which is commonly used in the literature, which is to require that the total effect of each smoothing function across the entire input space sums to zero: $$ \\sum_i f(x_{i, j}) = 0 \\quad \\forall j $$\nIn our case, because the effect of our smoothing functions is totally defined by the $\\beta$ vectors this simplifies to: $$ \\sum_i \\beta_{i, j} = 0 \\quad \\forall j $$\nThis effectively constrains the implicit intercept of each smoothing function to be zero, which solves the identifiability problem, with some caveats.\nPutting all of this together, we can finally fit our model:\n# For each observed y value get the relevant beta coefficient for that X observation # by using the reconstruction index based on the unique values vector alpha = cp.Variable(name=\u0026#34;alpha\u0026#34;) y_hat = alpha + cp.sum( [params[i][\u0026#34;beta_vec\u0026#34;][params[i][\u0026#34;recon_idx\u0026#34;]] for i in params.keys()] ) # Compute separate l1 norms for each input variable and sum penalty = cp.sum( [cp.norm(params[i][\u0026#34;D_mat\u0026#34;] @ params[i][\u0026#34;beta_vec\u0026#34;], 1) for i in params.keys()] ) lam = 5 objective = cp.Minimize(0.5 * cp.sum_squares(obs_y - y_hat) + lam * penalty) # Sum to zero constraint to fix identifiability problems constraints = [cp.sum(params[i][\u0026#34;beta_vec\u0026#34;]) == 0 for i in params.keys()] prob = cp.Problem(objective, constraints) results = prob.solve(solver=\u0026#34;CLARABEL\u0026#34;) Then we can compare our fitted function to the true function:\nZ_fitted = scipy.interpolate.griddata( (X[:, 0], X[:, 1]), y_hat.value, (x_grid, y_grid), method=\u0026#34;nearest\u0026#34; ) fig = make_subplots( rows=1, cols=2, specs=[[{\u0026#34;is_3d\u0026#34;: True}, {\u0026#34;is_3d\u0026#34;: True}]], subplot_titles=[ \u0026#34;True Function\u0026#34;, \u0026#34;Fitted Piecewise Function\u0026#34;, ], ) fig.add_trace(go.Surface(x=plot_x, y=plot_y, z=Z), row=1, col=1) fig.add_trace(go.Surface(x=plot_x, y=plot_y, z=Z_fitted), row=1, col=2) fig.show() We can see this is working. We can also look at the marginal relationship for each input variable:\nplot_x_0 = np.linspace( params[0][\u0026#34;unique_vals\u0026#34;].min(), params[0][\u0026#34;unique_vals\u0026#34;].max(), len(params[0][\u0026#34;unique_vals\u0026#34;]), ) fig = go.Figure( [ go.Scatter(x=plot_x_0, y=np.sin(plot_x_0), name=\u0026#34;True Function\u0026#34;), go.Scatter( x=params[0][\u0026#34;unique_vals\u0026#34;], y=params[0][\u0026#34;beta_vec\u0026#34;].value, name=\u0026#34;Fitted Function\u0026#34;, ), ], ) fig.update_layout(title=\u0026#34;Marginal Relationship for First Variable\u0026#34;) fig.show() plot_x_1 = np.linspace( params[1][\u0026#34;unique_vals\u0026#34;].min(), params[1][\u0026#34;unique_vals\u0026#34;].max(), len(params[1][\u0026#34;unique_vals\u0026#34;]), ) fig = go.Figure( [ go.Scatter(x=plot_x_1, y=np.exp(plot_x_1), name=\u0026#34;True Function\u0026#34;), go.Scatter( x=params[1][\u0026#34;unique_vals\u0026#34;], y=params[1][\u0026#34;beta_vec\u0026#34;].value, name=\u0026#34;Fitted Function\u0026#34;, ), ], ) fig.update_layout(title=\u0026#34;Marginal Relationship for Second Variable\u0026#34;) fig.show() We can see that for the first variable, the sine relationship is capture very well, but for the second variable, our fitted graph has the right shape but is shifted down. Let\u0026rsquo;s look at the fitted intercept value:\nalpha.value array(6.17078177) It\u0026rsquo;s not 5 as we would expect. In fact, this is the identifiability problem rearing its head. If we take the difference between this $\\alpha$ value and 5 we can see that it is very close to the gap between the true function value and our fitted curve for the second variable:\n# Average distance between true and fitted curve print(np.mean(np.exp(plot_x_1) - params[1][\u0026#34;beta_vec\u0026#34;].value)) # Distance between fitted and true intercept values print(alpha.value - 5) 1.1728523581910208 1.1707817719249132 The model has \u0026ldquo;moved\u0026rdquo; some of the intercept value to the marginal relationship for the second variable. Now, this has no effect when we look at our predictions, $\\hat{y}$, because the effect naturally washes out, so our predicted vs actual surfaces above are very close. But we obviously don\u0026rsquo;t recover the true intercept or the true marginal relationship for the second variable.\nAs far as I can tell, this is because the second variable follows an exponential curve. This means that the additive term for this variable will always be positive. You can see how this is an issue because we\u0026rsquo;ve constrained the model to have the total marginal effect sum to zero, when we expect the true total marginal effect to always be positive. If you replace the exponential function in the synthetic data code with something else that takes positive and negative values you can recover the correct intercept and marginal relationships. I have yet to work out if there is a better way to deal with this, or if some cases like the exponential are not fixable from this perspective. I\u0026rsquo;m not too worried about it because all of this only matters up to an additive constant, so the shape of the marginal relationship is correct, and predictions are unaffected, but it would be nice to be able to perfectly recover the true parameters in general. There may be a more clever way to handle the identifiability constraint that resolves this problem, but I don\u0026rsquo;t know it.\nInput Standardization One question that may arise with this model is whether we need to standardize our data beforehand like you have to in a normal ridge or lasso penalty setting. In those settings, if your model coefficients need to be on different scales the regularization penalty will improperly penalize larger variables more so than smaller ones, so you standardize the variables in advance so the penalty applied \u0026ldquo;equally\u0026rdquo; to everything.\nMy speculation is that it would make no difference here, although this is based on an argument that is very mathematically hand-wavy and I\u0026rsquo;m not confident in it. First, let\u0026rsquo;s start with our assumption of the true model form: $$ y_i = \\alpha + f(x_{1, i}) + g(x_{2, i}) + \\epsilon $$ Our $\\beta$ coefficients seek to estimate the values of the true functions at our sample points (we want our estimated functions $\\hat{f}$ and $\\hat{g}$ to be close to the true $f$ and $g$): $$ \\begin{align*} \\beta_{1, i} \u0026amp;= \\hat{f}(x_{1, i}) \\newline \\beta_{2, i} \u0026amp;= \\hat{g}(x_{2, i}) \\end{align*} $$ Our penalty is then: $$ \\begin{align} \\vert \\beta_{1, i+1} - \\beta_{1, i} \\vert \u0026amp;= \\vert \\hat{f}(x_{1, i+1}) - \\hat{f}(x_{1, i}) \\vert \\newline \u0026amp;= \\vert \\hat{f}(x_{1, i} + h) - \\hat{f}(x_{1, i}) \\vert \\newline \u0026amp;= \\vert h \\hat{f}\u0026rsquo;(x_{1, i}) \\vert \\end{align} $$ Where we get the second line by assuming that our input points are evenly spaced and close by and we get to the third line by using the usual limit definition of a derivative. You can easily make this same argument for $g$. Buying this, we can see that our penalty is based on the first derivative of our estimated function. This qualitatively means that larger regularization values will promote flatter estimated functions that converge to a constant function (a constant function has zero derivative), which is the behavior we see:\nlams = np.logspace(1, 3, 5) betas = [] for lam in lams: # For each observed y value get the relevant beta coefficient for that X observation # by using the reconstruction index based on the unique values vector alpha = cp.Variable(name=\u0026#34;alpha\u0026#34;) y_hat = alpha + cp.sum( [params[i][\u0026#34;beta_vec\u0026#34;][params[i][\u0026#34;recon_idx\u0026#34;]] for i in params.keys()] ) # Compute separate l1 norms for each input variable and sum penalty = cp.sum( [cp.norm(params[i][\u0026#34;D_mat\u0026#34;] @ params[i][\u0026#34;beta_vec\u0026#34;], 1) for i in params.keys()] ) objective = cp.Minimize(0.5 * cp.sum_squares(obs_y - y_hat) + lam * penalty) # Sum to zero constraint to fix identifiability problems constraints = [cp.sum(params[i][\u0026#34;beta_vec\u0026#34;]) == 0 for i in params.keys()] prob = cp.Problem(objective, constraints) results = prob.solve(solver=\u0026#34;CLARABEL\u0026#34;) betas.append(params[0][\u0026#34;beta_vec\u0026#34;].value) fig = go.Figure( [ go.Scatter( x=params[1][\u0026#34;unique_vals\u0026#34;], y=betas[i], name=f\u0026#34;Lambda Value: {lams[i]:.0f}\u0026#34;, ) for i in range(len(lams)) ], ) fig.update_layout(title=\u0026#34;First Variable Betas by Regularization Penalty\u0026#34;) fig.show() Note that our sum-to-zero constraint ensures that the function we fit converges to a constant zero. Moving back to our original problem, we can see from this: $$ \\vert h \\hat{f}\u0026rsquo;(x_{1, i}) \\vert $$ that the only thing we will change by standardizing our $x$ values beforehand is the thing that goes inside the derivative of our fitted function. If we do this for our first and second variable, the relative magnitudes of the penalties will still depend on the exact form and magnitude of the first derivatives of our fitted functions, which should largely match with the true underlying function. In particular, given our first variable\u0026rsquo;s true function is $sin(x)$ the derivative is $cos(x)$ so the magnitude of these values won\u0026rsquo;t change at all whether we standardize or not:\nstandard_X = (X - np.mean(X, axis=0)) / np.std(X, axis=0) sort_idx = np.argsort(X[:, 0]) fig = go.Figure( [ go.Scatter(x=X[:, 0][sort_idx], y=np.sin(X[:, 0][sort_idx]), name=\u0026#34;Original Values\u0026#34;), go.Scatter( x=standard_X[:, 0][sort_idx], y=np.cos(standard_X[:, 0][sort_idx]), name=\u0026#34;Standardized Values\u0026#34;, ), ], ) fig.update_layout(title=\u0026#34;Original vs Transformed x Values for First Variable\u0026#34;) fig.show() So, we condense the range of $x$ values that are being supplied, but the magnitude of those values is the same. This is even more striking for our second variable because the derivative of the exponential is itself, so the values we get back are identical, but just evaluated over a smaller range of $x$ values.\nAll of this is to say that I don\u0026rsquo;t think that standardizing the input values will help with the differing magnitude of penalty values across different variables. I do think that having a separate regularization term for each variable would be more correct but obviously this causes a more complicated hyper-parameter search scheme. At least for this problem, using a single regularization term seems to work, so I can somewhat confidently say that this issue is less significant in this setting than in the normal linear setting, but it certainly still matters. If you can afford the computation, you may get slightly better results giving each variable a separate regularization parameter.\nConclusion With this I think the basic mathematical and conceptual setup for extending trend filtering to GAMs is set. I still have some uncertainty about the need for applying separate penalty terms to each variable, but the general outline of how to make this work seems pretty clear. The only extension I would like to get around to eventually is covering a two-variate interaction case. You can imagine that instead of $\\beta$ vectors you would end up with matrices and the penalty matrices would need another dimension.\n","permalink":"http://optionallybayes.com/posts/gam_trend_filtering/","summary":"Extending the trend filtering approach to the multivariate additive case","title":"Extending Trend Filtering to the GAM Case"},{"content":"Inspired by Mahler (1990) and the use of baseball win percentages as an illustrative example of how insurance experience ratemaking works, I wanted to provide an implementation of the exponential smoothing approach mentioned. The dataset contains data of the annual season loss percentages for 8 teams from 1901 to 1960. The notion is that in the same way you would use the past history of an insured to predict their forward policy losses, you can use the history of a team\u0026rsquo;s performance to predict their performance going forward. Compared to a real-life insurance example, this dataset is simple because it has the same number of individuals throughout and they are all observed at every time step.\nThe paper already does a good job covering many possible approaches to this problem. It notes that first we want to pick a method that puts heavier weight on more recent observations because of the inherent non-stationarity of the underlying process. Including far-away past years may actually worsen your estimate because the environment going forward may have shifted substantially. The author discusses multiple solutions: the estimate of every team is the grand average of the data, use the most recent years loss percentage for each team as the prediction, weighting together the most recent year and the overall average, etcetera. Here, however, I\u0026rsquo;m going to focus on what is to me the most elegant solution, which is exponential smoothing.\nFirst I\u0026rsquo;ll cover the exponential smoothing approach on the whole dataset as given, and then I\u0026rsquo;ll extend it to the more complex case of having differing numbers and placements of observations for each team through time.\nProperties of the Data The first question to ask is if there is any value in the first place to using previous observations of each team to predict their future loss rate. The paper answers the more basic question of whether the teams have meaningfully different overall loss rates compared to each other, so I\u0026rsquo;ll take that as given here. To answer the temporal question, we can look at the autocorrelation of the observations of each team.\nimport polars as pl import polars.selectors as cs import statsmodels.api as sm import matplotlib.pyplot as plt import cmdstanpy as stan import arviz as az import numpy as np from scipy import stats rng = np.random.default_rng() df = pl.read_csv(\u0026#34;..//data//nl-data.csv\u0026#34;).sort(\u0026#34;Year\u0026#34;) df shape: (60, 9)YearNL1NL2NL3NL4NL5NL6NL7NL8i64f64f64f64f64f64f64f64f6419010.50.4190.6190.6260.620.4070.3530.45719020.4670.4570.5040.50.6470.5910.2590.58219030.580.4850.4060.4680.3960.6370.350.68619040.6410.6340.3920.4250.3070.6580.4310.51319050.6690.6840.3990.4840.3140.4540.3730.623\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;19560.3960.4030.610.4090.5650.5390.5710.50619570.4550.3830.5970.4810.5520.50.5970.43519580.5390.4030.5320.5060.4810.5520.4550.53219590.4360.4490.5190.5190.4610.5840.4940.53919600.4680.4290.610.5650.4870.6170.3830.442 fig, axes = plt.subplots(2, 4, figsize=(15, 10)) for i in range(8): ax = axes.ravel()[i] sm.graphics.tsa.plot_acf(df[f\u0026#34;NL{i+1}\u0026#34;], ax=ax, zero=False) ax.set_title(f\u0026#34;NL{i+1}\u0026#34;) The light blue areas are the 95% confidence intervals for the autocorrelations. We can obviously see significant autocorrelations for at least one lag for every team, meaning there is indeed valuable information to be gained by using past years data to make predictions.\nHomogenous Observations Case First, I\u0026rsquo;ll cover the original and more simple case where we observe loss rates for every team in every year. A basic univariate exponential smoothing model looks like this:\n$$ \\mu_t = \\alpha y_{t-1} + (1 - \\alpha) \\mu_{t-1} $$\nOur estimate for each time period $t$ is $\\mu_t$ and this is taken to be a weighted sum of the actually observed value from the last time period and our previous smoothed estimator. This does mean that you have to handle setting the first smoothed value $\\mu_0$, which I\u0026rsquo;m letting be a free parameter here.\nThe model here extends this simply by assuming this univariate form for each team separately:\n$$ \\mu_{i, t} = \\alpha y_{i, t-1} + (1 - \\alpha) \\mu_{i, t-1} $$\nwhere $i$ indexes over each team. Note that the smoothing parameter, $\\alpha$, is assumed to be the same for each team, although you could easily extend this to the case where it is allowed to vary.\nIn probabilistic terms, here is the full set of equations with priors:\n$$ \\begin{align*} k \u0026amp;\\sim \\text{Exponential}(1) \\newline \\alpha \u0026amp;\\sim \\text{Beta}(2, 2) \\newline \\mu_{i, 0} \u0026amp;\\sim \\text{Beta}(2, 2) \\newline \\mu_{i, t} \u0026amp;= \\alpha y_{i, t-1} + (1 - \\alpha) \\mu_{i, t-1} \\newline y_{i, t} \u0026amp;\\sim \\text{Beta}(\\mu = \\mu_{i, t}, \\sigma^2 = k) \\end{align*} $$\nwhere $y_{i, t}$ is the loss rate observation for team $i$ at time $t$.\nI\u0026rsquo;m assuming that the prior for the first smoothed value and the smoothing strength parameter are $\\text{Beta}(2, 2)$ distributed, which looks like this:\nxs = np.linspace(0, 1, 100) plt.plot(xs, stats.beta.pdf(xs, 2, 2)) Here, $\\frac{1}{2}$ is the mode. By assuming this prior we are saying that it\u0026rsquo;s unlikely that either of these quantities assume very extreme values, which is reasonable in this case. For the initial smoothing value, we are saying a 50/50 win/loss performance is the most probable, and values close to 0 or 1 are unlikely, which is a sensible prior assumption. For the smoothing strength parameter we are assuming that the case where we evenly weight the last observed value and the last smoothed value is the most probable, and its unlikely that we want to put full weight on either one or the other, which, again, is a sensible prior assumption. I\u0026rsquo;ve chosen an exponential distribution for the outcome variance parameter because it\u0026rsquo;s the maximum entropy distribution for random variables bounded below by zero. In this case, the parameter for this prior variable has little effect on the results.\nThis can all be encoded in Stan as follows:\ndata { int N; // Number of teams int T; // Number of observations array[N] vector[T] L; // Observation vector for each team } parameters { real\u0026lt;lower=0\u0026gt; k; real\u0026lt;lower=0, upper=1\u0026gt; alpha; real\u0026lt;lower=0, upper=1\u0026gt; mu_zero; } transformed parameters { array[N] vector[T] mu; for (i in 1:N) { mu[i][1] = mu_zero; } for (t in 2:T) { for (i in 1:N) { mu[i][t] = alpha * L[i][t-1] + (1 - alpha) * mu[i][t-1]; } } } model { k ~ exponential(1); alpha ~ beta(2, 2); mu_zero ~ beta(2, 2); for (t in 1:T) { for (i in 1:N) { L[i][t] ~ beta_proportion(mu[i][t], k); } } } generated quantities { array[N] vector[T] L_hat; for (t in 1:T) { for (i in 1:N) { L_hat[i][t] = beta_proportion_rng(mu[i][t], k); // Posterior predictive } } } model = stan.CmdStanModel(stan_file=\u0026#34;../experience_rating/model.stan\u0026#34;) data = {\u0026#34;N\u0026#34;: 8, \u0026#34;T\u0026#34;: len(df), \u0026#34;L\u0026#34;: [df[f\u0026#34;NL{i}\u0026#34;].to_list() for i in range(1, 9)]} fit = model.sample(data=data) Fitting this, we can check the summary statistic for our chain and convergence statistics:\nfit.summary()[\u0026#34;R_hat\u0026#34;].describe() count 964.000000 mean 0.999956 std 0.000440 min 0.999100 25% 0.999736 50% 0.999888 75% 1.000110 max 1.002550 Name: R_hat, dtype: float64 print(fit.diagnose()) Checking sampler transitions treedepth. Treedepth satisfactory for all transitions. Checking sampler transitions for divergences. No divergent transitions found. Checking E-BFMI - sampler transitions HMC potential energy. E-BFMI satisfactory. Effective sample size satisfactory. Split R-hat values satisfactory all parameters. Processing complete, no problems detected. Everything looks normal here, let\u0026rsquo;s look at the posterior distribution of our parameters:\nidata = az.from_cmdstanpy( fit, posterior_predictive=\u0026#34;L_hat\u0026#34;, observed_data={\u0026#34;L\u0026#34;: [df[f\u0026#34;NL{i}\u0026#34;].to_list() for i in range(1, 9)]}, ) az.plot_trace(idata, var_names=[\u0026#34;k\u0026#34;, \u0026#34;alpha\u0026#34;, \u0026#34;mu_zero\u0026#34;]) Again, this looks like healthy convergence.\nNow we can look at the actually fit of the model. I\u0026rsquo;m not going to do a full-blown analysis here because this post is mainly meant to be a proof-of-concept, but we can make some general observations. First, let\u0026rsquo;s look at the observed values and the smoothed values, $\\mu$, for a given team:\nmu = az.extract(idata, var_names=[\u0026#34;mu\u0026#34;]) plt.plot(mu[0, :, :].mean(axis=1)) plt.plot(df[\u0026#34;NL1\u0026#34;]) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;Loss Rate\u0026#34;) plt.title(\u0026#34;NL1 Fit\u0026#34;) plt.legend([\u0026#34;Smoothed Values\u0026#34;, \u0026#34;Observed Values\u0026#34;]) We can see that the smoothed values are indeed capturing the behavior of the observed values.\nNext we can look at a posterior predictive plot to see how closely our distributional assumptions match the observed distribution of the data across all the teams:\naz.plot_ppc( idata, data_pairs={\u0026#34;L\u0026#34;: \u0026#34;L_hat\u0026#34;} ) We can see there is a fair bit of noise in the observed data, and maybe one could argue there is less kurtosis in the observed data than our model assumes, but we aren\u0026rsquo;t too far off distributionally here. I imagine this could be fixed by changing the model to have a variance term that varies by team or maybe even over time. In the model present the variance is assumed to be constant for all teams and for all time periods.\nNext, we can look at the implied weights on lagged data points given from the $\\alpha$ parameter. Our model for $\\mu$ is again defined as: $$ \\mu_{i, t} = \\alpha y_{i, t-1} + (1 - \\alpha) \\mu_{i, t-1} $$ So, we can continue to expand the autoregressive terms to get this purely in terms of the lagged $y$ values: $$ \\begin{align*} \\mu_{i, t} \u0026amp;= \\alpha y_{i, t-1} + (1 - \\alpha) (\\alpha y_{i, t-2} + (1 - \\alpha) \\mu_{i, t-2}) \\newline \u0026amp;= \\alpha y_{i, t-1} + \\alpha (1-\\alpha) y_{i, t-2} + (1 - \\alpha)^2 \\mu_{i, t-2} \\newline \u0026amp;= \\ldots \\newline \u0026amp;= \\alpha \\sum_{i=1}^\\infty (1 - \\alpha)^{i-1} y_{i, t-i} \\end{align*} $$\nWe can adjust the index term on the sum to make it into a standard geometric series: $$ \\mu_{i, t }= \\alpha \\sum_{i=0}^\\infty (1 - \\alpha)^{i} y_{i, t-i+1} $$\nIf we then assume that $y_{i,t} = 1$ for all $t$ for the sake of argument then we can see that this series converges to $$ \\mu_{i, t} = \\frac{\\alpha}{1 - (1 - \\alpha)} = \\frac{\\alpha}{\\alpha} = 1 $$\nSo, our prediction is a weighted sum of infinitely many lagged observation values. We can plot these weights to see how quickly they decay:\nalpha = az.extract(idata, var_names=[\u0026#34;alpha\u0026#34;]).mean().item() L_coefs = [alpha * (1 - alpha) ** (i - 1) for i in range(1, 15)] plt.plot(L_coefs) plt.xlabel(\u0026#34;Number of Lags\u0026#34;) plt.ylabel(\u0026#34;Weight\u0026#34;) plt.title(\u0026#34;Lagged Observation Weights\u0026#34;) So, we can see that a significant majority of the weight is being placed on observations in the 5 years. Since we know these weights also sum to 1, we can also see the cumulative total weight:\nplt.plot(np.cumsum(L_coefs)) plt.axhline(0.95, color=\u0026#34;red\u0026#34;) plt.xlabel(\u0026#34;Number of Lags\u0026#34;) plt.ylabel(\u0026#34;Cumulative Weight\u0026#34;) plt.title(\u0026#34;Cumulative Lagged Observation Weights\u0026#34;) The red line is placed at 95% total weight, so we can see that 95% of the weight of our smoothed values come from data points in the last 3 seasons.\nLastly, let\u0026rsquo;s see if we have controlled for the autocorrelation effect we saw in the original data by looking to see if there is any remaining patterns in the residuals:\nresids = df.select(cs.contains(\u0026#34;NL\u0026#34;)).to_numpy() - mu.mean(axis=2).values.T fig, axes = plt.subplots(2, 4, figsize=(15, 10)) for i in range(8): ax = axes.ravel()[i] sm.graphics.tsa.plot_acf(resids[:, i], ax=ax, zero=False) ax.set_title(f\u0026#34;NL{i+1}\u0026#34;) These autocorrelation plots now look much more random than they did originally, meaning we have captured much of the time-varying pattern present in the data.\nHeterogeneous Observations Case Now that we\u0026rsquo;ve proven the concept of the model in the easy case, we can move on to the harder and more realistic case where we don\u0026rsquo;t observe every individual at each time step. To accomplish this I\u0026rsquo;m setting the starting date of each teams observations to be random uniform from 1901 to 1920 and their ending dates to be random uniform from 1941 to 1960. I\u0026rsquo;m still making one simplifying observation which is that when do we observe a team we observe it every year for however long we do observe it for, which is not always true in real-life insurance datasets.\nSo, these are the new starting and ending dates for each team:\nstarting_years = rng.integers(1901, 1920, 8) ending_years = rng.integers(1941, 1960, 8) # Don\u0026#39;t forget fencepost counting pl.DataFrame( { \u0026#34;Team\u0026#34;: df.select(cs.contains(\u0026#34;NL\u0026#34;)).columns, \u0026#34;StartingYear\u0026#34;: starting_years, \u0026#34;EndingYear\u0026#34;: ending_years, } ).with_columns(NumYears=(pl.col(\u0026#34;EndingYear\u0026#34;) - pl.col(\u0026#34;StartingYear\u0026#34;)) + 1) shape: (8, 4)TeamStartingYearEndingYearNumYearsstri64i64i64\u0026quot;NL1\u0026quot;1917194933\u0026quot;NL2\u0026quot;1918194528\u0026quot;NL3\u0026quot;1910195849\u0026quot;NL4\u0026quot;1903195755\u0026quot;NL5\u0026quot;1919195436\u0026quot;NL6\u0026quot;1919194729\u0026quot;NL7\u0026quot;1911195545\u0026quot;NL8\u0026quot;1907195650 Now, the problem is that our observation vectors are of varying lengths, which means we cannot use the same data structure we used in the original model because it assume that each team\u0026rsquo;s observation vector was the same length. Unfortunately, Stan does not currently support ragged arrays that would let us handle this natively, so we have to use some clever programming to make it work.\nThe solution is to append every team\u0026rsquo;s observation vector into one single longer vector and separately keep track of what entries in this new vector belong to each team. So, first we create the observation vectors for each team, and then I\u0026rsquo;ve borrowed a nice function that creates a concatenated vector and returns the index of where each team\u0026rsquo;s data ends:\ndf2 = df.with_columns( [ pl.when(~pl.col(\u0026#34;Year\u0026#34;).is_between(start, end)) .then(None) .otherwise(pl.col(f\u0026#34;NL{i+1}\u0026#34;)) .alias(f\u0026#34;NL{i+1}\u0026#34;) for i, (start, end) in enumerate(zip(starting_years, ending_years)) ] ) data_vectors = [df2[f\u0026#34;NL{i+1}\u0026#34;].drop_nulls() for i in range(0, 8)] # From https://tonysyu.github.io/ragged-arrays.html def stack_ragged(arrays): lens = [len(array) for array in arrays] idx = np.cumsum(lens) stacked = np.concatenate(arrays) return stacked, idx stacked_data, data_idx = stack_ragged(data_vectors) stacked_data.shape (325,) data_idx array([ 33, 61, 110, 165, 201, 230, 275, 325]) So, team 1\u0026rsquo;s data is from index 0 to 32, team 2\u0026rsquo;s is from 33 to 60, and so on. Note that in Python the range slicing using a colon is left-side inclusive and right-side exclusive. So in the indexing array the endpoint for team 1 is 33 because you would use y[:33] to select it, but this selection would not include the value at index 33, it would stop at index 32. You could then use y[33:61] to select the data for team 2.\nThe next clever bit of programming comes in our Stan code to actually ingest these two structures. We are assuming the same model form, the only change we are making is the inconsistent observations, so, our model code now looks like this:\ndata { int N; // # of observations int K; // # of teams array[K] int idx; // Ending index for each team\u0026#39;s observations vector[N] ys; // Concatenated observation vector } parameters { real\u0026lt;lower=0\u0026gt; sigma; real\u0026lt;lower=0, upper=1\u0026gt; alpha; real\u0026lt;lower=0, upper=1\u0026gt; mu_zero; } transformed parameters { vector[N] mu; { int start_idx = 1; for (k in 1:K) { mu[start_idx] = mu_zero; for (t in 1:(idx[k] - start_idx)) { mu[start_idx + t] = alpha * ys[start_idx + t - 1] + (1 - alpha) * mu[start_idx + t - 1]; } start_idx = idx[k] + 1; } } } model { sigma ~ exponential(1); alpha ~ beta(2, 2); mu_zero ~ beta(2, 2); ys ~ beta_proportion(mu, sigma); } All of the magic here happens in the transformed parameters block because once we have our $\\mu$ vector the observation probabilities come directly from that. This is all made a bit more confusing because in Stan the starting index for arrays is 1 instead of 0 like in Python.\nTo explain briefly:\nWe start at the beginning of the array at start_idx = 1 For each team: Set the first value, which is t=0 for this team to mu_zero Then iterating over the index values corresponding to this team compute the exponential smoothing quantity in the normal way Set the new starting index for the next team to be the ending index for the current team plus 1 As an example for teams 1 and 2:\nstart_idx = 1 Set mu[1] to mu_zero For t in 1:32 (idx[k] = 33) compute the exponential smoothing quantity start_idx is already 1 so start_idx + 1 = 2 when t = 1 So, start_idx + t tracks the relative time sequence for the currently considered team Then, to get the lagged values you just need to subtract 1 from the current relative time value Set start_idx = 34 Set mu[34] to mu_zero because this is now t=0 for team 2 For t in 1:28 (61 - 33 = 28) compute the smoothed quantity And so on up to the last team Essentially we want to go through time for each team as if we are starting at t=0 but we need to keep track of where we are in the overall vector so we apply the right computations to the right data.\nmodel2 = stan.CmdStanModel(stan_file=\u0026#34;../experience_rating/model2.stan\u0026#34;) data2 = {\u0026#34;N\u0026#34;: len(stacked_data), \u0026#34;K\u0026#34;: 8, \u0026#34;idx\u0026#34;: data_idx, \u0026#34;ys\u0026#34;: stacked_data} fit2 = model2.sample(data=data2) We can look again at our convergence diagnostics:\nprint(fit2.diagnose()) Checking sampler transitions treedepth. Treedepth satisfactory for all transitions. Checking sampler transitions for divergences. No divergent transitions found. Checking E-BFMI - sampler transitions HMC potential energy. E-BFMI satisfactory. Effective sample size satisfactory. Split R-hat values satisfactory all parameters. Processing complete, no problems detected. fit2.summary()[\u0026#34;R_hat\u0026#34;].describe() count 654.000000 mean 1.000096 std 0.000442 min 0.999109 25% 0.999827 50% 1.000125 75% 1.000280 max 1.002640 Name: R_hat, dtype: float64 idata2 = az.from_cmdstanpy( fit2, posterior_predictive=\u0026#34;ys_hat\u0026#34;, observed_data={\u0026#34;ys\u0026#34;: stacked_data}, ) az.plot_trace(idata2, var_names=[\u0026#34;sigma\u0026#34;, \u0026#34;alpha\u0026#34;, \u0026#34;mu_zero\u0026#34;]) Everything looks good, so we can move on to making our posterior checks:\nmu = az.extract(idata2, var_names=[\u0026#34;mu\u0026#34;]).mean(axis=1) plt.plot(mu[: data_idx[0]].values) plt.plot(df2[\u0026#34;NL1\u0026#34;].drop_nulls()) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;Loss Rate\u0026#34;) plt.title(\u0026#34;NL1 Fit\u0026#34;) plt.legend([\u0026#34;Smoothed Values\u0026#34;, \u0026#34;Observed Values\u0026#34;]) az.plot_ppc( idata2, data_pairs={\u0026#34;ys\u0026#34;: \u0026#34;ys_hat\u0026#34;} ) We can see here that again we are fitting the data as we were before.\nOne interesting difference in the fit between this model and the previous are the values of the smoothing parameter $\\alpha$:\nalpha1 = az.extract(idata, var_names=[\u0026#34;alpha\u0026#34;]) alpha2 = az.extract(idata2, var_names=[\u0026#34;alpha\u0026#34;]) az.plot_dist(alpha1, color=\u0026#34;black\u0026#34;, label=\u0026#34;First Model\u0026#34;) az.plot_dist(alpha2, color=\u0026#34;red\u0026#34;, label=\u0026#34;Second Model\u0026#34;) plt.axvline(alpha1.mean(), color=\u0026#34;black\u0026#34;, linestyle=\u0026#34;dashed\u0026#34;, alpha=0.5) plt.axvline(alpha2.mean(), color=\u0026#34;red\u0026#34;, linestyle=\u0026#34;dashed\u0026#34;, alpha=0.5) plt.xlabel(\u0026#34;Alpha Value\u0026#34;) We can see that the second model has a mean value for $\\alpha$ that is smaller than for the first model, which implies that less weight is put on lagged observations and more weight is put on lagged smoothed values in the second model. This results in more smoothing overall compared to the first model. This is somewhat intuitive because our effective sample size is smaller in the second model because we have fewer observations for each team and varying numbers of them, so we get a nice bit of regularization here because the model selects a slightly more aggressive smoothing parameter.\nConclusion This was a small proof-of-concept to firstly provide an actual implementation of the exponential smoothing model presented in Mahler (1990), as well as provide the extension to the case where we have a heterogeneously observed sample which is very common in real insurance datasets. I think exponential smoothing models are a conceptually elegant way to handle time-series prediction problems like this because it implicitly chooses how many lagged values are relevant, in a sense, as compared to a standard autoregressive model where the number of lags to include is effectively a hyperparameter. This model can also be easily extended to incorporate exogenous variables, time-varying variances, a hierarchical model to allow parameters to vary by individual, etcetera.\nReferences Mahler, H. C. (1990). An Example of Credibility and Shifting Risk Parameters. Proceedings of the Casualty Actuarial Society, LXXX, 225308.\n","permalink":"http://optionallybayes.com/posts/exp_rating_exp_smooth/","summary":"Modeling baseball win rates using exponential smoothing to illustrate insurance experience rating","title":"Experience Rating With Exponential Smoothing"},{"content":"I\u0026rsquo;ve recently come across a new-to-me, and generally relatively recently developed, method of non-parametric regression called trend filtering. There is a paper by Tibshirani (2014) that establishes nice theoretical results, namely that trend filtering achieves a better minimax convergence rate to the true underlying function than traditional smoothing splines. It is also very fast to fit in the mean squared error regime, as I will show below.\nUnderlying Function As usual for these demos, we start by defining our true underlying function and take some noisy sample of it. Here, I\u0026rsquo;m borrowing the example given in Politsch et al. (2020) which is a function that has a smooth global trend with some intermittent \u0026ldquo;bumps\u0026rdquo; given by some radial basis functions:\n$$ f(t) = 6 \\sum_{k=1}^3 (t - 0.5)^k + 2.5 \\sum_{j=1}^4 (-1)^j \\phi_j(t) $$\nwhere the $\\phi_j$ are given by the Gaussian RBF:\n$$ \\phi_j(x) = \\exp \\left( - \\left( \\epsilon \\left( x - \\psi \\right) \\right)^2 \\right) $$\nwhere $\\psi$ is the center point and $\\epsilon$ is the bandwidth. Here we set $\\psi=0.2, 0.4, 0.6, 0.8$ and $\\epsilon=50$. We take the inputs as $t_i \\sim \\text{Uniform}(0, 1)$\nimport numpy as np import matplotlib.pyplot as plt from functools import partial import cvxpy as cp import scipy rng = np.random.default_rng() t_i = rng.uniform(0, 1, 500) t_i.sort() def rbf(x, center, epsilon): return np.exp(-((epsilon * (x - center)) ** 2)) rbfs = [partial(rbf, center=i, epsilon=50) for i in [0.2, 0.4, 0.6, 0.8]] true_y = 6 * np.sum([(t_i - 0.5) ** k for k in [1, 2, 3]], axis=0) + 2.5 * np.sum( [(-1) ** j * rbfs[j - 1](t_i) for j in [1, 2, 3, 4]], axis=0 ) obs_y = true_y + rng.normal(0, 0.5**2, len(t_i)) plt.plot(t_i, true_y, color=\u0026#34;r\u0026#34;) plt.scatter(t_i, obs_y, alpha=0.3, color=\u0026#34;black\u0026#34;) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.legend([\u0026#34;True Function\u0026#34;, \u0026#34;Observed Values\u0026#34;]) Trend Filtering as a Basis Expansion The first useful way to look at trend filtering is via a basis expansion perspective exactly like smoothing splines. Tibshirani prove that trend filtering has a basis function expansion given in equation (25), which I recreate here. Politsch et al. call this a falling factorial basis because of the iterated multiplicative form they take. Altering Tibshirani\u0026rsquo;s notation slightly:\n$$ \\begin{gather} j = 1, 2, \\ldots, n \\newline h_j(x) = x^{j-1} \\quad \\text{when} \\quad j \\leq k+1 \\newline h_j(x) = \\prod_{\\ell=1}^k (x - x_{j-k-1+\\ell}) * \\mathbb{1}(x \\ge x_{j-1}) \\quad \\text{when} \\quad j \\ge k+2 \\end{gather} $$\nwhere $n$ gives the total number of input points, and $k$ is the order of the trend filtering method. This number is related to the order of a smoothing spline in that it determines the degree of the piecewise polynomial that gets fitted to each segment of the input space. $k=0$ corresponds to a piecewise constant basis, $k=1$ to piecewise linear, and so on.\nPython ranges generating sequences from 0 to $n-1$ so let\u0026rsquo;s adjust our $j$ index to account for this:\n$$ \\begin{gather} i = j - 1 = 0, 1, \\ldots, n-1 \\newline h_i(x) = x^i \\quad \\text{when} \\quad i \\le k \\newline h_i(x) = \\prod_{\\ell=1}^k (x - x_{i-k+\\ell}) * \\mathbb{1}(x \\ge x_{i}) = \\prod_{\\ell=0}^{k-1} (x - x_{i-k+\\ell+1}) * \\mathbb{1}(x \\ge x_{i})\\quad \\text{when} \\quad i \\ge k+1 \\end{gather} $$\nHowever, Python lists are also zero-indexed so we need another adjustment on our subscripts to account for that too: $$ h_i(x) = \\prod_{\\ell=0}^{k-1} (x - x_{i-k+\\ell}) * \\mathbb{1}(x \\ge x_{i-1})\\quad \\text{when} \\quad i \\ge k+1 $$\ndef tf_basis(x, k, x_i): results = [] n = len(x_i) for i in range(n): if i \u0026lt;= k: h = x**i results.append(h) elif i \u0026gt;= k + 1: h = 1 for ell in range(k): h *= x - x_i[i - k + ell] h *= int(x \u0026gt;= x_i[i - 1]) results.append(h) return results Now, we can visualize this on some sample points. Here we set $k=2$, take our data to be $x_i=0, 0.2, 0.4, 0.6, 0.8, 1$, and plot the basis functions evaluated over the range $[0, 1]$:\nx_i = [0, 0.2, 0.4, 0.6, 0.8, 1] xs = np.linspace(0, 1, 100) test = np.array([tf_basis(x, 2, x_i) for x in xs]) for n in range(test.shape[1]): plt.plot(xs, test[:, n]) for x in x_i: plt.axvline(x, alpha=0.25, color=\u0026#34;black\u0026#34;) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.title(\u0026#34;k = 2\u0026#34;) We can also see how changing $k$ changes the type of basis functions we get. Namely, if we set $k=0$ then we get a piecewise constant set of basis functions:\ntest = np.array([tf_basis(x, 0, x_i) for x in xs]) for n in range(test.shape[1]): plt.plot(xs, test[:, n]) for x in x_i: plt.axvline(x, alpha=0.25, color=\u0026#34;black\u0026#34;) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.title(\u0026#34;k = 0\u0026#34;) Fitting the Basis Function Approach Now, we can express out model in the typical way as a function of the basis components:\n$$ f(x) = \\sum_{i=1}^n \\alpha_i h_i(x) $$\nwhere $\\alpha_i$ are our unknown coefficients. To fit this model we can form a design matrix where each row is the vector of the basis function outputs evaluated at the input point:\n$$ \\mathbf{X}_{i,j} = h_j(x_i) \\quad \\text{where} \\quad i = 1, \\ldots, n \\quad j = 1, \\ldots, n $$\nwhich gives us an $n \\times n$ matrix. We can then form an $n \\times 1$ coefficient vector and our regression becomes:\n$$ y = \\mathbf{X} \\beta + \\epsilon $$\nwhere if we assume that $\\epsilon \\sim \\text{Normal}(0, \\sigma^2)$ then we can use the standard least squares approach to find $\\beta$. Note that because of the form of the basis expansion the first column of $\\mathbf{X}$ will be all ones so we don\u0026rsquo;t need to include a seperate intercept term.\nWe can also include an $\\ell_1$ regularization term to our $\\beta$ vector to promote sparsity, which gives us the objective function:\n$$ \\hat{\\beta} = \\mathop{\\arg \\min}\\limits_{\\beta} \\frac{1}{2} \\Vert{y - \\mathbf{X}\\beta} \\Vert_2^2 + \\lambda \\Vert \\beta \\Vert_1 $$\nLetting $k=1$ for a piecewise linear basis, and $\\lambda = 0.01$, we can easily fit this model using cvxpy because our objective is convex:\nk = 1 basis_eval = np.array([tf_basis(t, k, t_i) for t in t_i]) beta = cp.Variable(len(t_i)) objective = cp.Minimize( 0.5 * cp.sum_squares(obs_y - basis_eval @ beta) + 0.01 * cp.norm(beta[k + 1 :], 1) ) prob = cp.Problem(objective) results = prob.solve(solver=\u0026#34;CLARABEL\u0026#34;, verbose=False) predict = basis_eval @ beta.value.round(2) plt.plot(t_i, predict, color=\u0026#34;black\u0026#34;) plt.scatter(t_i, obs_y, alpha=0.4) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.legend([\u0026#34;Fitted Function\u0026#34;, \u0026#34;Observed Values\u0026#34;]) plt.title(\u0026#34;k=1, lambda = 0.01\u0026#34;) We can look at our fitted coefficients to see how many non-zero coefficients we get:\nsum(np.abs(beta.value.round(2)) \u0026gt; 0) 33 We can also see what happens when we increase our regularization parameter to $\\lambda=5$:\nk = 1 basis_eval = np.array([tf_basis(t, k, t_i) for t in t_i]) beta = cp.Variable(len(t_i)) objective = cp.Minimize( 0.5 * cp.sum_squares(obs_y - basis_eval @ beta) + 5 * cp.norm(beta[k + 1 :], 1) ) prob = cp.Problem(objective) results = prob.solve(solver=\u0026#34;CLARABEL\u0026#34;, verbose=False) predict = basis_eval @ beta.value.round(2) plt.plot(t_i, predict, color=\u0026#34;black\u0026#34;) plt.scatter(t_i, obs_y, alpha=0.4) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.legend([\u0026#34;Fitted Function\u0026#34;, \u0026#34;Observed Values\u0026#34;]) plt.title(\u0026#34;k=1, lambda = 5\u0026#34;) sum(np.abs(beta.value.round(2)) \u0026gt; 0) 3 We see a dramatically fewer number of non-zero coefficients, and we can also see one of the nice features of trend filtering: the \u0026ldquo;knots\u0026rdquo; of the basis functions are chosen automatically, in a sense. If we look at where the $\\beta$ coefficients are non-zero:\nnp.where(np.abs(beta.value.round(2)) \u0026gt; 0) array([ 0, 1, 281]) We can see that it\u0026rsquo;s the first column, the intercept, and then another value along the domain of our input data. Essentially, the model has chosen to fit a piecewise linear function from the first value to this intermediate value, and another piecewise linear function over the rest of the domain. We can add the respective $x$-values of these points to our plot to see this:\npredict = basis_eval @ beta.value.round(2) plt.plot(t_i, predict, color=\u0026#34;black\u0026#34;) plt.scatter(t_i, obs_y, alpha=0.4) for i in np.where(np.abs(beta.value).round(2) \u0026gt; 0)[0]: plt.axvline(t_i[i], color=\u0026#34;red\u0026#34;, alpha=0.25) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.legend([\u0026#34;Fitted Function\u0026#34;, \u0026#34;Observed Values\u0026#34;, \u0026#34;Knots\u0026#34;]) plt.title(\u0026#34;k=1, lambda = 5\u0026#34;) Where you can see that values where the $\\beta$ coefficients are non-zero are the same places where the slope of the lines changes. As the regularization parameter gets smaller, more and more knot points are selected, creating more and more piecewise functions over different segments of the input domain. This is a huge plus for trend filtering compared to smoothing splines: the model selects the knot points automatically.\nDifference Operator Approach Now, the basis function approach is easy to interpret and understand, but it is very computationally expensive because we have to evaluate every basis function at every point. The size of our design matrix scales with the square of the number of inputs, which is untenable for even moderately sized datasets.\nLuckily, there is an easier way: we can still fit a unique parameter to each input data point, but we can adjust our regularization penalty so that it constrains adjacent coefficients to be the same as each other. As Tibshirani proves, this process is equivalent to the basis function approach, and, as we shall see, grants certain sparsity properties that make the model fitting process much faster.\nThe first key to this is the difference operator matrix. This is what defines our regularization constraint. Let\u0026rsquo;s first look at the case where $k=0$. Here, we want to constrain the first difference of our coefficients:\n$$ \\sum_{i=1}^{n-1} \\vert \\beta_i - \\beta_{i+1} \\vert $$\nand we can achieve this by creating this matrix given by equation (3) in Tibshirani:\n$$ D^{(1)} = \\begin{bmatrix} -1 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; -1 \u0026amp; 1 \\end{bmatrix} \\in \\mathbb{R}^{(n-1) \\times n} $$\nand then we can see that:\n$$ \\Vert D^{(1)} \\beta \\Vert_1 = \\sum_{i=1}^{n-1} \\vert \\beta_i - \\beta_{i+1} \\vert $$\nDefining our constraint like this gives us a huge computational speedup because the $D$ matrix is banded, meaning only specific values along the diagonal are non-zero and the rest are all exactly zero. This means that smart optimization algorithms can ignore the computations where it knows that there is a multiplication by zero, which don\u0026rsquo;t contribute anything to the objective value.\nThe first order difference matrix gives us the piecewise-constant basis, but you can construct higher order differences to get other bases for higher values of $k$. For instance, the second difference matrix when $k=1$ looks like this:\n$$ D^{(2)} = \\begin{bmatrix} 1 \u0026amp; -2 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; -2 \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 \u0026amp; -2 \u0026amp; 1 \\end{bmatrix} \\in \\mathbb{R}^{(n-2) \\times n} $$\nThere is one additional subtlety: these matrices only work when we have evenly space inputs, which is very often not the case in practice. In the supplement to Tibshirani (2014), an adjustment to these difference matrices is provided to account for unevenly space inputs. First, note that we can define the above difference matrices recursively:\n$$ D^{(k+1)} = \\hat{D}^{(1)} \\cdot D^{(k)} $$\nwhere $\\hat{D}^{(1)}$ in this formula is the $(n-k-1) \\times (n-k)$ version of the $D^{(1)}$ matrix we defined above. So, for instance, if $k=1$ then we have:\n$$ D^{(k+1)} = D^{(2)} = \\hat{D}^{(1)} \\cdot D^{(1)} $$\nwhere $\\hat{D}^{(1)}$ has dimensions $(n-2) \\times (n-1)$ and $D^{(1)}$ has dimension $(n-1) \\times (n)$ so in the end we get a matrix with dimension $(n-2) \\times (n)$ which is the desired form.\nThen, the adjustment for non-even spacing is given as:\n$$ D^{(k+1)} = \\hat{D}^{(1)} \\cdot \\text{diag} \\left( \\frac{k}{x_{k+1} - x_1}, \\frac{k}{x_{k+2} - x_2}, \\ldots, \\frac{k}{x_n - x_{n-k}} \\right) \\cdot D^{(k)} $$\nSo, in our case of $k=1$ we have:\n$$ D^{(2)} = \\hat{D}^{(1)} \\cdot \\text{diag} \\left( \\frac{k}{x_2 - x_1}, \\frac{k}{x_3 - x_2}, \\ldots, \\frac{k}{x_n - x_{n-1}} \\right) \\cdot D^{(1)} $$\nNote that the case where $k=0$ requires no spacing adjustment.\nWe can make a nice recursive function that computes this matrix as follows:\ndef make_D_matrix(xs: np.ndarray, k: int, uneven: bool): n = xs.shape[0] ones = np.ones(n) if k == 0: return scipy.sparse.spdiags( np.vstack([-ones, ones]), range(2), m=n - k - 1, n=n ) else: d_hat_1 = scipy.sparse.spdiags( np.vstack([-ones, ones]), range(2), m=n - k - 1, n=n - k ) if uneven: return scipy.sparse.bsr_array( d_hat_1 @ np.diag(1 / (t_i[k:] - t_i[:-k])) @ make_D_matrix(xs, k - 1, uneven) ) else: return d_hat_1 @ make_D_matrix(xs, k - 1, uneven) D = make_D_matrix(t_i, k=1, uneven=True) We can then define our objective function as:\n$$ \\hat{\\beta} = \\mathop{\\arg \\min}\\limits_{\\beta} \\frac{1}{2} \\Vert y - \\beta \\Vert_2^2 + \\lambda \\Vert D^{(k+1)} \\beta \\Vert_1 $$\nbeta = cp.Variable(len(t_i)) objective = cp.Minimize( 0.5 * cp.sum_squares(obs_y - beta) + 5 * cp.norm(D @ beta, 1) ) prob = cp.Problem(objective) results = prob.solve(solver=\u0026#34;CLARABEL\u0026#34;) We can see that we recover the same piecewise linear solution as we did above, although with one fewer breakpoint and a slightly different value for the other, likely because of small numerical differences:\nplt.plot(t_i, beta.value.round(2), color=\u0026#34;black\u0026#34;) plt.scatter(t_i, obs_y, alpha=0.4) for i in np.where(np.abs(D @ beta.value).round(2) \u0026gt; 0)[0]: plt.axvline(t_i[i], color=\u0026#34;red\u0026#34;, alpha=0.25) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.legend([\u0026#34;Fitted Function\u0026#34;, \u0026#34;Observed Values\u0026#34;, \u0026#34;Knots\u0026#34;]) plt.title(\u0026#34;k=1, lambda = 5\u0026#34;) np.where(np.abs(D @ beta.value).round(2) \u0026gt; 0) array([279]) We can also look at what a piecewise constant approach, with $k=0$, looks like:\n# Note that the uneven parameter is redundant here D = make_D_matrix(t_i, k=0, uneven=True) beta = cp.Variable(len(t_i)) objective = cp.Minimize( 0.5 * cp.sum_squares(obs_y - beta) + 5 * cp.norm(D @ beta, 1) ) prob = cp.Problem(objective) results = prob.solve(solver=\u0026#34;CLARABEL\u0026#34;) We can see that maintaining the same $\\lambda$ value gives us a function with many more degrees of freedom, but each segment is a constant value:\nplt.plot(t_i, beta.value.round(2), color=\u0026#34;black\u0026#34;) plt.scatter(t_i, obs_y, alpha=0.4) for i in np.where(np.abs(D @ beta.value).round(2) \u0026gt; 0)[0]: plt.axvline(t_i[i], color=\u0026#34;red\u0026#34;, alpha=0.25) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.legend([\u0026#34;Fitted Function\u0026#34;, \u0026#34;Observed Values\u0026#34;, \u0026#34;Knots\u0026#34;]) plt.title(\u0026#34;k=0, lambda = 5\u0026#34;) Final Notes From my experimentation the difference matrix approach scales very well when you either have $k=0$ or evenly space inputs. If you have unevenly spaced inputs and set $k \\ge 1$ then the optimization can fail depending on your regularization coefficient. This is because the scaling constants added in for the uneven spacing case can cause the result difference matrix to have quite large values. You can see this from the terms, where each is the multiplicative inverse of the difference between consecutive observed $x$ values. In cases where the difference is small, these values end up being large, which causes numerical instability when doing the optimization.\nHowever, sticking to the evenly-spaced case, or the piecewise constant case, results in optimizations that can be solved on the order of seconds even for more than 100,000 input points. Given this is a non-parametric approach the computational scaling is surprisingly nice, especially since I\u0026rsquo;m using an off-the-shelf optimizer. Tibshirani and some other authors have implemented custom optimization procedures that could very well outperform what I\u0026rsquo;m doing here.\nAll-in-all I find this method to be theoretically quite appealing, from the minimax convergence properties to the \u0026ldquo;automatic\u0026rdquo; knot selection. Add to this an acceptable computational scaling, and this seems very promising to me, and I find it surprising that I\u0026rsquo;m just now hearing about it. Admittedly, there are a number of practical implementation issues I can see:\nDoing out-of-sample predictions, whether new values inside of the training domain, or for values outside of it Handling real data where we have possibly many observations for a single given $x$ value Extension to the multivariate case Moving from MSE loss to the general maximum likelihood case Luckily, I think these are all issues that can be taken care of with enough clever software engineering.\nReferences Collin A Politsch, Jessi Cisewski-Kehe, Rupert A C Croft, Larry Wasserman, Trend filtering  I. A modern statistical tool for time-domain astronomy and astronomical spectroscopy, Monthly Notices of the Royal Astronomical Society, Volume 492, Issue 3, March 2020, Pages 40054018, https://doi.org/10.1093/mnras/staa106\nRyan J. Tibshirani. \u0026ldquo;Adaptive piecewise polynomial estimation via trend filtering.\u0026rdquo; The Annals of Statistics, 42(1) 285-323 February 2014. https://doi.org/10.1214/13-AOS1189\n","permalink":"http://optionallybayes.com/posts/trend_filter/","summary":"Fitting a Trend Filtering Model Using CVXPY","title":"Trend Filtering using CVXPY"},{"content":"Inspired by Ole Peters\u0026rsquo; lovely blog post about this game and ergodicity breaking, I wanted to dive a little deeper into the mathematics of this to deepen my understanding. This concept is deep and subtle, so this simple game is a good place to start.\nDescription \u0026amp; Properties of the Game This coin flip game goes as follows: each round a coin is flipped with probability $p$ of landing heads, and when it comes heads you get a return of $G$ and when it lands tails you get a return of $L$. In other words, each round with probability $p$ your wealth is multiplied by a factor of $1+G$ and with probability $1-p$ your wealth is multiplied by a factor of $1+L$.\nWe can then start working out some basic properties of this game.\nFirst, given you\u0026rsquo;ve played $t$ rounds of the game, the probability that you win $w$ times is given by the binomial distribution: $$ P(t, w; p) = {t \\choose w} p^w (1-p)^{t-w} $$\nNext, given you have won $w$ times after player $t$ rounds, your terminal wealth is given by: $$ R(t, w; G, L) = (1+G)^w (1+L)^{t-w} $$ here, without loss of generality, we assume here that starting wealth is 1.\nNow, what can we say about expected wealth of a player after $t$ rounds? Following Peters, we have to differentiate between what he calls the ensemble average and the time average. Let\u0026rsquo;s start with the ensemble average. Here, this quantity matches what one usually thinks of when talking about mathematical expectation: multiply the probability of a game path by its terminal wealth. After $t$ rounds there are $t+1$ possible paths, each corresponding to the number of times a player could have won: $0$ to $t$ times. Now, given the above formulas, we can find this expectation, suppressing notation for the constant parameters $p$, $G$, and $L$: $$ \\text{EA}(t) = E_W \\left[ R(t, w) \\right] = \\sum_{w=0}^t P(t, w) R(t, w) $$\nLet\u0026rsquo;s consider the time average. As opposed to the ensemble average, which considers the possibilities over all possible paths, this average considers the expected behavior of a single path. We know from the properties of the binomial distribution that after $t$ rounds the expected number of wins is given by $pt$. So, for any single player, we expect that player to have won $pt$ times after playing for $t$ rounds. We can then work out this single players expected terminal wealth at that point: $$ \\text{TA}(t) = R(t, pt) = (1+G)^{pt} (1+L)^{t - pt} = ((1+G)^p (1+L)^{1-p})^t $$\nFrom this equation we can see that the term $(1+G)^p(1+L)^{1-p}$ determines the behavior of the time average as $t \\to \\infty$. If this term is $\u0026gt; 1$ then a player\u0026rsquo;s wealth will tend towards $\\infty$, if it\u0026rsquo;s $=1$ then the player is expected to breakeven, and if it\u0026rsquo;s $\u0026lt;1$ then the players wealth will tend towards 0. This means that we can determine, given either the loss or gain return, what the other must be so that the player is expected to break even. For example, given the winning return: $$ (1+G)^p (1+L)^{1-p} = 1 $$ $$ L = (1+G)^{\\frac{-p}{1-p}} - 1 $$\nErgodicity Breaking The entire idea of ergodicity breaking comes from the differences between these two formulas, namely the fact that they aren\u0026rsquo;t equal. The ensemble average can be interpreted as the average wealth of a group of players, as the number of people playing tends towards infinity. The time average, however, tells us what we expect the terminal wealth of any single player from that group to look like. In an ergodic system, these things are equal. This is to say that you can work out the properties of the entire system by following a single player, given enough time. However, that isn\u0026rsquo;t the case here. The system as a whole has a different behavior than that of any single player. This isn\u0026rsquo;t completely obvious from the formulas alone, but by running some game simulations, we can see how the ensemble and time averages diverge dramatically.\nFirst, we can encode the equations defined above:\nimport numpy as np from scipy.special import comb from scipy import stats import matplotlib.pyplot as plt rng = np.random.default_rng() def P(t, w, p): return comb(t, w) * p**w * (1 - p) ** (t - w) def R(t, w, G, L): return (1 + G) ** w * (1 + L) ** (t - w) def EA(t, G, L, p): sum = 0 for w in range(t + 1): sum += P(t, w, p) * R(t, w, G, L) return sum def TA(t, G, L, p): return ((1 + G) ** p * (1 + L) ** (1 - p)) ** t Then we can simulate game paths. My approach here is to generate a sequence of integers uniformly from the set ${0, 1}$ and treat each $1$ as a win and each $0$ as a loss. The wealth over time is then simply given by the cumulative product of the respective gain/loss factors:\ndef sim_path(t, G, L, p): game_hist = rng.choice([1.0, 0.0], t, p=[p, 1 - p]) game_hist[game_hist == 1] = 1 + G game_hist[game_hist == 0] = 1 + L return np.cumprod(game_hist) Setting the gain return to $50%$, the loss return to $-40%$, and of course we\u0026rsquo;ll treat this coin as fair so $p=0.5%$. We can then plot 50 different wealth paths after having played 1000 rounds of the game, along with our ensemble and time averages.\nG = 0.5 L = -0.4 p = 0.5 plt.plot( np.array([sim_path(1000, G, L, p) for x in range(50)]).T, color=\u0026#34;black\u0026#34;, alpha=0.15 ) plt.plot([EA(t, G, L, p) for t in range(1000)], color=\u0026#34;red\u0026#34;, label=\u0026#34;EA(t)\u0026#34;) plt.plot([TA(t, G, L, p) for t in range(1000)], color=\u0026#34;blue\u0026#34;, label=\u0026#34;TA(t)\u0026#34;) plt.axhline(1, color=\u0026#34;black\u0026#34;, linestyle=\u0026#34;dashed\u0026#34;) plt.yscale(\u0026#34;log\u0026#34;) plt.legend() plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;Terminal Wealth\u0026#34;) We can see right away the strangeness. All of the game paths end up losing money, following the negatively sloped time average line. However, the ensemble average tells us that the average wealth should be increasing! What gives? How can all of the players individually be expected to lose wealth over time, yet the ensemble average tells us that on the whole, the group should have increasing wealth? This is the insane mystery of ergodicity breaking.\nGoing Deeper Luckily for us, this game has simple mathematics, so we can get some nice theoretical results to shore up our intuition and understand what\u0026rsquo;s going on here.\nFirst of all, let\u0026rsquo;s see how many wins we need to get so that we expect to at least break even, in other words, we want $w$ such that $R(t, w) \\ge 1$. Doing some simple algebra, we can see that: $$ w \\ge -t \\frac{\\ln(1+L)}{\\ln(1+G) - \\ln(1+L)} := w_{\\text{BE}} $$\nFrom this we can work out the probability using the binominal distribution CDF, denoted $F$. Letting $W$ denote the random variable of how many wins we\u0026rsquo;ve got, we can see: $$ \\text{P}(W \\ge w_{\\text{BE}}) = 1 - F(w_{\\text{BE}}) $$\nWe can plot this probability over $t$ as follows:\ndef w_BE(t, G, L): return -t * np.log(1 + L) / (np.log(1 + G) - np.log(1 + L)) plt.plot([1 - stats.binom.cdf(k=w_BE(n, G, L), n=n, p=p) for n in range(1, 1000)]) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;Probability\u0026#34;) We can see that as $t$ increases, the probability of at least breaking even decreases to 0. This explains why our time average has negative slope: as you play more and more, the probability that you end up at least breaking even asymptotically approaches zero. However, this does not explain why our ensemble average has positive slope. Let\u0026rsquo;s move to that next.\nFirst, let\u0026rsquo;s look at the average wealth of paths that are at least breaking even compared to the wealth of paths that are not. At a given time $t$, the function below computes the average wealth of all the possible game paths that are at or above break even, and the same for paths below break even.\ndef calc_wealths(t, G, L): w_be = w_BE(t, G, L) winning_wealth = [] losing_wealth = [] for w in range(t + 1): if w \u0026lt; w_be: losing_wealth.append(R(t, w, G, L)) else: winning_wealth.append(R(t, w, G, L)) return np.mean(winning_wealth), np.mean(losing_wealth) wealths = [calc_wealths(t, G, L) for t in range(1, 100)] plt.plot([x[0] for x in wealths]) plt.plot([x[1] for x in wealths]) plt.legend([\u0026#34;Winning Paths\u0026#34;, \u0026#34;Losing Paths\u0026#34;]) plt.yscale(\u0026#34;log\u0026#34;) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;Average Wealth\u0026#34;) We can see that the average wealth of winning paths is exponentially increasing (log-scale axes), while the average wealth of losing paths tends towards 0, as we would expect. This gives us a clue to the phenomena: the wealth of those winning paths is increasing so quickly that they are able to overcome the ever smaller probability of their occurrence, so when we aggregate them into the ensemble expectation, they alone are influential enough that it becomes positive. We can see this more clearly by looking at the relative contribution of each path to the ensemble average.\nFrom $\\text{EA}(t)$ we can see that it is the sum of the probability-weighted terminal wealth of each possible path. We can therefore compute the contribution of each term over the range of possible $w$ values, which is what we do below:\nt = 100 results = [] for w in range(t+1): path_prob = P(t, w, p) path_wealth = R(t, w, G, L) results.append(path_prob * path_wealth) Then, we can plot this curve, which we normalize so that each term is given as the percentage of the total, as well as include the line indicating the break-even number of wins $w_{\\text{BE}}$ computed above:\nplt.plot(range(t+1), np.array(results) / np.sum(results)) plt.axvline(w_BE(t, G, L), color=\u0026#34;black\u0026#34;) plt.legend([\u0026#34;Contribution\u0026#34;, \u0026#34;Break Even Wins\u0026#34;]) plt.xlabel(\u0026#34;Number of Wins\u0026#34;) plt.ylabel(\u0026#34;% of Total Expectation\u0026#34;) Here, we can see that the paths that contribute the most to the ensemble average are the winning paths! This is what causes the ensemble average to be positive: the biggest contributors to it are those paths that have gained wealth.\nOne interesting thing about this graph is that it\u0026rsquo;s not the paths that have won the absolute most that contribute heavily, but those that win just enough. Before I plotted this, I thought that the biggest contributors would be those paths with the largest number of wins, all the way to the right on the x-axis of this graph. The paths that say win more than 90% of the time have wealths that grow so exceptionally fast that I thought it would more than outweigh their exceptionally low probability of occurring. However, this is not the case!\nIt turns out that the probability-weighted terminal wealth simplifies nicely: $$ P(t, w) R(t, w) = {t \\choose w} p^w (1-p)^{t-w} (1+g)^w (1+L)^{t-w} $$ $$ = {t \\choose w} (p(1+G))^w ((1-p)(1+L))^{t-w} $$ which looks very much like a rescaled binominal distribution. So, we can work out its mode in a very similar way. This procedure gives us: $$ w_\\text{mode}(t) = \\frac{(1-p)(1+L) - t p (1+G)}{-p(1+G) - (1-p)(1+L)} $$ there is some subtlety around the fact that $w$ need be an integer that I\u0026rsquo;m ignoring here for simplicity. Regardless, we can add this line to our plot:\nmode = ((1-p) * (1+L) - t * p * (1+G)) / (-p*(1+G) - (1-p)*(1+L)) plt.plot(range(t+1), np.array(results) / np.sum(results)) plt.axvline(w_BE(t, G, L), color=\u0026#34;black\u0026#34;) plt.axvline(mode, linestyle=\u0026#34;dashed\u0026#34;, color=\u0026#34;black\u0026#34;) plt.legend([\u0026#34;Contribution\u0026#34;, \u0026#34;Break Even Wins\u0026#34;, \u0026#34;Mode\u0026#34;]) plt.xlabel(\u0026#34;Number of Wins\u0026#34;) plt.ylabel(\u0026#34;% of Total Expectation\u0026#34;) We can go slightly further and convert this into a winning percentage by taking: $$ \\lim_{t \\to \\infty} \\frac{w_\\text{mode}(t)}{t} = \\lim_{t \\to \\infty} \\frac{(1-p)(1+L) - t p (1+G)}{-t p (1+G) - t (1-p)(1+L)} $$ $$ = \\lim_{t \\to \\infty} \\frac{\\frac{(1-p)(1+L) - t p (1+G)}{t} - p (1+G)}{-p(1+G) - (1-p)(1+L)} = \\frac{p(1+G)}{p(1+G) + (1-p)(1+L)} $$\npeak_win_pct = (p * (1+G)) / (p*(1+G) + (1-p)*(1+L)) print(peak_win_pct) 0.7142857142857143 which is about 70% with our parameters. So, we can say that really it\u0026rsquo;s the game paths that win around 70% of the time that are the ones that really push our ensemble average to be positive versus our time average.\nLet\u0026rsquo;s do one final exercise to see this effect a different way. We can compare the growth rates of the probability of a path and it\u0026rsquo;s wealth. This function computes the cumulative percentage change of those quantities in log space, in other words, their geometric growth rate. We take absolute values here because for the values we consider probabilities are decreasing and wealths are increasing, and we want to compare them on the same scale.\ndef prb_wealth_curves(win_pct, p, G, L): prbs = [] wealths = [] for t in range(1, 250): w = win_pct * t prbs.append(P(t, w, p)) wealths.append(R(t, w, G, L)) prbs = np.array(prbs) wealths = np.array(wealths) return ( np.abs(np.cumsum(np.diff(np.log(prbs), n=1))), np.abs(np.cumsum(np.diff(np.log(wealths), n=1))) ) First, we can look at these curve for the case where we win 70% of the time:\nprbs, wealths = prb_wealth_curves(0.70, 0.5, G, L) plt.plot(prbs) plt.plot(wealths) plt.legend([\u0026#34;Probability\u0026#34;, \u0026#34;Wealth\u0026#34;]) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;Cumulative Growth\u0026#34;) We see that at first, the probabilities are decreasing faster than wealth is increasing, but eventually wealth starts growing faster than the probabilities are decreasing, resulting in a large positive contribution to the ensemble average, as we saw above.\nNow, let\u0026rsquo;s look at paths that win 90% of the time:\nprbs, wealths = prb_wealth_curves(0.9, 0.5, G, L) plt.plot(prbs) plt.plot(wealths) plt.legend([\u0026#34;Probability\u0026#34;, \u0026#34;Wealth\u0026#34;]) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;Cumulative Growth\u0026#34;) Here we can see that right away the probabilities are decreasing faster than wealths are increasing, and this relationship holds, and indeed becomes more extreme, as time progresses. This results in the small contribution to the ensemble average.\nNow, in the case where we are winning below the break even rate, both the probabilities and wealths are decreasing, resulting in a contribution that shrinks very rapidly.\nConclusion This behavior is strange and very unintuitive to me, but it has enormous implications for how we think about many processes in the real world. Essentially, any process that follows a multiplicative reward structure will have this non-ergodic property. Notably, many economic systems have this kind of structure. Returns on capital are nearly always multiplicative: stock returns, GDP changes, certain gambling games, etcetera.\nIt\u0026rsquo;s almost immediate to see how this idea relates to income inequality. An economic system can be growing when considered as a whole, but much of that aggregate growth is coming from the exceptionally huge gains of an increasingly smaller subset of the people as time progresses. Obviously, in the real world the situation is much more complicated, but the principle is there. It\u0026rsquo;s interesting to think about the ultra-wealthy as nothing more than those people that happened to have gotten lucky with their investments compared to others. In fact, there is a noteworthy paper, \u0026ldquo;Wealth condensation in a simple model of economy,\u0026rdquo; that talks about this effect.\nNotably for financial economics, this exact same effect occurs if you move from discrete time to continuous time by analyzing Geometric Brownian Motion, and I\u0026rsquo;d like to get around to deriving how to move from a simple game like this to its continuous time counterpart. Stay tuned!\n","permalink":"http://optionallybayes.com/posts/ergo_breaking/","summary":"Revealing the strangeness of ergodicity breaking with a simple game","title":"Ergodicity Breaking and the Coin Toss Game"},{"content":"I\u0026rsquo;ve been taking a class about stochastic calculus, so in this post I want to explore some basic monte carlo simulations of Brownian Motion and two fundamental stochastic differential equations.\nBrownian Motion First, we have to simulate the fundmanetal process that drives most of stochastic calculus: Brownian Motion.\nA stochastic process $W$ is Brownian Motion if the following hold:\n$ W_0 = 0 $ $ W $ has independent increments, i.e. if $r \u0026lt; s \\le t \u0026lt; u$ then $W_u - W_t$ and $W_s - W_r$ are independent random variables For $s\u0026lt;t$ the random variable $W_t - W_s$ is normally distributed with $\\mu=0$ and $\\sigma^2=t-s$ $W$ is continuous Proving that a process like this exists is possible, but very hard, especially property 4, but we will take it as given.\nSo, given these properties, how do we generate sample paths? Well let\u0026rsquo;s define $T$ to be the terminal time we want to simulate till and $n$ be the number of sample points we want to generate. Then we have a set of times $t_0 = 0 \u0026lt; t_1 \u0026lt; \\ldots \u0026lt; t_{n-1} \u0026lt; t_n=T$ where $t_n - t_{n-1} = \\frac{T}{n-1}$. So, from property (3) in the definition we know that $W_{t_n} - W_{t_{n-1}} \\sim \\mathcal{N}(0, \\frac{T}{n-1})$. If we define a set of independent random variables $Z_r \\sim \\mathcal{N}(0, \\frac{T}{n-1})$ then we can say that $W_{t_n} - W_{t_{n-1}} = Z_{t_n}$, or $W_{t_n} = Z_{t_n} + W_{t_{n-1}}$. Then using property (1) in the definition we get (taking liberties with the time subscript) that $W_0 = 0$ and $W_1 = Z_1 + W_0 = Z_1$, $W_2 = Z_1 + W_1 = Z + Z_2$, $W_3 = Z_3 + W_2 = Z_3 + Z_2 + Z_1$, and so on. In other words, $W_t$ is just a sum of independent normal random variables with $\\mu = 0$ and $\\sigma^2 = \\frac{T}{n-1}$. So, to generate $n$ samples of Brownian Motion from time $t=0$ to $t=T$ we simply need to compute the cumulative sum of $n-1$ samples from a normal distribution with $\\mu = 0$ and $\\sigma^2 = \\frac{T}{n-1}$, which is what the function below does.\nimport numpy as np import matplotlib.pyplot as plt plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,10) rng = np.random.default_rng() def BM_gen(T, n): # Variance scales with difference in time between each sample point # dividing by (n-1) because of fencepost-counting sigma_sq = T/(n-1) t = np.linspace(0, T, n) norm_sample = rng.normal(0, np.sqrt(sigma_sq), n-1) # Brownian motion assumed to start at 0 at time t=0 W_t = np.append([0], norm_sample.cumsum()) return (t, W_t) And then we can simulate a few sample paths and plot them. Note that $T=2$ and $n=1000$. As $n \\rightarrow \\infty$ the sample path converges to true continous Brownian Motion, these are all discrete approximations.\nt, bm = BM_gen(2, 1000) _, bm2 = BM_gen(2, 1000) _, bm3 = BM_gen(2, 1000) plt.plot(t, bm) plt.plot(t, bm2) plt.plot(t, bm3) plt.title(\u0026#34;Brownian Motion Sample Paths\u0026#34;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Value\u0026#34;) Geometric Brownian Motion Now we move on to what is probably the most widely applied stochastic process, Geometric Brownian Motion:\n$ dX_t = \\mu X_t dt + \\sigma X_t dW_t $\n$X_0 = x_0$\nUsing methods of stochastic calculus, one can arrive at the following solution:\n$ X_t = x_0 \\exp((\\mu - \\frac{\\sigma^2}{2}) t + \\sigma W_t) $.\nWhich has expectation\n$E[X_t] = x_0 e^{\\mu t}$\nNote one important property of this solution is that it always has the same sign as the initial condition. This makes it useful for modelling stock prices as they need to be always positive.\nBecause of this simple formula, it\u0026rsquo;s very easy to simulate sample paths of Geometric Brownian Motion once you have a Brownian Motion sample path:\ndef GBM_gen(x_0, mu, sigma, T, n): t, W_t = BM_gen(T, n) X_t = x_0 * np.exp((mu - sigma**2/2) * t + sigma * W_t) return (t, X_t) And then we can again generate some sample paths and plot the expected value function:\nx_0 = 1 mu = 1 sigma = 0.4 t, gbm = GBM_gen(x_0, mu, sigma, 2, 1000) _, gbm2 = GBM_gen(x_0, mu, sigma, 2, 1000) _, gbm3 = GBM_gen(x_0, mu, sigma, 2, 1000) plt.plot(t, x_0 * np.exp(mu * t), color=\u0026#34;black\u0026#34;) plt.plot(t, gbm) plt.plot(t, gbm2) plt.plot(t, gbm3) plt.legend([\u0026#34;E(X_t)\u0026#34;]) plt.title(\u0026#34;Geometric Brownian Motion Sample Paths\u0026#34;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Value\u0026#34;) The Linear Stochastic Differential Equation Next is the Linear SDE:\n$dX_t = \\mu X_t dt + \\sigma dW_t$\n$X_0 = x_0$\nThis has the same drift term as GBM, but doesn\u0026rsquo;t scale it\u0026rsquo;s noise by the current value of the process. This has solution\n$ X_t = x_0 e^{\\mu t} + \\sigma \\int_{0}^{t} e^{\\mu(t-s)} dW_s $\nWith expectation\n$E[X_t] = x_0e^{\\mu t} $\nwhich we expect given the matching drift term to GBM.\nThis, unlike the GBM solution, includes a stochastic integral which must be estimated. In a similar vein to traditional RiemannStieltjes integrals, we can estimate stochastic integrals by\n$ \\int_{a}^{b} g_s dW_s = \\sum_{k=0}^{n-1} g_{t_k}(W_{t_{k+1}} - W_{t_k}) $\nwhere $g$ is some arbitrary function or process and each $t_k$ is a time partition like we defined in the section above on Brownian Motion. As $n \\rightarrow \\infty$ this sum converges to the integral. The function below estimates this sum given a function, a set of times, and a Brownian Motion sample path:\ndef stoch_int_est(g, t, W_t): # W_(t+1) - W_t # Duplicate last difference so that the length of this vector matches t # This is not technically correct, but is good enough forward_diff = np.append(W_t[1:] - W_t[:-1], W_t[-1] - W_t[-2]) func_vals = g(t, t[-1]) return (func_vals * forward_diff).cumsum() We can test that this function works by estimating a stochastic integral we know the solution to. The integral of Brownian Motion with respect to itself\n$ \\int_{0}^{t} W_s dW_s $\ncan be shown using Ito\u0026rsquo;s Lemma to have the solution\n$\\frac{W_t^2 - t}{2}$\nBelow, we plot a Brownian Motion sample path, the known solution of the integral of that path with respect to itself, and the estimate given by the above function:\nt, W_t = BM_gen(2, 1000) plt.plot(t, W_t) plt.plot(t, stoch_int_est(lambda s, t: W_t, t, W_t)) plt.plot(t, (W_t**2 - t)/2) plt.legend([\u0026#34;W_t\u0026#34;, \u0026#34;Est\u0026#34;, \u0026#34;Real\u0026#34;]) plt.title(\u0026#34;Stochastic Integrals\u0026#34;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Value\u0026#34;) You can see the estimate and the real solutions are quite close to each other, verifying the behavior of our estimation function. As we set $n$ to larger values, the convergence will improve. Now, we can use our estimate of the stochastic integral to create sample paths of the linear SDE:\ndef linear_sde_gen(x_0, mu, sigma, T, n): t, W_t = BM_gen(T, n) X_t = x_0 * np.exp(mu * t) + sigma * stoch_int_est(lambda s, t: np.exp(mu * (t - s)), t, W_t) return t, X_t And again we can simulate some sample paths along with their expectation:\nx_0 = 1 mu = 1 sigma = 0.4 t, linear_sde = linear_sde_gen(x_0, mu, sigma, 2, 1000) _, linear_sde2 = linear_sde_gen(x_0, mu, sigma, 2, 1000) _, linear_sde3 = linear_sde_gen(x_0, mu, sigma, 2, 1000) plt.plot(t, x_0 * np.exp(mu * t), color=\u0026#34;black\u0026#34;) plt.plot(t, linear_sde) plt.plot(t, linear_sde2) plt.plot(t, linear_sde3) plt.legend([\u0026#34;E(X_t)\u0026#34;]) plt.title(\u0026#34;Linear SDE Sample Paths\u0026#34;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Value\u0026#34;) As compared to GBM, we can see that firstly, the values can become negative even with a positive initial condition. Secondly, because the noise doesn\u0026rsquo;t scale with the value of the process, we can see that the noise becomes smaller as time increases.\nConclusion This was a basic introduction to simulation of stochastic processes along with an example of how to estimate stochastic integrals.\nReferences Bjrk, T. (2020). Arbitrage theory in continuous time. Oxford University Press.\n","permalink":"http://optionallybayes.com/posts/stoch_proc/","summary":"Simulating Brownian Motion and two common stochastic processes.","title":"Simulation of Stochastic Processes"},{"content":" ","permalink":"http://optionallybayes.com/posts/intprog_dfs/","summary":"Using Integer Programming to find Daily Fantasy Sports lineups with the highest expected points.","title":"Integer Programming for Daily Fantasy Sports"},{"content":" ","permalink":"http://optionallybayes.com/posts/vol_decomp/","summary":"Illustrating how realized volatility can decomposed into a continuous and jump component, providing new modelling opportunities.","title":"Decomposing Volatility: Continuous and Jump Component"},{"content":" ","permalink":"http://optionallybayes.com/posts/kelly_multiperiod/","summary":"Exploring how mean-variance methods and the Kelly Criterion compare in multi-period investments","title":"Exploring the Kelly Criterion: The Multi-Period Problem"},{"content":" ","permalink":"http://optionallybayes.com/posts/kelly_drawdown/","summary":"Exploring the Kelly Criterion and its drawdown properties using Monte Carlo","title":"Exploring the Kelly Criterion: Drawdown"},{"content":"In this post I want to apply clustering to ETFs to determine the most effective diversified portfolio. There are a lot of ETFs out there, in my database I count 3612, included delisted ones. If I wanted to build a simple, well-diversified portfolio to hold long-term, which ones should I pick? Would I do better with an S\u0026amp;P 500 ETF or a total stock market ETF? What kind of bond ETFs provide the most diversification to a portfolio? Clustering can help to answer these kinds of questions.\nData I\u0026rsquo;m immediately going to introduce one simplification to this problem, which is to restrict which tickers are analyzed. Firstly, I\u0026rsquo;m going to use only ETFs that have price histories before the 2008 crash in September. Secondly, I\u0026rsquo;m going to make some stylistic choices. I like Vanguard, so I\u0026rsquo;m including all of their ETFs that meet the date criteria. However, there are some asset groups they don\u0026rsquo;t offer ETFs for, so I\u0026rsquo;m also including some iShares ETFs to fill the gaps for things like real estate and junk bonds.\nIn total, this brings the total to 31 tickers, which is a far cry from how many I could include. This makes the results much easier to interpret and more practical.\nimport psycopg2 as pg import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits import mplot3d from sklearn.cluster import AffinityPropagation from sklearn.covariance import GraphicalLassoCV from sklearn.manifold import LocallyLinearEmbedding from config import DATABASE_URI plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,10) # All vanguard funds plus HYG, EMB, IYR, and SPY for junk bonds, emerging markets bonds, real estate, and the S\u0026amp;P 500 # respectively. # All created before the 2008 crash, IE before september 29 2008 tickers = [\u0026#34;EDV\u0026#34;, \u0026#34;BIV\u0026#34;, \u0026#34;BLV\u0026#34;, \u0026#34;BSV\u0026#34;, \u0026#34;BND\u0026#34;, \u0026#34;VIG\u0026#34;, \u0026#34;VUG\u0026#34;, \u0026#34;VYM\u0026#34;, \u0026#34;VV\u0026#34;, \u0026#34;MGC\u0026#34;, \u0026#34;MGK\u0026#34;, \u0026#34;MGV\u0026#34;, \u0026#34;VTI\u0026#34;, \u0026#34;VTV\u0026#34;, \u0026#34;VXF\u0026#34;, \u0026#34;VO\u0026#34;, \u0026#34;VOT\u0026#34;, \u0026#34;VOE\u0026#34;, \u0026#34;VB\u0026#34;, \u0026#34;VBK\u0026#34;, \u0026#34;VBR\u0026#34;, \u0026#34;VT\u0026#34;, \u0026#34;VEU\u0026#34;, \u0026#34;VEA\u0026#34;, \u0026#34;VGK\u0026#34;, \u0026#34;VPL\u0026#34;, \u0026#34;VWO\u0026#34;, \u0026#34;HYG\u0026#34;, \u0026#34;EMB\u0026#34;, \u0026#34;IYR\u0026#34;, \u0026#34;SPY\u0026#34;] with pg.connect(DATABASE_URI) as conn: with conn.cursor() as cur: cur.execute(f\u0026#34;SELECT date, ticker, closeadj FROM prices WHERE ticker IN {tuple(tickers)}\u0026#34;) results = cur.fetchall() df = pd.DataFrame.from_records(results, columns=[\u0026#34;date\u0026#34;, \u0026#34;ticker\u0026#34;, \u0026#34;closeadj\u0026#34;], coerce_float=True) # Set index, sort index, then transform into Series via squeeze df = df.set_index([\u0026#34;date\u0026#34;, \u0026#34;ticker\u0026#34;], verify_integrity=True).sort_index().squeeze() returns = df.unstack().pct_change().dropna() Estimating Structure Alright, so now I have a big matrix of daily returns for 31 different ETFs, what can we say about their structure? An obvious way to look at this is by calculating a covariance matrix. However, this problem brings some challenges here. Luckily, because of the length of the data, I don\u0026rsquo;t have to worry about the number of features being larger than the number of data points.\nThe problem really is that estimating the empirical covariance matrix doesn\u0026rsquo;t do well at uncovered structure. What I\u0026rsquo;m looking for is a graphical model that links together the ETFs we have. I want to know which are meaningfully correlated with which others and which have no meaningful correlation. An empirical covariance matrix will mostly say everything is at least a little correlated with everything else. Which while may be true, isn\u0026rsquo;t useful for what I want to do.\nWhat I need is a sparse covariance matrix. When two tickers don\u0026rsquo;t have much to do with each other, I want that covariance value to be zero. This is what a Graphical Lasso will do. Exactly as in Lasso-regularized regression, this procedure shrinks values towards zero.\n# Standardizing improves estimation of sparse covariance matrix X = returns.values.copy() X /= X.std(axis=0) edge_model = GraphicalLassoCV().fit(X) plt.matshow(np.cov(returns.T)) plt.title(\u0026#34;Normal Covariance\u0026#34;) plt.matshow(edge_model.covariance_) plt.title(\u0026#34;Sparse Covariance\u0026#34;) In the two matrices above you can clearly see the differences. Much of the second matrix is zero, but the first one has a mix of near-zero values. The Graphical Lasso shows us which connections are the most important. Note that the diagonal elements no longer represent each tickers individual variance, but we don\u0026rsquo;t need that anyways.\nClustering Okay so we have a matrix that tells us which ETFs are structurally related, so we can move the estimating clusters. I\u0026rsquo;m going to use Affinity Propagation for this because of two reasons: it selects the number of clusters automatically, and it provides a member of each cluster that best represents it. The latter reason is the coolest feature of this method. After it finds out which ETFs belong to which cluster, it will tell us which one of them best represents each cluster. This is exactly what we want! I don\u0026rsquo;t want to deal with 31 different ETFs, I want to deal with a small number that best represents the whole group. This is essentially a dimensionality reduction problem.\nI\u0026rsquo;m using a slightly smaller preference value than sklearn would use because I want the number of cluster to be smaller. This is a flexible value. Sklearn by default sets it as the median of the affinity matrix, in this case the covariance matrix. If you set it smaller, there will be fewer clusters, and bigger means more clusters.\nclustering = AffinityPropagation(affinity=\u0026#34;precomputed\u0026#34;, preference=0.15).fit(edge_model.covariance_) n_labels = clustering.labels_.max() cluster_centers = returns.columns[clustering.cluster_centers_indices_] for i in range(n_labels + 1): print(f\u0026#34;Cluster {i+1}: {\u0026#39;, \u0026#39;.join(returns.columns[clustering.labels_ == i])}\u0026#34;) print(f\u0026#34; Cluster Representative: {cluster_centers[i]}\u0026#34;) print(\u0026#34;\\n\u0026#34;) Cluster 1: BIV, BLV, BND, EDV Cluster Representative: BLV Cluster 2: BSV Cluster Representative: BSV Cluster 3: EMB Cluster Representative: EMB Cluster 4: HYG Cluster Representative: HYG Cluster 5: VEA, VEU, VGK, VPL, VT, VWO Cluster Representative: VEU Cluster 6: IYR, MGC, MGK, MGV, SPY, VB, VBK, VBR, VIG, VO, VOE, VOT, VTI, VTV, VUG, VV, VXF, VYM Cluster Representative: VTI Okay, so we get 6 clusters from our 31 ETFs. Looking at them qualitatively, it matches with our expectations. Cluster 1 is longer term bonds, cluster 5 is non-US equities, and cluster 6 is US equities. Looking at which are selected as being representative, we get a very intuitive answer: long term bonds, short term bonds, emerging market bonds, junk bonds, non-US equities, and US total stock market. One slightly unexpected grouping is that real estate (IYR) is grouped with equities.\nVisualization For some extra fun, how do we visualize this? We have the concept of clusters being things that are close together in space, but in this case, what does space even mean? It\u0026rsquo;s very high-dimensional and non-intuitive. Well, luckily there is a way to attempt to embed higher-dimensional space into lower-dimensional space called Manifold learning. This method tries to find a way in two-dimensions to best represent patterns and groupings in higher-dimensions.\nembed = LocallyLinearEmbedding(n_neighbors=10, n_components=2) embed = embed.fit_transform(edge_model.covariance_) plt.scatter(embed[:, 0], embed[:, 1], c=clustering.labels_, cmap=\u0026#34;Set1\u0026#34;) labels = returns.columns[clustering.cluster_centers_indices_] points = embed[clustering.cluster_centers_indices_] for i, label in enumerate(labels): plt.annotate(label, (points[i, 0], points[i, 1])) So, here we can see each of our clusters color-coded with the representative ETF from each cluster labeled. You can obviously see the clustering: bonds in red on the left, non-US equities pink on the bottom right, and US equities in grey in the upper right. Note the overlaid text between EMB and BSV on the left-hand side. The clustering algorithm views them as being distinct, but they are right on top of each other in this 2d embedding. This most likely means there is some high-dimensional difference between them that cannot be projected downward into 2 dimensions.\nConclusion This is way a casual look into clustering applied to ETFs. There\u0026rsquo;s a lot more you can do with this. There are more sophisticated estimation methods that are potentially non-linear, and you can obviously greatly expand the number of tickers under analysis. This can also be applied to groups of stocks, which can be useful for finding pairs trades.\n","permalink":"http://optionallybayes.com/posts/etf_clustering/","summary":"Using clustering algorithms for identifying optimal subset of ETFs for portfolio construction.","title":"Clustering ETFs for Optimally Diversified Portfolio"},{"content":"Today I\u0026rsquo;ll be running through Stochastic Volatility Models! These are related to GARCH models in that they allow for time-varying volatility in the return distribution. In other words, it accounts for heteroscedasticity.\nData I\u0026rsquo;m interested in the weekly returns of the S\u0026amp;P 500 index. My intent is to trade weekly options to go short volatility, so weekly forecasts are what I need.\nimport numpy as np import pandas as pd from cmdstanpy import cmdstan_path, CmdStanModel import matplotlib.pyplot as plt import arviz as az from scipy import stats import statsmodels.api as sm from psis import psisloo from datamodel import SPX, StockData plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,10) spx = SPX() spx_wk_prices = spx.prices.resample(\u0026#34;W-FRI\u0026#34;).last() spx_wk_returns = (np.log(spx_wk_prices) - np.log(spx_wk_prices.shift(1))).dropna() Here are what the weekly returns look like for the past 20-ish years. You can see that volatility \u0026ldquo;clusters\u0026rdquo; meaning that periods of extreme returns are generally followed by periods of extreme returns.\nspx_wk_returns.plot(title=\u0026#34;S\u0026amp;P 500 Index Weekly Returns\u0026#34;, xlabel=\u0026#34;Date\u0026#34;, ylabel=\u0026#34;Return\u0026#34;) This can be more easily seen with autocorrelation plots. Let\u0026rsquo;s look at the returns themselves first:\nfig = sm.tsa.graphics.plot_acf(spx_wk_returns, title=\u0026#34;Returns Autocorrelation\u0026#34;) plt.xlabel(\u0026#34;Lags\u0026#34;) plt.ylabel(\u0026#34;Correlation\u0026#34;) There\u0026rsquo;s very little autocorrelation, meaning that returns at each time period are unrelated to the returns that came at past time periods. However, let\u0026rsquo;s look at the square of returns, which is a crude way to estimate their volatility.\nfig = sm.tsa.graphics.plot_acf(spx_wk_returns**2, title=\u0026#34;Squared Returns Autocorrelation\u0026#34;) plt.xlabel(\u0026#34;Lags\u0026#34;) plt.ylabel(\u0026#34;Correlation\u0026#34;) Now there is clearly some significant autocorrelation, meaning volatility is affected by past volatility, thus the clustering effect. When there is a volatility shock, we expect to see periods of lasting higher volatility.\nModel 1 I\u0026rsquo;m going to be fitting a Stochastic Volatility Model which differs from a standard GARCH model. In a GARCH model, variance is modeled as a deterministic function of past errors and past variances:\n$$ \\sigma_{t}^2 = \\omega + \\alpha_{1} \\epsilon_{t-1}^2 + \\beta_{1} \\sigma_{t-1}^2 $$\nHowever, in a Stochastic Volatility Model, variance is modeled as a stochastic function of past variance:\n$$ \\sigma_{t}^2 = \\mu + \\phi (\\sigma_{t-1}^2 - \\mu) + \\epsilon_{t}$$\n$$ \\epsilon_{t} \\sim \\mathcal{N}(0, \\omega) $$\nThis model is what is encapsulated below in Stan model language. To use the symbols below it\u0026rsquo;s like this:\n$$ r_{t} \\sim \\mathcal{N}(\\mu_{r}, \\exp(\\frac{h_{t}}{2})) $$\n$$ h_{t} = \\mu_{h} + \\phi (h_{t-1} - \\mu_{h}) + \\epsilon_{t}$$\n$$ \\epsilon_{t} \\sim \\mathcal{N}(0, \\sigma) $$\nNote here that \\(h\\) represents the logarithm of variance. This makes its distribution far more symmetrical than in its normal form, making fitting the model easier. The gist of the model is that there exists a normal mean variance level represented by \\(\\mu_{h}\\) and when shocks occur, whose magnitude is governed by \\(\\sigma\\), variance will tend back towards that mean at a rate dictated by \\(\\phi\\).\nI generate the posterior predictive distribution in the generated quantities block, this will be useful for analysis of the model. I\u0026rsquo;m using broad uninformative priors here because I have plenty enough data points that they hardly matter.\nmodel_spec = \u0026#34;\u0026#34;\u0026#34; data { int N; // Length of data vector[N] r; // SPX returns } parameters { real mu_h; // Volatility mean term real mu_r; // Returns mean term real\u0026lt;lower=-1, upper=1\u0026gt; phi; // Persistence of volatility real\u0026lt;lower=0\u0026gt; sigma; // Volatility noise vector[N] h_std; // Log volatility } transformed parameters { vector[N] h = h_std * sigma; // h ~ normal(0, sigma); h[1] /= sqrt(1 - square(phi)); // h[1] ~ normal(0, sigma / sqrt(1 - square(phi))) h += mu_h; // h ~ normal(mu_h, sigma) for (t in 2:N) { h[t] += phi * (h[t-1] - mu_h); // h[2:N] ~ normal(mu_h + phi * (h[t-1] - mu_h), sigma) } } model { phi ~ uniform(-1, 1); sigma ~ normal(0, 10); mu_h ~ normal(0, 10); mu_r ~ normal(0, 10); h_std ~ std_normal(); r ~ normal(mu_r, exp(h / 2)); } generated quantities { real r_tilde[N]; real log_prob[N]; r_tilde = normal_rng(mu_r, exp(h / 2)); for (t in 1:N) { log_prob[t] = normal_lpdf(r[t] | mu_r, exp(h[t] / 2)); // Need log probabilities later on } } \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;./stan_model/model.stan\u0026#34;, \u0026#34;w\u0026#34;) as file: file.write(model_spec) model = CmdStanModel(stan_file=\u0026#34;./stan_model/model.stan\u0026#34;) data = {\u0026#34;N\u0026#34;: len(spx_wk_returns), \u0026#34;r\u0026#34;: spx_wk_returns.values} sample = model.sample(data=data, chains=4, parallel_chains=4, output_dir=\u0026#34;./stan_model\u0026#34;, iter_warmup=1000, iter_sampling=2500) model1_data = az.from_cmdstanpy(posterior=sample, posterior_predictive=\u0026#34;r_tilde\u0026#34;, observed_data={\u0026#34;r\u0026#34;: spx_wk_returns.values}, log_likelihood=\u0026#34;log_prob\u0026#34;) First let\u0026rsquo;s look at how our chain sampled to make sure everything looks okay.\naz.plot_trace(model1_data, compact=True, var_names=[\u0026#34;mu_h\u0026#34;, \u0026#34;mu_r\u0026#34;, \u0026#34;phi\u0026#34;, \u0026#34;sigma\u0026#34;]) Okay there is no obvious issues here. The parameter distributions from each chain look mostly the same, and there aren\u0026rsquo;t any obvious signs of autocorrelation in the samples. Next, let\u0026rsquo;s look at the summary statistics from our posterior predictive distribution versus our data. The blue histogram bars represent the posterior predictive, and the black line represents that statistic calculated from the data.\nr_tilde = model1_data.posterior_predictive.r_tilde.values.reshape(10000, -1) vol = model1_data.posterior.h.values.reshape(10000, -1) mean = np.mean(r_tilde, axis=1) std = np.std(r_tilde, axis=1) skew = stats.skew(r_tilde, axis=1) kurt = stats.kurtosis(r_tilde, axis=1) fig, axs = plt.subplots(2, 2) axs[0, 0].hist(mean, bins=50) axs[0, 0].axvline(np.mean(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 0].set_title(\u0026#34;Mean\u0026#34;) axs[0, 1].hist(std, bins=50) axs[0, 1].axvline(np.std(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 1].set_title(\u0026#34;Standard Deviation\u0026#34;) axs[1, 0].hist(skew, bins=50) axs[1, 0].axvline(stats.skew(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 0].set_title(\u0026#34;Skew\u0026#34;) axs[1, 1].hist(kurt, bins=50) axs[1, 1].axvline(stats.kurtosis(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 1].set_title(\u0026#34;Kurtosis\u0026#34;) There are some issues here, right off the bat. Ideally, the black line should fall in a high probability region of the histogram. This would mean that the data simulated from our model closely matches the qualities of the input data. This looks true mostly only for the standard deviation and kurtosis. It seems like the model is not modeling the mean or skew very well. Next let\u0026rsquo;s look at the distribution of our input data versus the distribution of the posterior predictive.\naz.plot_ppc(model1_data, data_pairs={\u0026#34;r\u0026#34;: \u0026#34;r_tilde\u0026#34;}) This looks pretty good! The distributions look mostly the same. Next, I want to look at how well calibrated the model is. The model outputs a distributional estimate at each time point. So ideally, for instance, if we calculate the 95th percentile of that distribution, the input data should have values higher than that only 5% of the time. Likewise that data should have values smaller than the 5% percentile only 5% of the time.\n# 95% bounds exceedances np.sum(spx_wk_returns.values \u0026gt; np.percentile(r_tilde, 95, axis=0)) / len(spx_wk_returns) 0.015459723352318959 # 5% bounds exceedances np.sum(spx_wk_returns.values \u0026lt; np.percentile(r_tilde, 5, axis=0)) / len(spx_wk_returns) 0.03986981285598047 So 1.6% of the time the data values are above the 95% bounds, and 3.8% of the time the data values are below the 5% bounds. If anything then, our distribution may be too broad. However, in this case that could be considered a good thing because I\u0026rsquo;d rather predict a broader distribution of returns than a too restrictive one. It\u0026rsquo;s best to be over-prepared for extreme outcomes than under-prepared. The next plot is the 95% and 5% bounds plotted against the return data. You can see the points where the returns exceed those bounds.\nplt.plot(np.percentile(r_tilde, 95, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(np.percentile(r_tilde, 5, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(spx_wk_returns.values, color=\u0026#34;red\u0026#34;, alpha=0.5) The next test is doing a probability integral transform. When you put a value through a CDF it gets transformed onto the range 0 to 1. Ideally, if I put the data through the CDF implied by the model, those output values should be uniformly distributed. This implies that the predicted distribution accurately predicts the probabilities of events. Unlike the exceedances test, which only looks at the tails, this test looks at the entire distribution.\nvalues = [] for t in range(len(spx_wk_returns)): ecdf = sm.distributions.empirical_distribution.ECDF(r_tilde[:, t]) values.append(ecdf(spx_wk_returns.iloc[t])) fig = sm.graphics.qqplot(np.array(values), dist=stats.uniform, line=\u0026#34;45\u0026#34;) A QQ plot displays the transformed data against a reference distribution. If the samples match a uniform distribution, they should all fall perfectly on the 45 degree line in the figure. It\u0026rsquo;s clear there is some odd behavior at the right tail and in the center. It seems like our distributional estimate doesn\u0026rsquo;t match the data too well.\nModel 2 Okay, so issues are that the mean and skew seem off, and the distribution estimate doesn\u0026rsquo;t match too well with the data. What should I try? Well, we expect negative skew, because large negative returns happen rarely. So instead of assuming a normal error for returns, let\u0026rsquo;s try a skew normal! So everything is the same but the sampling statement for the returns looks like this now:\n$$ r_{t} \\sim Skew Normal(\\mu_{r}, \\exp(\\frac{h_{t}}{2}), \\alpha) $$\nWhere \\(\\alpha\\) is a new parameter that dictates the level of skew. In Stan, that model looks like this.\nmodel_spec = \u0026#34;\u0026#34;\u0026#34; data { int N; // Length of data vector[N] r; // SPX returns } parameters { real mu_h; // Volatility mean term real mu_r; // Returns mean term real\u0026lt;lower=-1, upper=1\u0026gt; phi; // Persistence of volatility real\u0026lt;lower=0\u0026gt; sigma; // Volatility noise vector[N] h_std; // Log volatility real alpha; // Skew Normal shape parameter } transformed parameters { vector[N] h = h_std * sigma; // h ~ normal(0, sigma); h[1] /= sqrt(1 - square(phi)); // h[1] ~ normal(0, sigma / sqrt(1 - square(phi))) h += mu_h; // h ~ normal(mu_h, sigma) for (t in 2:N) { h[t] += phi * (h[t-1] - mu_h); // h[2:N] ~ normal(mu_h + phi * (h[t-1] - mu_h), sigma) } } model { phi ~ uniform(-1, 1); sigma ~ normal(0, 10); mu_h ~ normal(0, 10); mu_r ~ normal(0, 10); alpha ~ normal(0, 10); h_std ~ std_normal(); r ~ skew_normal(mu_r, exp(h / 2), alpha); } generated quantities { real r_tilde[N]; real log_prob[N]; r_tilde = skew_normal_rng(mu_r, exp(h / 2), alpha); for (t in 1:N) { log_prob[t] = skew_normal_lpdf(r[t] | mu_r, exp(h[t] / 2), alpha); // Need log probabilities later on } } \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;./stan_model/model.stan\u0026#34;, \u0026#34;w\u0026#34;) as file: file.write(model_spec) model = CmdStanModel(stan_file=\u0026#34;./stan_model/model.stan\u0026#34;) data = {\u0026#34;N\u0026#34;: len(spx_wk_returns), \u0026#34;r\u0026#34;: spx_wk_returns.values} sample = model.sample(data=data, chains=4, parallel_chains=4, output_dir=\u0026#34;./stan_model\u0026#34;, iter_warmup=1000, iter_sampling=2500) model2_data = az.from_cmdstanpy(posterior=sample, posterior_predictive=\u0026#34;r_tilde\u0026#34;, observed_data={\u0026#34;r\u0026#34;: spx_wk_returns.values}, log_likelihood=\u0026#34;log_prob\u0026#34;) az.plot_trace(model2_data, compact=True, var_names=[\u0026#34;mu_h\u0026#34;, \u0026#34;mu_r\u0026#34;, \u0026#34;phi\u0026#34;, \u0026#34;sigma\u0026#34;, \u0026#34;alpha\u0026#34;]) Again, everything looks good here. Alpha centers around a negative value, which is a good sign, because negative skew was expected.\nr_tilde = model2_data.posterior_predictive.r_tilde.values.reshape(10000, -1) vol = model2_data.posterior.h.values.reshape(10000, -1) mean = np.mean(r_tilde, axis=1) std = np.std(r_tilde, axis=1) skew = stats.skew(r_tilde, axis=1) kurt = stats.kurtosis(r_tilde, axis=1) fig, axs = plt.subplots(2, 2) axs[0, 0].hist(mean, bins=50) axs[0, 0].axvline(np.mean(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 0].set_title(\u0026#34;Mean\u0026#34;) axs[0, 1].hist(std, bins=50) axs[0, 1].axvline(np.std(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 1].set_title(\u0026#34;Standard Deviation\u0026#34;) axs[1, 0].hist(skew, bins=50) axs[1, 0].axvline(stats.skew(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 0].set_title(\u0026#34;Skew\u0026#34;) axs[1, 1].hist(kurt, bins=50) axs[1, 1].axvline(stats.kurtosis(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 1].set_title(\u0026#34;Kurtosis\u0026#34;) Now the mean value lies right in the center of the distribution and the skew value is closer to the middle then it was before. That looks like progress!\naz.plot_ppc(model2_data, data_pairs={\u0026#34;r\u0026#34;: \u0026#34;r_tilde\u0026#34;}) # 95% bounds exceedances np.sum(spx_wk_returns.values \u0026gt; np.percentile(r_tilde, 95, axis=0)) / len(spx_wk_returns) 0.025223759153783564 # 5% bounds exceedances np.sum(spx_wk_returns.values \u0026lt; np.percentile(r_tilde, 5, axis=0)) / len(spx_wk_returns) 0.025223759153783564 Our exceedances are again a bit too broad but they are more even than the first model.\nplt.plot(np.percentile(r_tilde, 95, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(np.percentile(r_tilde, 5, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(spx_wk_returns.values, color=\u0026#34;red\u0026#34;, alpha=0.5) values = [] for t in range(len(spx_wk_returns)): ecdf = sm.distributions.empirical_distribution.ECDF(r_tilde[:, t]) values.append(ecdf(spx_wk_returns.iloc[t])) fig = sm.graphics.qqplot(np.array(values), dist=stats.uniform, line=\u0026#34;45\u0026#34;) The QQ plot here looks a little bit more funky than the one in model 1, which is concerning.\nModel Comparison So there are two models. There must be a better way to find out which is better than looking at visualizations. Turns out there is a really cool method for this called Pareto Smoothed Importance Sampling. This paper covers it very well. It sounds more complicated than it is. It allows the use of posterior samples and log probabilities to estimate the out-of-sample error of the model. It seeks to approximate the error estimated by leave-one-out (LOO) cross validation without running the model repeatedly. In this case, the model doesn\u0026rsquo;t take long to fit, and I could run fewer samples, but it would still have to be run nearly 1200 times to do true LOO cross validation\nI\u0026rsquo;ll take a brief aside to discuss leave-one-out methods on time series. This paper does some analysis on K-Fold cross validation on time series considering stationarity. They find little difference in error estimation using walk-forward versus K-Fold cross validation on stationary time series. This makes intuitive sense in that stationary time series display no time dependence, so the order in which you use the data shouldn\u0026rsquo;t matter. It\u0026rsquo;s obvious that stock market returns are not stationary. However, for the sake of this analysis, I\u0026rsquo;m going to assume that they are conditionally stationary given the volatility process. This gives some legitimacy to what I\u0026rsquo;m about to do. I will warn, however, that it\u0026rsquo;s not a perfect method and it\u0026rsquo;s applicability here can be called into question. With all of that said, let\u0026rsquo;s continue.\nThe author of the first paper linked very nicely has coded this process in python already, available here.\nmodel1_probs = model1_data.log_likelihood.log_prob.values.reshape(10000, -1) model2_probs = model2_data.log_likelihood.log_prob.values.reshape(10000, -1) loo1, loos1, ks1 = psisloo(model1_probs) loo2, loos2, ks2 = psisloo(model2_probs) diff = round(loo2 - loo1, 2) diff_se = round(np.sqrt(len(loos1) * np.var(loos2 - loos1)), 2) diff_interval = [round(diff - 2.6 * diff_se, 2), round(diff + 2.6 * diff_se, 2)] print(f\u0026#34;Model 1 ELPD: {round(loo1, 2)}\\nModel 2 ELPD: {round(loo2, 2)}\u0026#34;) Model 1 ELPD: 3042.64 Model 2 ELPD: 3050.58 print(f\u0026#34;Model 2 - Model 1: {diff}\\nStandard Error: {diff_se}\\nDifference 99% Interval: {diff_interval[0]} | {diff_interval[1]}\u0026#34;) Model 2 - Model 1: 7.94 Standard Error: 3.96 Difference 99% Interval: -2.36 | 18.24 ELPD stands for expected log predictive density. This is what we expect the out-of-sample log probability to be for the model, so we want it to be higher. Higher values imply that the probability of seeing the data given the model is higher, which means the model more closely matches the nature of the data. So, it looks like model 2 is better. Although, given the standard error of the estimate, there is some region of the sampling distribution where model 2 is similar or worse but not by very much. This method also returns a value for the shape parameter fitted to the Pareto distribution. Ideally we want this parameter to be less than 0.5 for every point, but 0.5 to 1 is okay. At these higher levels the variance of the estimator is higher and makes it less reliable. Parameter values greater than 1 are highly undesirable. At these levels, the variance of the estimator is infinite and totally unreliable.\nks1_max = round(np.max(ks1), 2) ks2_max = round(np.max(ks2), 2) ks1_gt = round(sum(ks1 \u0026gt; 0.5) / len(ks1) * 100, 2) ks2_gt = round(sum(ks2 \u0026gt; 0.5) / len(ks2) * 100, 2) print(f\u0026#34;Max k for Model 1: {ks1_max}\\nMax k for Model 2: {ks2_max}\u0026#34;) print(f\u0026#34;Percentage of values greater than 0.5 for Model 1: {ks1_gt}%\\nPercentage of values greater than 0.5 for Model 2: {ks2_gt}%\u0026#34;) Max k for Model 1: 0.87 Max k for Model 2: 0.95 Percentage of values greater than 0.5 for Model 1: 10.9% Percentage of values greater than 0.5 for Model 2: 11.64% It looks like the estimates for model 2 are slightly less reliable than model 1, which is worth considering because of how close the difference above was. The max value of k is also quite high, indicating that there are som significant data points making estimation more difficult. All-in-all I would consider model 2 to be better, but the differences are slight.\nModel Volatility versus Realized Volatility The model basically finds the value of volatility that fits the return data we give it. It\u0026rsquo;s a type of hierarchical model where volatility is a latent quantity. We cannot directly observe the volatility of a return series in the real world, we can only imply it. In the literature, there is a great deal about how to estimate that latent volatility. I\u0026rsquo;ve covered a few of those methods in a previous post. Let\u0026rsquo;s compare what our model thinks volatility is to a realized volatility estimator. I\u0026rsquo;m going to be taking the volatility from the second model.\nNote I\u0026rsquo;m taking the proper transformations to ensure both series are in standard deviation form. My volatility data is shorter than my weekly returns data, so I have to truncate some of it.\nreal_vol = np.sqrt(spx.vol.resample(\u0026#34;W-FRI\u0026#34;).sum()) model_vol = pd.Series(np.mean(arviz_data.posterior.h.values.reshape(10000, -1), axis=0), index=spx_wk_returns.index) model_vol = np.sqrt(np.exp(model_vol)) common_index = real_vol.index.intersection(model_vol.index) real_vol = real_vol.loc[common_index] model_vol = model_vol.loc[common_index] real_vol.plot() model_vol.plot(color=\u0026#34;r\u0026#34;) With the model volatility in red and the realized measure in blue. The model pretty well captures the realized volatility! It\u0026rsquo;s a smoother estimate, which makes sense considering the linear model for it we are using. Cool!\nConclusion The model isn\u0026rsquo;t perfect, but then again no model is! The first one fails to capture the negative skew, and while the second one does better, the QQ plot looks less pleasing. This may mean that in doing better capturing skew, it fails to as effectively capture the middle of the distribution.\nThere are a lot of interesting extensions you could make to this model. The mean process of volatility could include exogenous regressors like VIX levels, or it could include past values of the returns themselves! Next the volatility of volatility, \\(\\sigma\\) in the model, could be made to have a stochastic or deterministic process of its own! Essentially, it could be made to vary with time, just like volatility of the returns.\nI\u0026rsquo;m becoming very interested in Bayesian methods for time series analysis, and there seems to be a lot less literature about that than non-time series models. I think the process for writing, fitting, and interpreting Bayesian models is much more straightforward and clear than frequentist methods. Credible intervals, the fact that parameter uncertainty is automatically accounted for in the posterior predictive distribution, and the methods for estimating out-of-sample error make life much easier.\n","permalink":"http://optionallybayes.com/posts/stoch_vol/","summary":"Fitting 2 different Stochastic Volatility Models to S\u0026amp;P 500 returns and finding out which is better","title":"Stochastic Volatility Models in Stan"},{"content":"I\u0026rsquo;ve been using a SQLite database to store my financial data locally for a while. This meant I had to have my personal computer running to do updates, I didn\u0026rsquo;t have a consistent way to access data, and ran the risk of losing my data. I decided it would be best to use Amazon Web Services (AWS) to handle data storage and updating from here on. I learned a lot along the way!\nThings that didn\u0026rsquo;t work Amazon Aurora Serverless I went in excited about Amazon Aurora Serverless. It seemed perfect for my needs. I don\u0026rsquo;t need to make database calls very often, so it would automatically shut off and cost nothing after a period of no usage. Great! However, I eventually learned that you cannot connect to an Aurora Serverless instance like you would connect to a normal SQL database to make calls. You have to use the Amazon Data API through the AWS command line interface or through their various SDKs. I frankly didn\u0026rsquo;t feel like putting the effort in to transition all of my current SQL calls and data handling to this, so I abandoned it.\nI ended up using a normal Relational Database Service instance. I\u0026rsquo;m using the smallest instance, which only costs about 15 dollars a month and is suitable for my needs.\nAWS Lambda Again, Lambda seemed perfect for my needs. It would run my python code to update my database on a schedule, charge me for that usage, then shut off and cost nothing the rest of the time. However, I found the documentation to be difficult at best and debugging to be challenging.\nYou have to upload your all of your package dependencies with your code in a zip file that ends up being 100\u0026rsquo;s of megabytes. The size means that you can\u0026rsquo;t use Amazon\u0026rsquo;s cloud IDE to update your code, so you have to re-upload it every time you need to change it, and it\u0026rsquo;s not fast. This made fixing bugs very tedious. You also have to ensure your zip file has certain read/write privileges before you upload it, something that I only found out via Stack Overflow after having errors.\nNext up was scheduling. I need this to run nightly. You have to go through Amazon CloudWatch to do this. Ugh.\nI ended up using an EC2 instance here. The smallest one is part of the free tier, so I don\u0026rsquo;t even have to pay for it!\nSetup I have found the AWS documentation to be generally hard to get through. Much of it seems to assume that you have background in AWS already, so it\u0026rsquo;s confusing to start from scratch. I want to document what I did to get this all working.\nVPC AWS creates a virtual network for you to connect all of your instances together. It\u0026rsquo;s also key to setup properly so you can access your instances from a computer connected via the internet. The easiest way to start is by using the automatic wizard from the dashboard page on the VPC console:\nThen you can select the option for a single public subnet:\nIn the next screen everything can remain the default, and a name for the VPC can be entered. This process automatically creates a VPC, an internet gateway, and a subnet with that gateway attached. We do want to create another subnet under our VPC that is in a different availability zone. This is relevant for the database setup because Amazon puts the backup in a different zone than the database itself. Going to the subnets tab there should be a single subnet under the VPC you created, make note of its availability zone ID. Then you can create a new subnet under that same VPC. The IP block will need to be different. For instance the default subnet will be something like 10.0.0.0/24 so this new subnet will need to be 10.0.1.0/24. Then select an availability zone that is different than the default subnet.\nNext up is the security group that defines what connections will be accepted and from where. Create a new security group under the security heading and make the inbound rules look like this:\nThe two top rules are so other instances in your subnets can connect to your database. The third can be set to accept connections from your personal computers IP by selecting \u0026ldquo;My IP\u0026rdquo; in the source box. The fourth has a type of SSH, again from your own IP, this allows you to connect to your EC2 instance via SSH to configure it. For outbound rules you can set destination to 0.0.0.0/0 and everything else to All so everything going out will be allowed.\nNow the networking and security is configured!\nRDS Subnet Group Next we have to make a subnet group for the database to use. In the RDS console, there is a subnet groups link. Create a new one, select the VPC configured earlier, and then select the two subnets. That\u0026rsquo;s it!\nRDS Instance Now moving to the database instance. Important settings to note:\nThe free tier instance classes are under \u0026ldquo;Burstable classes\u0026rdquo; Make sure to deselect Multi-AZ deployment, this costs extra Select the VPC configured earlier under Connectivity, select the subnet group configured earlier, then choose the security group also configured earlier Make sure that public access is set to \u0026ldquo;yes\u0026rdquo; Once the instance starts, on its summary page, make note of the endpoint URL and the port. This is the IP and port you\u0026rsquo;ll use when connecting to the database.\nEC2 Instance You can select a variety of machine images when creating these, I use the Ubuntu Server option. Then you can select the instance type that dictates how many resources the instance has access to. I use the free tier eligible t2.micro. On the configuration page, you can select the VPC, subnet, and other options. When you launch it, you\u0026rsquo;ll be directed to download a private key file. This is very important to keep. This file allows you to connect to your instance via SSH.\nOnce launched, on the instance summary page, there is the \u0026ldquo;Public IPv4 DNS.\u0026rdquo; This is the IP you\u0026rsquo;ll use to connect to your instance. The SSH command to connect looks like this:\nssh -i [path to .pem file] [Instance IP address] Once in, you can do whatever to get your code where it needs to be to run.\nFor scheduling, I use a cron job to run every night at midnight. Use crontab -e and put a line looking something like this:\n0 0 * * * source ~/RDSDatabase/update.sh Where update.sh is whatever you need to run. Mine looks like this:\n#!/bin/bash cd ~/RDSDatabase source venv/bin/activate python data_update.py Conclusion After all the fuss of figuring this out, it has been very well worth it. My data is there and up-to-date whenever I need it. I\u0026rsquo;ve created some data classes to fetch and hold the data the way I need it, so I have a consistent way to access it. It all just works. Most importantly, it\u0026rsquo;s not costing me that much money!\n","permalink":"http://optionallybayes.com/posts/aws_database/","summary":"Lessons learned migrating my data to AWS","title":"Moving my Data to Amazon Web Services"},{"content":"I\u0026rsquo;ve moved my blog to Hugo for one main reason: I want to start posting work about Julia here, but Pluto.jl only exports in HTML, and Hugo supports that kind of content. In the process of moving, however, I\u0026rsquo;ve noticed that Hugo is much nicer to work with than Pelican. Hugo is written in Go, which I have no experience with, but it was very easy to install and use. Pelican, on the other hand, is written in python so I had to create a virtual environment and go through the normal annoyances with dependencies.\nHugo also includes a built in local server so you can work on your site and know what you\u0026rsquo;re getting. It works very smoothly. Writing posts and managing content is also far easier and more organized in Hugo. I can group my posts and their content together all in one folder so everything is nice and neat.\nIt\u0026rsquo;s much easier also to push my content directly to github pages. I was using a seperate plugin for Pelican, but I\u0026rsquo;ve created this shell script now that automatically builds my site, pushes the source to the master branch, and pushes only the static site content to the gh-pages branch.\n#!/usr/bin/bash hugo git add * read -p \u0026#34;Commit Message: \u0026#34; m git commit -m \u0026#34;$m\u0026#34; git push origin master git subtree push --prefix public origin gh-pages Themes are very well supported also, and there are many of them to choose from. Using the PaperMod theme, it was easy to get LaTeX support also, which is important to me.\nIt\u0026rsquo;s very usable, well documented, and I would recommend it to anyone looking for a static site generator to use for their own personal blog.\n","permalink":"http://optionallybayes.com/posts/hugo_move/","summary":"My comments on how Hugo compares to Pelican for static site generation","title":"A Note on Hugo"},{"content":"Alright, in this post I\u0026rsquo;m going to run through how to price options using Monte Carlo methods and also compute the associated greeks using automatic differentiation in PyTorch.\nBlack-Scholes First, let\u0026rsquo;s look at implementing the Black-Scholes model in PyTorch.\nThe input variables are as follows:\n\\(K\\) : Strike price of the option\n\\(S(t)\\) : Price of the underlying asset at time \\(t\\)\n\\(t\\) : Current time in years.\n\\(T\\) : Time of option expiration\n\\(\\sigma\\) : Standard deviation of the underlying returns\n\\(r\\) : Annualized risk-free rate\n\\(N(x)\\) : Standard Normal cumulative distribution function\nThe price of a call option is given by:\n$$C(S_t, t) = N(d_1) S_t - N(d_2) K e^{-r(T-t)}$$\n$$d_1 = \\frac{1}{\\sigma\\sqrt{T-t}}[\\ln(\\frac{S_t}{K}) + (r + \\frac{\\sigma^2}{2})(T-t)]$$\n$$d_2 = d_1 - \\sigma\\sqrt{T-t}$$\nAnd by parity the price of a put option is given by:\n$$P(S_t, t) = N(-d_2) K e^{-r(T-t)} - N(-d_1) S_t$$\nNow, let\u0026rsquo;s implement that using PyTorch functions. For simplicity I replace \\(T\\) and \\(t\\) and their difference by a single term \\(T\\) specifying the total time left to expiry in years.\nimport torch from torch.distributions import Normal std_norm_cdf = Normal(0, 1).cdf std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x)) def bs_price(right, K, S, T, sigma, r): d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T) d_2 = d_1 - sigma * torch.sqrt(T) if right == \u0026#34;C\u0026#34;: C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T) return C elif right == \u0026#34;P\u0026#34;: P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S return P With this function I can calculate the price of a call option with the underyling at 100, strike price at 100, 1 year to expiration, 5% annual volatility, and a risk-free rate of 1% annually.\nright = \u0026#34;C\u0026#34; K = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) price = bs_price(right, K, S, T, sigma, r) print(price) tensor(2.5216, grad_fn=\u0026lt;SubBackward0\u0026gt;) Now, the magic of PyTorch is that it tracks all of those computations in a graph and can use its automatic differentiation feature to give us all the greeks. That\u0026rsquo;s why I told it that I needed a gradient on all of the input variables.\n# Tell PyTorch to compute gradients price.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.5890103578567505 Vega: 38.89707946777344 Theta: 1.536220908164978 Rho: 56.379390716552734 How do these compare to the greeks computed directly by differentiating the Black-Scholes formula?\nd_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T) d_2 = d_1 - sigma * torch.sqrt(T) delta = std_norm_cdf(d_1) vega = S * std_norm_pdf(d_1) * torch.sqrt(T) theta = ((S * std_norm_pdf(d_1) * sigma) / (2 * torch.sqrt(T))) + r * K * torch.exp(-r * T) * std_norm_cdf(d_2) rho = K * T * torch.exp(-r * T) * std_norm_cdf(d_2) print(f\u0026#34;Delta: {delta}\\nVega: {vega}\\nTheta: {theta}\\nRho: {rho}\u0026#34;) Delta: 0.5890103578567505 Vega: 38.89707946777344 Theta: 1.5362210273742676 Rho: 56.379390716552734 Exactly the same to a high level of precision! Amazing. It\u0026rsquo;s easy to see how much simpler the PyTorch autograd approach is. Note that it is possible to calculate second-order derivatives like Gamma, it just requires remaking the computation graph. If anyone knows of a workaround to this let me know.\nS = torch.tensor(100.0, requires_grad=True) price = bs_price(right, K, S, T, sigma, r) delta = torch.autograd.grad(price, S, create_graph=True)[0] delta.backward() print(f\u0026#34;Autograd Gamma: {S.grad}\u0026#34;) # And the direct Black-Scholes calculation gamma = std_norm_pdf(d_1) / (S * sigma * torch.sqrt(T)) print(f\u0026#34;BS Gamma: {gamma}\u0026#34;) Autograd Gamma: 0.07779412716627121 BS Gamma: 0.0777941569685936 Monte Carlo Pricing Now that\u0026rsquo;s all fine, but nothing new except some computation tricks. Black-Scholes makes assumptions that can often violate what is observed in the real world. The problem is creating closed form pricing models under other market dynamics is usually impossible. That\u0026rsquo;s where Monte Carlo sampling comes in. It\u0026rsquo;s a trivial task to create future market paths given a model for its dynamics. You can calculate option payoffs from those paths and get a price. But how can you calculate greeks from Monte Carlo samples? Again, PyTorch and autograd can help.\nI\u0026rsquo;ll use all of the same parameters as in the example above. Let\u0026rsquo;s simulate the result of a Geometric Brownian Motion process after one year, just like Black-Scholes does.\nK = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) Z = torch.randn([1000000]) # Brownian Motion W_T = torch.sqrt(T) * Z # GBM prices = S * torch.exp((r - 0.5 * torch.square(sigma)) * T + sigma * W_T) import matplotlib.pyplot as plt plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) plt.hist(prices.detach().numpy(), bins=25) plt.xlabel(\u0026#34;Prices\u0026#34;) plt.ylabel(\u0026#34;Occurences\u0026#34;) plt.title(\u0026#34;Distribution of Underlying Price after 1 Year\u0026#34;) Now, let\u0026rsquo;s calculate the option payoffs under each of those future prices, discount them using the risk-free rate, and then take the mean to get the option price. The price calculated with this method is close to the price calculated using Black-Scholes.\npayoffs = torch.max(prices - K, torch.zeros(1000000)) value = torch.mean(payoffs) * torch.exp(-r * T) print(value) tensor(2.5215, grad_fn=\u0026lt;MulBackward0\u0026gt;) Now, the magic comes in. The only random sampling I used above was a parameter-less standard normal. This fact allows PyTorch to keep track of gradients throughout all of the calculations above. This is called a Pathwise Derivative. This means we can use autograd just like above to get greeks.\nvalue.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.5890941023826599 Vega: 38.89133834838867 Theta: 1.536162257194519 Rho: 56.38788604736328 All the same! This means that we can simulate any Monte Carlo process we want, as long as its random component can be reparameterized, and get prices and greeks. Obviously this is a trivial example, but let\u0026rsquo;s look at a more complicated path-dependent option contract like an Asian Option. This type of option has a payoff based on the average price of the underlying over it\u0026rsquo;s duration, rather than only the price at expiration like a Vanilla Option. This means we must simulate the price movement each day instead of just at the end.\n# All the same parameters for the price process K = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) dt = torch.tensor(1 / 252) Z = torch.randn([1000000, int(T * 252)]) # Brownian Motion W_t = torch.cumsum(torch.sqrt(dt) * Z, 1) # GBM prices = S * torch.exp((r - 0.5 * torch.square(sigma)) * T + sigma * W_t) plt.plot(prices[0, :].detach().numpy()) plt.xlabel(\u0026#34;Number of Days in Future\u0026#34;) plt.ylabel(\u0026#34;Underlying Price\u0026#34;) plt.title(\u0026#34;One Possible Price path\u0026#34;) plt.axhline(y=torch.mean(prices[0, :]).detach().numpy(), color=\u0026#34;r\u0026#34;, linestyle=\u0026#34;--\u0026#34;) plt.axhline(y=100, color=\u0026#39;g\u0026#39;, linestyle=\u0026#34;--\u0026#34;) The payoff of an Asian Option given this price path is the difference between the strike price, the green dashed line, and the daily average price over the year, shown by the dashed red line. In this case, the payoff would be zero because the average daily price is below the strike.\n# Payoff is now based on mean of underlying price, not terminal value payoffs = torch.max(torch.mean(prices, axis=1) - K, torch.zeros(1000000)) #payoffs = torch.max(prices[:, -1] - K, torch.zeros(100000)) value = torch.mean(payoffs) * torch.exp(-r * T) print(value) tensor(1.6765, grad_fn=\u0026lt;MulBackward0\u0026gt;) value.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.6314291954040527 Vega: 20.25724220275879 Theta: 0.5357358455657959 Rho: 61.46644973754883 PyTorch Autograd once again gives us greeks even though we are now pricing a totally different contract. Awesome!\nConclusion Monte Carlo methods provide a way to price options under a much broader range of market process models. However, computing greeks can be challenging, either having to use finite difference methods or calculating pathwise derivatives symbolically. Using PyTorch can mitigate those issues and use automatic differentiation to provide greeks straight out of the box with no real overhead.\n","permalink":"http://optionallybayes.com/posts/option_pricing/","summary":"Using PyTorch to easily compute Option Greeks first using Black-Scholes and then Monte Carlo methods.","title":"Monte Carlo Methods for Option Pricing and Greeks"},{"content":"Okay, today we are moving up in the world and I\u0026rsquo;m going to use the magic of neural networks to forecast volatility.\nThe Data import pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) pd.plotting.register_matplotlib_converters() # Here\u0026#34;s my minute data for the S\u0026amp;P 500 spx_minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#34;datetime\u0026#34;, \u0026#34;open\u0026#34;, \u0026#34;high\u0026#34;, \u0026#34;low\u0026#34;, \u0026#34;close\u0026#34;], index_col=\u0026#34;datetime\u0026#34;, parse_dates=True) # Here\u0026#34;s the function for calculating the 1-min RV, as discussed in my last post def rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#34;close\u0026#34;]) - np.log(data[\u0026#34;close\u0026#34;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) spx_variance = rv_calc(spx_minute) conn = sqlite3.Connection(\u0026#34;data.db\u0026#34;) spx_data = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^GSPC\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=\u0026#34;date\u0026#34;) spx_returns = np.log(spx_data[\u0026#34;close\u0026#34;]) - np.log(spx_data[\u0026#34;close\u0026#34;].shift(1)) spx_returns = spx_returns.dropna() vix_data = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^VIX\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=\u0026#34;date\u0026#34;) # This puts it into units of daily standard deviation vix = vix_data[\u0026#34;close\u0026#34;] / np.sqrt(252) / 100 def create_lags(series, lags, name=\u0026#34;x\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Creates a dataframe with lagged values of the given series. Generates columns named x_t-n which means the value of each row is the value of the original series lagged n times \u0026#34;\u0026#34;\u0026#34; result = pd.DataFrame(index=series.index) result[f\u0026#34;{name}_t\u0026#34;] = series for n in range(lags): result[f\u0026#34;{name}_t-{n+1}\u0026#34;] = series.shift((n+1)) return result The predictive variables are the VIX, returns of the index, and our calculated realized variance. I include the 21 past values of these variables.\nvix_lags = create_lags(np.log(vix), 21, name=\u0026#34;vix\u0026#34;) return_lags = create_lags(spx_returns, 21, name=\u0026#34;returns\u0026#34;) rv_lags = create_lags(np.log(spx_variance), 21, name=\u0026#34;rv\u0026#34;) x = pd.concat([vix_lags, return_lags, rv_lags], axis=1).dropna() # We want to predict log of variance y = np.log(spx_variance.rolling(5).sum().shift(-5)).dropna() common_index = x.index.intersection(y.index) x = x.loc[common_index] y = y.loc[common_index] The Model I\u0026rsquo;m using a mixture density network to model future volatility. This is because I want an estimate of the future distribution of volatility, not just a point estimate. A mixture density network outputs the parameters for making a mixture of normal distributions. This is useful because you can approximate any arbitrary distribution with a large enough mixture of only normal distributions.\nimport torch import torch.nn as nn from torch.distributions import Categorical, Normal, Independent, MixtureSameFamily from torch.optim.swa_utils import AveragedModel, SWALR torch.set_default_dtype(torch.float64) class MDN(nn.Module): def __init__(self, in_dim, out_dim, hidden_dim, n_components): super().__init__() self.n_components = n_components # Last layer output dimension rationale: # Need two parameters for each distributionm thus 2 * n_components. # Need each of those for each output dimension, thus that multiplication self.norm_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, 2 * n_components * out_dim) ) self.cat_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, n_components * out_dim) ) def forward(self, x): norm_params = self.norm_network(x) # Split so we get parameters for mean and standard deviation mean, std = torch.split(norm_params, norm_params.shape[1] // 2, dim=1) # We need rightmost dimension to be n_components for mixture mean = mean.view(mean.shape[0], -1, self.n_components) std = std.view(std.shape[0], -1, self.n_components) normal = Normal(mean, torch.exp(std)) cat_params = self.cat_network(x) # Again, rightmost dimension must be n_components cat = Categorical(logits=cat_params.view(cat_params.shape[0], -1, self.n_components)) return MixtureSameFamily(cat, normal) test_index = int(len(x) * .75) train_x = torch.Tensor(x.iloc[:test_index].values) train_y = torch.Tensor(y.iloc[:test_index].values) test_x = torch.Tensor(x.iloc[test_index:].values) test_y = torch.Tensor(y.iloc[test_index:].values) in_dim = len(x.columns) out_dim = 1 n_components = 5 hidden_dim = 250 Below here is the training loop. I\u0026rsquo;m using a cosine annealing learning rate schedule to better explore the parameter space, as well as using model averaging over the last 500 iterations so the model generalizes better.\nmodel = MDN(in_dim, out_dim, hidden_dim, n_components) optimizer = torch.optim.AdamW(model.parameters(), lr=.001) scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 100, 2) swa_model = AveragedModel(model) swa_start = 400 swa_scheduler = SWALR(optimizer, swa_lr=0.001, anneal_epochs=10, anneal_strategy=\u0026#34;cos\u0026#34;) train_losses = [] validation_losses = [] model.train() swa_model.train() for epoch in range(500): optimizer.zero_grad() output = model(train_x) train_loss = -output.log_prob(train_y.view(-1, 1)).sum() train_losses.append(train_loss.detach()) test_loss = -model(test_x).log_prob(test_y.view(-1, 1)).sum() validation_losses.append(test_loss.detach()) train_loss.backward() optimizer.step() if epoch \u0026gt; swa_start: swa_model.update_parameters(model) swa_scheduler.step() else: scheduler.step() plt.plot(train_losses) plt.plot(validation_losses) plt.xlabel(\u0026#34;Training Epochs\u0026#34;) plt.ylabel(\u0026#34;Model Loss\u0026#34;) plt.title(\u0026#34;Training \u0026amp; Validation Losses\u0026#34;) plt.legend([\u0026#34;Training\u0026#34;, \u0026#34;Validation\u0026#34;]) swa_model.eval() output_mean = np.sqrt(np.exp(swa_model(test_x).mean.detach().numpy().squeeze())) y_trans = np.sqrt(np.exp(test_y.numpy().squeeze())) output_sample = np.sqrt(np.exp(swa_model(test_x).sample([5000]).numpy().squeeze())) Our out-of-sample R-squared is excellent, much higher than my previous simple linear model.\nregress = stats.linregress(output_mean, y_trans) print(f\u0026#34;R-squared: {regress.rvalue**2}\u0026#34;) R-squared: 0.7128714654332561 plt.plot(output_mean) plt.plot(y_trans) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Volatility\u0026#34;) plt.title(\u0026#34;Predicted and Actual Volatility\u0026#34;) plt.legend([\u0026#34;Model\u0026#34;, \u0026#34;Actual\u0026#34;]) Our distributional assumption also does well. We expect 5% of cases to be outside what the model distribution forecasts, and we find that to be the case.\npercent = np.percentile(output_sample, 95, axis=0) print(f\u0026#34;Number of exceedences: {(y_trans \u0026gt; percent).sum() / len(y_trans)}\u0026#34;) Number of exceedences: 0.04477611940298507 Further testing the distribution accuracy, let\u0026rsquo;s see if doing a probability integral transform yields a uniform.\nclass ECDF: def __init__(self, data): self.sorted = data self.sorted.sort() self.y = np.arange(1, len(self.sorted) + 1) / len(self.sorted) def __call__(self, x): ind = np.searchsorted(self.sorted, x) - 1 return self.y[ind] values = [] for x in range(len(y_trans)): ecdf = ECDF(output_sample[x]) values.append(ecdf(y_trans[x])) plt.hist(values, bins=10) stats.kstest(values, \u0026#34;uniform\u0026#34;) KstestResult(statistic=0.028702640642939155, pvalue=0.46125545362008036) We can\u0026rsquo;t reject the null hypothesis that the transformed values come from a uniform distribution! That means our distributions accurately models the data\u0026rsquo;s real distribution.\nConclusion This model seems quite excellent. I\u0026rsquo;m going to use this model for my future posts about how to make an effective trading strategy. Next time I\u0026rsquo;m going to discuss Kelly Bet Sizing and its application to continuous distributions.\n","permalink":"http://optionallybayes.com/posts/vol_mdn/","summary":"Using a mixture density neural network implemented in PyTorch to forecast the distribution of future realized volatility.","title":"Mixture Density Network for Forecasting Realized Volatility"},{"content":"So now that I\u0026rsquo;ve decided that I\u0026rsquo;m going to use 1-min RV as my volatility proxy, I can move on to the juicy part: forecasting.\nThe Data import pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) pd.plotting.register_matplotlib_converters() # Here\u0026#39;s my minute data for the S\u0026amp;P 500 spx_minute = minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#39;datetime\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;], index_col=\u0026#39;datetime\u0026#39;, parse_dates=True) # Here\u0026#39;s the function for calculating the 1-min RV, as discussed in my last post def rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) spx_rv = rv_calc(spx_minute) The Model My goal is to predict the volatility over the next week, or 5 trading days, with the past 5 days of daily volatility. This means my independent variables will be the last 5 days of volatility, and my dependent variable is the realized volatility over the next 5 days. For the sake of increased samples, I\u0026rsquo;m going to create a rolling 5-day window of volatility and shift it 5 periods backwards and use that as the dependent variable. This means I can create a 5-day volatility forecast for each day, rather than each week.\ndef create_lags(series, lags): \u0026#34;\u0026#34;\u0026#34; Creates a dataframe with lagged values of the given series. Generates columns named x_{n} which means the value of each row is the value of the original series lagged n times \u0026#34;\u0026#34;\u0026#34; result = pd.DataFrame(index=series.index) result[\u0026#34;x\u0026#34;] = series \u0026#34;\u0026#34; for n in range(lags): result[f\u0026#34;x_{n+1}\u0026#34;] = series.shift((n+1)) return result dep_var = spx_rv.rolling(5).sum().shift(-5).dropna() indep_var = create_lags(spx_rv, 5).dropna() # This ensures that we only keep rows that occur in each set. This means their length is the same and # rows match up properly common_index = dep_var.index.intersection(indep_var.index) dep_var = dep_var.loc[common_index] indep_var = indep_var.loc[common_index] # I\u0026#39;m going to take the log of the variance because it has better distributional qualities dep_var = np.log(dep_var) indep_var = np.log(indep_var) I\u0026rsquo;m going to use a very simple Bayesian linear regression for this model. It assumes the data is distributed according to\n$$y \\sim normal(\\mu + X\\beta, \\sigma)$$\nimport pystan as stan import arviz model_spec = \u0026#39;\u0026#39;\u0026#39; data { int len; int vars; vector[len] dep_var; matrix[len, vars] indep_var; } parameters { real mu; vector[vars] beta; real\u0026lt;lower=0\u0026gt; sigma; } model { mu ~ cauchy(0, 10); beta ~ cauchy(0, 10); sigma ~ cauchy(0, 5); dep_var ~ normal(mu + (indep_var * beta), sigma); } \u0026#39;\u0026#39;\u0026#39; model = stan.StanModel(model_code=model_spec) Model Testing and Verification Okay, let\u0026rsquo;s do some out of sample testing to see how our model does! Below, I\u0026rsquo;m defining the training and testing sets. I\u0026rsquo;m going to use 75% of the data for in-sample fitting and the remaining 25% for out-of-sample testing.\ntest_index = int(len(indep_var) * .75) train_x = indep_var.iloc[:test_index] train_y = dep_var[:test_index] test_x = indep_var.iloc[test_index:] test_y = dep_var[test_index:] Now, I fit the model to the data.\nparams = {\u0026#39;len\u0026#39;: len(train_x), \u0026#39;vars\u0026#39;: len(train_x.columns), \u0026#39;dep_var\u0026#39;: train_y, \u0026#39;indep_var\u0026#39;: train_x} sample = model.sampling(data=params, chains=4, warmup=250, iter=1500) Let\u0026rsquo;s check our sampling statistics to ensure the sampler converged. R-hats all look very good and our effective samples also look good.\nprint(sample.stansummary(pars=[\u0026#39;mu\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;])) Inference for Stan model: anon_model_842ef31b1beae12ccaeb1a8773757520. 4 chains, each with iter=1500; warmup=250; thin=1; post-warmup draws per chain=1250, total post-warmup draws=5000. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat mu 0.59 1.2e-3 0.09 0.42 0.53 0.59 0.65 0.77 5682 1.0 beta[1] 0.46 3.4e-4 0.02 0.42 0.45 0.46 0.47 0.5 3219 1.0 beta[2] 0.14 4.5e-4 0.02 0.1 0.13 0.14 0.16 0.18 2408 1.0 beta[3] 0.09 3.9e-4 0.02 0.04 0.07 0.09 0.1 0.13 3317 1.0 beta[4] 0.08 3.7e-4 0.02 0.03 0.06 0.08 0.09 0.12 3753 1.0 beta[5] 0.06 4.1e-4 0.02 0.01 0.04 0.06 0.07 0.1 2966 1.0 beta[6] 0.07 3.0e-4 0.02 0.03 0.06 0.07 0.08 0.11 4026 1.0 sigma 0.49 9.5e-5 6.9e-3 0.48 0.49 0.49 0.5 0.51 5295 1.0 Samples were drawn using NUTS at Wed Mar 17 19:28:01 2021. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). arviz_data = arviz.from_pystan( posterior=sample ) We can look at trace plots for our samples. Good samples should look like fuzzy caterpillars, which is what we see here. The distributions also match across sampling chains. The variables also match our intuition: $\\mu$ and $\\beta$ are positive, and the regression coefficients are all positive.\narviz.plot_trace(arviz_data, var_names=[\u0026#39;mu\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;]) The code below creates the posterior predictive distribution for the in-sample and out-of-sample data. These represent what the model predicts the distribution of the data is. The job now is to compare this predicted distribution to the reality.\nmu = sample[\u0026#39;mu\u0026#39;] beta = sample[\u0026#39;beta\u0026#39;] sigma = sample[\u0026#39;sigma\u0026#39;] # This is some tensordot sorcery that works, but that I don\u0026#39;t frankly understand. It takes the matrix product of train_x # and beta over each row of beta. Essentially a higher-dimensional version of what the model does. train_post = np.random.normal(mu + (np.tensordot(train_x, beta, axes=(1,1))), sigma) test_post = np.random.normal(mu + (np.tensordot(test_x, beta, axes=(1,1))), sigma) train_post_mean = np.mean(train_post, axis=1) test_post_mean = np.mean(test_post, axis=1) Let\u0026rsquo;s take a look at the in-sample and out-of-sample residuals. In this case, I\u0026rsquo;m making a point estimate by taking the mean of the posterior predictive distribution. It\u0026rsquo;s obvious that the model has problems predicting volatility jumps, signified by unexpected jumps in the residuals.\nplt.plot(np.exp(train_y) - np.exp(train_post_mean)) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Residual\u0026#39;) plt.title(\u0026#39;In-Sample Residuals\u0026#39;) plt.plot(np.exp(test_y) - np.exp(test_post_mean)) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Residual\u0026#39;) plt.title(\u0026#39;Out-of-Sample Residuals\u0026#39;) Now, let\u0026rsquo;s look at the root mean square error of our model. Looks like our out-of-sample RMSE, using exponentiated values, is around 7% higher, not bad!\ntrain_rmse = np.sqrt(np.mean((np.exp(train_y) - np.exp(train_post_mean))**2)) test_rmse = np.sqrt(np.mean((np.exp(test_y) - np.exp(test_post_mean))**2)) print(f\u0026#39;In-Sample RMSE: {train_rmse}\\nOut-of-Sample RMSE: {test_rmse}\u0026#39;) print(f\u0026#39;Percent Increase: {(test_rmse / train_rmse) - 1}\u0026#39;) In-Sample RMSE: 0.0006314456099670146 Out-of-Sample RMSE: 0.0006745751839390536 Percent Increase: 0.06830291206600037 I like to do a Mincer-Zarnowitz regression to analyze out-of-sample forests. In this case, the out-of-sample predictions are treated as the independent variable and the true values are the dependent variable. The R-Squared for out model is about 64%, which means our out-of-sample predictions explain 64% of the variance of the true values. Not bad! The intercept is also very close to zero, which means our prediction isn\u0026rsquo;t biased.\nregress = stats.linregress(np.exp(test_post_mean), np.exp(test_y)) print(f\u0026#39;Intercept: {regress.intercept} \\nSlope: {regress.slope} \\nR-Squared: {regress.rvalue**2}\u0026#39;) Intercept: 1.7250208362578126e-05 Slope: 1.183989352654772 R-Squared: 0.6438180914963003 Next, I want to check the distributional assumptions. Specifically, I want to know how many times real volatility exceeds what our distribution predicts. To do this, I\u0026rsquo;m going to look at the posterior predictive distribution, which should, if our model is correct, accurately predict the distribution of the real data. I\u0026rsquo;ll figure out the 95th percentile of the posterior predictive, and see how many times real volatility exceeded that. We should expect exceedances to happen about 5% of the time.\nupper_bound_train = np.percentile(np.exp(train_post), 95, axis=1) num_exceeds_train = (np.exp(train_y) \u0026gt; upper_bound_train).sum() upper_bound_test = np.percentile(np.exp(test_post), 95, axis=1) num_exceeds_test = (np.exp(test_y) \u0026gt; upper_bound_test).sum() print(f\u0026#39;In-Sample Exceedances: {num_exceeds_train / len(upper_bound_train)}\u0026#39;) print(f\u0026#39;Out-of-Sample Exceedances: {num_exceeds_test / len(upper_bound_test)}\u0026#39;) In-Sample Exceedances: 0.0481139337952271 Out-of-Sample Exceedances: 0.09815242494226328 In-sample we are within 5%, and out-of-sample we are above 5% by about double, which isn\u0026rsquo;t a good sign. Next up is testing the empirical distribution of the data. If our posterior predictive distribution is a good representation of the underlying distribution, doing a probability integral transform should transform the data into a uniform distribution.\nclass ECDF: def __init__(self, data): self.sorted = data self.sorted.sort() self.y = np.arange(1, len(self.sorted) + 1) / len(self.sorted) def __call__(self, x): ind = np.searchsorted(self.sorted, x) - 1 return self.y[ind] values = [] for x in range(len(test_post)): ecdf = ECDF(np.exp(test_post[x])) values.append(ecdf(np.exp(test_y[x]))) plt.hist(values) plt.title(\u0026#39;Transformed Data\u0026#39;) We can see an obvious deviation from the expected uniform distribution here. It looks like our distribution most significantly under-predicts large volatiltiy values. This makes sense when looking back to the residual graph, large jumps aren\u0026rsquo;t handled well.\nstats.kstest(values, \u0026#39;uniform\u0026#39;) KstestResult(statistic=0.0760443418013857, pvalue=8.408548699568476e-05) This Kolmogorov-Smirnov test takes the null hypothesis that the data matches the specified distribution, in this case a uniform. It looks like we can handedly reject that hypothesis. This means that the posterior predictive is not fully capable of representing the real distribution.\nConclusion and Extensions It seems like this very simple model does pretty well providing a point-forecast of future volatility, however it fails at accurately describing the distribution of future volatility. This could be fixed in several ways. First is assuming a different distributional form in the model, such as something with fatter tails like a Student\u0026rsquo;s T. Another possibility is allowing the standard deviation of the normal to vary with time. That is more in line with models like traditional stochastic volatility.\n","permalink":"http://optionallybayes.com/posts/vol_linear_model/","summary":"Using a simple bayesian autoregressive model to forecast future volatility","title":"Bayesian Autoregressive Volatility Forecasting"},{"content":"Today, I\u0026rsquo;m going to be discussing the difference between two volatility estimators.\nThe Data I\u0026rsquo;m going to be using daily-resolution SPX data from Sharadar as well as minute-resolution SPX data from First Rate Data.\nimport pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) conn = sqlite3.Connection(\u0026#34;data.db\u0026#34;) spx_daily = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^GSPC\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=[\u0026#34;date\u0026#34;]) spx_minute = minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#39;datetime\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;], index_col=\u0026#39;datetime\u0026#39;, parse_dates=True) # A quick look at the data spx_daily.head() ticker open high low close volume dividends closeunadj lastupdated date 1997-12-31 ^GSPC 970.84 975.02 967.41 970.43 467280000 0 970.43 2019-02-03 1998-01-02 ^GSPC 970.43 975.04 965.73 975.04 366730000 0 975.04 2019-02-03 1998-01-05 ^GSPC 975.04 982.63 969.00 977.07 628070000 0 977.07 2019-02-03 1998-01-06 ^GSPC 977.07 977.07 962.68 966.58 618360000 0 966.58 2019-02-03 1998-01-07 ^GSPC 966.58 966.58 952.67 964.00 667390000 0 964.00 2019-02-03 spx_minute.head() open high low close datetime 2007-04-27 12:25:00 1492.39 1492.54 1492.39 1492.54 2007-04-27 12:26:00 1492.57 1492.57 1492.52 1492.56 2007-04-27 12:27:00 1492.58 1492.64 1492.58 1492.63 2007-04-27 12:28:00 1492.63 1492.73 1492.63 1492.73 2007-04-27 12:29:00 1492.91 1492.91 1492.87 1492.87 The Estimators Now, what I want to do is compare volatility estimates from these two data sets. I would prefer to use the daily data if possible, because in my case it\u0026rsquo;s easier to get and updates more frequently.\nGarman-Klass Estimator This estimator has been around for a while and is deemed to be far more effcient than a traditional close-to-close volatility estimator (Garman and Klass, 1980).\nFrom equation 20 in the paper, a jump adjusted volatility estimator:\n$$f = 0.73$$ percentage of the day trading is closed based on NYSE hours of 9:30 to 4\n$$a = 0.12$$ as they suggest in the paper\n$$\\sigma^2_{unadj} = 0.511(u - d)^2 - 0.019(c(u+d) - 2ud) - 0.383c^2$$\n$$\\sigma^2_{adj} = 0.12\\frac{(O_{1} - C_{0})^2}{0.73} + 0.12\\frac{\\sigma^2_{unadj}}{0.27}$$\nWhere\n$$u = H_{1} - O_{1}$$ the normalized high\n$$d = L_{1} - O_{1}$$ the normalized low\n$$c = C_{1} - O_{1}$$ the normalized close and subscripts indicating time. They also indicate in the paper that these equations expect the log of the price series.\ndef gk_vol_calc(data): u = np.log(data[\u0026#39;high\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) d = np.log(data[\u0026#39;low\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) c = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) vol_unadj = 0.511 * (u - d)**2 - 0.019 * (c * (u + d) - 2 * u * d) - 0.283 * c**2 jumps = np.log(data[\u0026#39;open\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) vol_adj = 0.12 * (jumps**2 / 0.73) + 0.12 * (vol_unadj / 0.27) return vol_adj # Let\u0026#39;s take a look gk_vol = np.sqrt(gk_vol_calc(spx_daily)) gk_vol.plot() As an aside, opening jumps have become more common and larger in recent years, maybe something to investigate. This is as a percentage, so it\u0026rsquo;s not a simple case of the index values becoming larger.\n(spx_daily[\u0026#39;open\u0026#39;] / spx_daily[\u0026#39;close\u0026#39;].shift(1) - 1).plot() Realized Volatility Estimator This estimator is very simply and has become more prominent in the literature in the last few years because of increasing availability of higher-frequency data. Based on (Liu, Patton, and Sheppard, 2012), it\u0026rsquo;s hard to beat a 5-minute RV. Here, I\u0026rsquo;m going to use a 1-minute estimator, which is also shown to be effective.\n$$RV_{t} = \\sum_{k=1}^n r_{t,k}^2$$ where the t index is each day, and the k index represents each intraday return\nFor daily volatility, it\u0026rsquo;s simply a sum of squared returns from within that day. So in this case we calculate returns for each 1 minute period, square them, and they sum them for each day.\ndef rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) # Let\u0026#39;s take a look at this one rv = np.sqrt(rv_calc(spx_minute)) rv.plot() Comparisons # Because the minute data has a shorter history, let\u0026#39;s match them up gk_vol = gk_vol.reindex(rv.index) rv.plot() gk_vol.plot() Here\u0026rsquo;s a plot of our two different volatility estimators with RV in blue and Garman-Klass in orange. The RV estimator is far less noisy, looking at each of their graphs above. The Garman-Klass estimator also seems to persistently return a lower result than RV. This is backed up by looking at a graph of their difference.\n(gk_vol - rv).plot() Netx, let\u0026rsquo;s analyze how they do at normalizing the returns of the S\u0026amp;P 500. According to (Molnr, 2015) normalizing a number of equity returns by their Garman-Klass estimated volatility does indeed make their distributions normal. Let\u0026rsquo;s see if we can replicate that result with either of our esimates on the S\u0026amp;P 500.\n# Daily close-to-close returns of the S\u0026amp;P 500 spx_returns = np.log(spx_daily[\u0026#39;close\u0026#39;]) - np.log(spx_daily[\u0026#39;close\u0026#39;].shift(1)) spx_returns = spx_returns.reindex(rv.index) # Normalizing by our estimated volatilties gk_vol_norm = (spx_returns / gk_vol).dropna() rv_norm = (spx_returns / rv).dropna() # Here are the unadjusted returns _, _, _ = plt.hist(spx_returns, bins=50) # Here\u0026#39;s normalized by the Garman-Klass Estimator _, _, _ = plt.hist(gk_vol_norm, bins=50) # And this is by the RV estimator _, _, _ = plt.hist(rv_norm, bins=50) At first glance, the RV adjusted returns seem most like normal to me, let\u0026rsquo;s run some tests. These Scipy tests set the null hypothesis that the data comes from a corresponding normal distribution. So if the p-value is small we can reject that hypothesis and conclude the distribution is non-normal.\nprint(stats.skewtest(gk_vol_norm)) print(stats.skewtest(rv_norm)) Garman-Klass Skew: SkewtestResult(statistic=-0.3767923327324783, pvalue=0.7063279391177064) RV-5min Skew: SkewtestResult(statistic=5.251294175425576, pvalue=1.5103423951480544e-07) print(stats.kurtosistest(gk_vol_norm)) print(stats.kurtosistest(rv_norm)) KurtosistestResult(statistic=-13.088609427904334, pvalue=3.825472809774632e-39) KurtosistestResult(statistic=0.315320709120601, pvalue=0.7525181628202805) Looks like the Garman-Klass-normalized returns have normal skew, but non-normal kurtosis. The RV-normalized returns have non-normal skew but normal kurtosis! There\u0026rsquo;s no winning here! Both are non-normal in different ways. Either normalization does do better than the unadjusted returns though.\nprint(stats.skewtest(spx_returns.dropna())) print(stats.kurtosistest(spx_returns.dropna())) SkewtestResult(statistic=-12.386230904806132, pvalue=3.1028724633560147e-35) KurtosistestResult(statistic=26.470418979318143, pvalue=2.124045513612033e-154) Conclusion While from a statistical point of view, neither option seems particularly favorable, my personal choice is going to be the RV estimator. I think the literature is clear on its efficacy and its less noisy and conceptually easier. It\u0026rsquo;s been said that when there are a bunch of competing theories, none of them are very good. So I\u0026rsquo;ll pick the simplest option and go with RV.\nReferences Garman, M., \u0026amp; Klass, M. (1980). On the Estimation of Security Price Volatilities from Historical Data. The Journal of Business, 53(1), 67-78. Retrieved February 14, 2021, from http://www.jstor.org/stable/2352358\nLiu, L., Patton, A., \u0026amp; Sheppard, K. (2012). Does Anything Beat 5-Minute RV? A Comparison of Realized Measures Across Multiple Asset Classes. SSRN. http://dx.doi.org/10.2139/ssrn.2214997\nMolnr, P. (2015). Properties of Range-Based Volatility Estimators. SSRN. Retrieved from https://ssrn.com/abstract=2691435\n","permalink":"http://optionallybayes.com/posts/vol_estimators/","summary":"Comparing Garman-Klass estimator to 5-minute Realized Volatility estimator.","title":"Comparison of Volatility Estimators"},{"content":"Alright, with this post I\u0026rsquo;m going to start a series on portfolio optimization techniques! This is one of my favorite topics in finance. This post is going to construct a portfolio based on the diversification ratio, which is outlined in the papers linked below. The basic idea is to maximize the Diversification ratio, which is defined as the weighted average volatilities of assets in the portfolio divided by the total portfolio volatility. This makes intuitive sense, by increasing diversification we lower portfolio volatility compared to the average volatility of the assets that make it up.\nChoueifaty, Y., \u0026amp; Coignard, Y. (2008). Toward Maximum Diversification. The Journal of Portfolio Management, 40-51. doi:https://doi.org/10.3905/JPM.2008.35.1.40\nChoueifaty, Y., Reynier, J., \u0026amp; Froidure, T. (2013). Properties of the Most Diversified Portfolio. Journal of Investment Strategies, 49-70. doi:http://doi.org/10.2139/ssrn.1895459\nThe assets I\u0026rsquo;m going to mainly focus on Vanguard ETFs as they have the lowest fees. For anything they don\u0026rsquo;t offer, I\u0026rsquo;m using iShares. I\u0026rsquo;m also limiting myself to funds with inception dates \u0026gt;10 years ago for stability.\nHere\u0026rsquo;s the list: Symbol Description VGSH Short-term Treasury VGIT Mid-term Treasury VGLT Long-term Treasury TIP TIPS Treasury Bonds VMBS Agency MBS SUB Municipal Bonds VCSH Short-term Investment Grade Corporate Bonds VCIT Mid-term Investment Grade Corporate Bonds VCLT Long-term Investment Grade Corporate Bonds HYG High-yield Corporate Bonds EMB Emerging Markets Bonds IGOV International Treasuries VV Large Cap US Stocks VO Mid-Cap US Stocks VB Small-Cap US Stocks VWO Emerging Markets Stocks VEA Non-US Developed Markets Stocks IYR US Real Estate IFGL Non-US Real Estate Data All of this is the code to fetch historical data from QuantConnect and calculate returns.\nimport numpy as np import pandas as pd symbols = [\u0026#39;VGSH\u0026#39;, \u0026#39;VGIT\u0026#39;, \u0026#39;VGLT\u0026#39;, \u0026#39;TIP\u0026#39;, \u0026#39;VMBS\u0026#39;, \u0026#39;SUB\u0026#39;, \u0026#39;VCSH\u0026#39;, \u0026#39;VCIT\u0026#39;, \u0026#39;VCLT\u0026#39;, \u0026#39;HYG\u0026#39;, \u0026#39;EMB\u0026#39;, \u0026#39;IGOV\u0026#39;, \u0026#39;VV\u0026#39;, \u0026#39;VO\u0026#39;, \u0026#39;VB\u0026#39;, \u0026#39;VWO\u0026#39;, \u0026#39;VEA\u0026#39;, \u0026#39;IYR\u0026#39;, \u0026#39;IFGL\u0026#39;] qb = QuantBook() symbols_data = {symbol: qb.AddEquity(symbol) for symbol in symbols} from datetime import datetime # This is QuantConnect API code to get price history history = qb.History(qb.Securities.Keys, datetime(2009, 1, 1), datetime(2020, 12, 31), Resolution.Daily) history = history[\u0026#39;close\u0026#39;].unstack(level=0).dropna() I\u0026rsquo;m using arithmetic returns here so I can easily weight the returns across assets when computing portfolio returns.\nreturns = (history / history.shift(1)) - 1 returns = returns.dropna() # Let\u0026#39;s define some helper functions to get cumulative return series and the total return def get_cum_returns(returns): return (returns + 1).cumprod() - 1 def get_total_return(returns): return np.product(returns + 1) - 1 The Optimization This function calculates the diversification ratio for a portfolio given asset weights and their covariance matrix. This is from equation (1) (Choueifaty \u0026amp; Coignard, 2008).\ndef diverse_ratio(weights, covariance): # Standard deviation vector stds = np.sqrt(np.diagonal(covariance)) # Asset-weighted standard deviation num = np.dot(weights, stds) # Portfolio standard deviation denom = np.sqrt(weights @ covariance @ weights) return num / denom Now, to confirm that scipy minimize works as we expect for this problem, I\u0026rsquo;m going to test a bunch of randomized starting weights to confirm that the final weights end up the same. I increase the level of precision using the \u0026lsquo;ftol\u0026rsquo; option because returns are fairly small decimal quantities and I want to ensure the optimization converges completely.\nfrom scipy.optimize import minimize cov = np.cov(returns.values.T) # Long-only constraint bounds = [(0, 1) for x in range(len(cov))] # Portfolio weights must sum to 1 constraints = ( {\u0026#39;type\u0026#39;: \u0026#39;eq\u0026#39;, \u0026#39;fun\u0026#39;: lambda x: np.sum(x) - 1} ) results = [] for x in range(100): # Set initial weights randomly initial = np.random.random(len(cov)) # Use negative of objective function to maximize result = minimize(lambda x, y: -1 * diverse_ratio(x, y), initial, method=\u0026#39;SLSQP\u0026#39;, args=(cov), bounds=bounds, constraints=constraints, options={\u0026#39;ftol\u0026#39;: 1e-10}) results.append(result.x) # Stack all optimized weight vectors, and round to 4 digits after the decimal results_array = np.round(np.stack(results), 4) # Let\u0026#39;s look at the standard deviation of the asset weights accross the different optimizations stds = np.std(results_array, axis=0) # Looks like they\u0026#39;re all zero or nearly zero! print(stds) [0.00000000e+00 3.12250226e-17 0.00000000e+00 1.24900090e-16 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.16333634e-17 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.08166817e-17 1.66533454e-16 1.38777878e-16 1.66533454e-16 0.00000000e+00 0.00000000e+00 1.24900090e-16 2.42861287e-17] Looks like the optimization converges to the same values every time regardless of starting point! This means it\u0026rsquo;s finding the true minimum (or maximum in this case). Let\u0026rsquo;s look the at the weights for each symbol.\n# I\u0026#39;ll just grab the last optimization result this way weights_series = pd.Series(data=np.round(result.x, 4), index=returns.columns) print(weights_series) EMB 0.0000 HYG 0.0215 IFGL 0.0000 IGOV 0.0586 IYR 0.0000 SUB 0.2759 TIP 0.0000 VB 0.0336 VCIT 0.0000 VCLT 0.0000 VCSH 0.0000 VEA 0.0265 VGIT 0.1150 VGLT 0.1868 VGSH 0.1825 VMBS 0.0000 VO 0.0000 VV 0.0817 VWO 0.0179 # Let\u0026#39;s drop everything with a zero weight final_weights = weights_series[weights_series \u0026gt; 0] # Sort by weight for viewing ease print(final_weights.sort_values(ascending=False)) SUB 0.2759 VGLT 0.1868 VGSH 0.1825 VGIT 0.1150 VV 0.0817 IGOV 0.0586 VB 0.0336 VEA 0.0265 HYG 0.0215 VWO 0.0179 # Confirm everything sums to 1. Looks good! final_weights.sum() 0.9999999999999999 Backtest Results After doing some offscreen magic to implement this in QuantConnect, we get a dataframe tracking portfolio weights over each month. The algorithm starts at the beginning of 2011 and runs to the end of 2020. On the first trading day of each month it computes the portfolio using the code above using the past year of returns data for each ticker. Note that this is slightly different than the above that uses the entire return history in the optimization. Let\u0026rsquo;s take a closer look at a particular ETFs portfolio allocation over time. I\u0026rsquo;m going to use VGSH for this because it\u0026rsquo;s the least risky, most cash-like instrument under consideration.\nYou can see that the weight changes quite drastically over time, from near zero to nearly 90% in the later parts of 2020. This reflects the nature of how we are calculating asset variances and correlations using only the last 252 days of data. When volatilities or correlations change it causes changes in the allocation.\nweights_frame[\u0026#39;VGSH\u0026#39;].plot(figsize=(15,10)) In this case, it\u0026rsquo;s caused by a large change in correlation for certain assets in early 2020. Shown in the graph below is the correlation between VGSH and the other ETFs over time. Note the large downward jump on the right side. This shows the weakness of using a rolling data approach like in the backtest. You get big market jumps that dramatically shift your allocation and then when they eventually fall out the backward-looking window, you get big jumps again. I want to come back to this topic some time in the future.\nrolling_corr = returns.rolling(252).corr() rolling_corr[\u0026#39;VGSH\u0026#39;].unstack().plot(figsize=(15,10)) # let\u0026#39;s reindex the monthly weights frame to daily with forward fill to match returns arra weights_frame = weights_frame.reindex(returns.index, method=\u0026#39;ffill\u0026#39;) # Now we can calculate portfolio returns by weight the returns and summing port_returns = (weights_frame.values * returns).sum(axis=1, skipna=False).dropna() # We can also calculate cumulative returns this way because we\u0026#39;re working with logarithmic returns cum_port_returns = get_cum_returns(port_returns) Alright, plotted below are the cumulative returns for the strategy! Note this is without transaction costs factored in.\ncum_port_returns.plot(figsize=(15, 10)) Now let\u0026rsquo;s assemble some backtest statistics. We\u0026rsquo;re going to be using mlfinlab for this task.\nfrom mlfinlab import backtest_statistics total_return = get_total_return(port_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(port_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(cum_port_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 29.77% CAGR 2.94% Sharpe Ratio 1.15 Maximum Drawdown 7.07% MAR Ratio 0.42 Let\u0026rsquo;s compare that to just US large cap stocks over the same period.\nvv_returns = returns[\u0026#39;VV\u0026#39;][\u0026#39;2011\u0026#39;:] vv_cum_returns = get_cum_returns(vv_returns) total_return = get_total_return(vv_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(vv_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(vv_cum_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 267.74% CAGR 15.57% Sharpe Ratio 0.84 Maximum Drawdown 34.28% MAR Ratio 0.45 Looks like the maximum diversification portfolio achieves a higher sharpe ratio! Although it comes at the cost of signficantly lower total returns. More interesting is the MAR ratio, defined as the CAGR over the maximum drawdown. This is a useful ratio because it gauges how much extra return you are getting for taking on heavier drawdown risk. It looks like large cap US stocks win out on this metric.\nIt gives a different perspective than the Sharpe ratio. The Sharpe ratio uses only standard deviation as a metric for risk. This can be very unrealistic because radically different equity curves can actually have the same Sharpe ratio and total return. That can be interestingly illustrated by reordering returns.\n# Okay, let\u0026#39;s sort VV returns from least to greatest. Note that these are the same returns, just reordered. sorted_returns = pd.Series(sorted(vv_returns.values), index=vv_returns.index) cum_sorted_returns = get_cum_returns(sorted_returns) # Here you can see the cumulative return graphs. The sorted one looks very unusual, but in fact, the total return ends # up exactly the same! cum_sorted_returns.plot(figsize=(15, 10)) vv_cum_returns.plot() total_return = get_total_return(sorted_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(sorted_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(cum_sorted_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 267.74% CAGR 15.57% Sharpe Ratio 0.84 Maximum Drawdown 99.97% MAR Ratio 0.16 As you can see the total return, CAGR, and Sharpe ratio are all the same as the original return series! But the maximum drawdown is significantly higher. Obviously this is a worst case scenario, but it shows how drawdowns can drastically affect your portfolio performance over time. Volatility by itself doesn\u0026rsquo;t reflect all kinds of risk because it ignores path dependency. This again is a topic worth covering in more detail at a later date.\nConclusions Even considering the backtest and attributes of this simple strategy shows deep complexity. In future, I want to compare this optimization strategy to others like the traditional mean-variance approach, hierarchical risk parity, minimum variance, and others. Along with that is discussing extensions like using models to provide forecasts for asset volatility and correlation.\nSo with all those things to think about, see you next time!\n","permalink":"http://optionallybayes.com/posts/mdp_etf_portfolio/","summary":"Constructing a portfolio from a selection of ETFs to maximize the diversification ratio","title":"A Most Diversified ETF Portfolio"}]
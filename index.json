[{"content":"","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/kelly_multiperiod/","summary":"Exploring how mean-variance methods and the Kelly Criterion compare in multi-period investments","title":"Exploring the Kelly Criterion: The Multi-Period Problem"},{"content":"","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/kelly_drawdown/","summary":"Exploring the Kelly Criterion and its drawdown properties using Monte Carlo","title":"Exploring the Kelly Criterion: Drawdown"},{"content":"In this post I want to apply clustering to ETFs to determine the most effective diversified portfolio. There are a lot of ETFs out there, in my database I count 3612, included delisted ones. If I wanted to build a simple, well-diversified portfolio to hold long-term, which ones should I pick? Would I do better with an S\u0026amp;P 500 ETF or a total stock market ETF? What kind of bond ETFs provide the most diversification to a portfolio? Clustering can help to answer these kinds of questions.\nData I\u0026rsquo;m immediately going to introduce one simplification to this problem, which is to restrict which tickers are analyzed. Firstly, I\u0026rsquo;m going to use only ETFs that have price histories before the 2008 crash in September. Secondly, I\u0026rsquo;m going to make some stylistic choices. I like Vanguard, so I\u0026rsquo;m including all of their ETFs that meet the date criteria. However, there are some asset groups they don\u0026rsquo;t offer ETFs for, so I\u0026rsquo;m also including some iShares ETFs to fill the gaps for things like real estate and junk bonds.\nIn total, this brings the total to 31 tickers, which is a far cry from how many I could include. This makes the results much easier to interpret and more practical.\nimport psycopg2 as pg import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits import mplot3d from sklearn.cluster import AffinityPropagation from sklearn.covariance import GraphicalLassoCV from sklearn.manifold import LocallyLinearEmbedding from config import DATABASE_URI plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,10) # All vanguard funds plus HYG, EMB, IYR, and SPY for junk bonds, emerging markets bonds, real estate, and the S\u0026amp;P 500 # respectively. # All created before the 2008 crash, IE before september 29 2008 tickers = [\u0026#34;EDV\u0026#34;, \u0026#34;BIV\u0026#34;, \u0026#34;BLV\u0026#34;, \u0026#34;BSV\u0026#34;, \u0026#34;BND\u0026#34;, \u0026#34;VIG\u0026#34;, \u0026#34;VUG\u0026#34;, \u0026#34;VYM\u0026#34;, \u0026#34;VV\u0026#34;, \u0026#34;MGC\u0026#34;, \u0026#34;MGK\u0026#34;, \u0026#34;MGV\u0026#34;, \u0026#34;VTI\u0026#34;, \u0026#34;VTV\u0026#34;, \u0026#34;VXF\u0026#34;, \u0026#34;VO\u0026#34;, \u0026#34;VOT\u0026#34;, \u0026#34;VOE\u0026#34;, \u0026#34;VB\u0026#34;, \u0026#34;VBK\u0026#34;, \u0026#34;VBR\u0026#34;, \u0026#34;VT\u0026#34;, \u0026#34;VEU\u0026#34;, \u0026#34;VEA\u0026#34;, \u0026#34;VGK\u0026#34;, \u0026#34;VPL\u0026#34;, \u0026#34;VWO\u0026#34;, \u0026#34;HYG\u0026#34;, \u0026#34;EMB\u0026#34;, \u0026#34;IYR\u0026#34;, \u0026#34;SPY\u0026#34;] with pg.connect(DATABASE_URI) as conn: with conn.cursor() as cur: cur.execute(f\u0026#34;SELECT date, ticker, closeadj FROM prices WHERE ticker IN {tuple(tickers)}\u0026#34;) results = cur.fetchall() df = pd.DataFrame.from_records(results, columns=[\u0026#34;date\u0026#34;, \u0026#34;ticker\u0026#34;, \u0026#34;closeadj\u0026#34;], coerce_float=True) # Set index, sort index, then transform into Series via squeeze df = df.set_index([\u0026#34;date\u0026#34;, \u0026#34;ticker\u0026#34;], verify_integrity=True).sort_index().squeeze() returns = df.unstack().pct_change().dropna() Estimating Structure Alright, so now I have a big matrix of daily returns for 31 different ETFs, what can we say about their structure? An obvious way to look at this is by calculating a covariance matrix. However, this problem brings some challenges here. Luckily, because of the length of the data, I don\u0026rsquo;t have to worry about the number of features being larger than the number of data points.\nThe problem really is that estimating the empirical covariance matrix doesn\u0026rsquo;t do well at uncovered structure. What I\u0026rsquo;m looking for is a graphical model that links together the ETFs we have. I want to know which are meaningfully correlated with which others and which have no meaningful correlation. An empirical covariance matrix will mostly say everything is at least a little correlated with everything else. Which while may be true, isn\u0026rsquo;t useful for what I want to do.\nWhat I need is a sparse covariance matrix. When two tickers don\u0026rsquo;t have much to do with each other, I want that covariance value to be zero. This is what a Graphical Lasso will do. Exactly as in Lasso-regularized regression, this procedure shrinks values towards zero.\n# Standardizing improves estimation of sparse covariance matrix X = returns.values.copy() X /= X.std(axis=0) edge_model = GraphicalLassoCV().fit(X) plt.matshow(np.cov(returns.T)) plt.title(\u0026#34;Normal Covariance\u0026#34;)  plt.matshow(edge_model.covariance_) plt.title(\u0026#34;Sparse Covariance\u0026#34;)  In the two matrices above you can clearly see the differences. Much of the second matrix is zero, but the first one has a mix of near-zero values. The Graphical Lasso shows us which connections are the most important. Note that the diagonal elements no longer represent each tickers individual variance, but we don\u0026rsquo;t need that anyways.\nClustering Okay so we have a matrix that tells us which ETFs are structurally related, so we can move the estimating clusters. I\u0026rsquo;m going to use Affinity Propagation for this because of two reasons: it selects the number of clusters automatically, and it provides a member of each cluster that best represents it. The latter reason is the coolest feature of this method. After it finds out which ETFs belong to which cluster, it will tell us which one of them best represents each cluster. This is exactly what we want! I don\u0026rsquo;t want to deal with 31 different ETFs, I want to deal with a small number that best represents the whole group. This is essentially a dimensionality reduction problem.\nI\u0026rsquo;m using a slightly smaller preference value than sklearn would use because I want the number of cluster to be smaller. This is a flexible value. Sklearn by default sets it as the median of the affinity matrix, in this case the covariance matrix. If you set it smaller, there will be fewer clusters, and bigger means more clusters.\nclustering = AffinityPropagation(affinity=\u0026#34;precomputed\u0026#34;, preference=0.15).fit(edge_model.covariance_) n_labels = clustering.labels_.max() cluster_centers = returns.columns[clustering.cluster_centers_indices_] for i in range(n_labels + 1): print(f\u0026#34;Cluster {i+1}: {\u0026#39;, \u0026#39;.join(returns.columns[clustering.labels_ == i])}\u0026#34;) print(f\u0026#34; Cluster Representative: {cluster_centers[i]}\u0026#34;) print(\u0026#34;\\n\u0026#34;) Cluster 1: BIV, BLV, BND, EDV Cluster Representative: BLV Cluster 2: BSV Cluster Representative: BSV Cluster 3: EMB Cluster Representative: EMB Cluster 4: HYG Cluster Representative: HYG Cluster 5: VEA, VEU, VGK, VPL, VT, VWO Cluster Representative: VEU Cluster 6: IYR, MGC, MGK, MGV, SPY, VB, VBK, VBR, VIG, VO, VOE, VOT, VTI, VTV, VUG, VV, VXF, VYM Cluster Representative: VTI Okay, so we get 6 clusters from our 31 ETFs. Looking at them qualitatively, it matches with our expectations. Cluster 1 is longer term bonds, cluster 5 is non-US equities, and cluster 6 is US equities. Looking at which are selected as being representative, we get a very intuitive answer: long term bonds, short term bonds, emerging market bonds, junk bonds, non-US equities, and US total stock market. One slightly unexpected grouping is that real estate (IYR) is grouped with equities.\nVisualization For some extra fun, how do we visualize this? We have the concept of clusters being things that are close together in space, but in this case, what does space even mean? It\u0026rsquo;s very high-dimensional and non-intuitive. Well, luckily there is a way to attempt to embed higher-dimensional space into lower-dimensional space called Manifold learning. This method tries to find a way in two-dimensions to best represent patterns and groupings in higher-dimensions.\nembed = LocallyLinearEmbedding(n_neighbors=10, n_components=2) embed = embed.fit_transform(edge_model.covariance_) plt.scatter(embed[:, 0], embed[:, 1], c=clustering.labels_, cmap=\u0026#34;Set1\u0026#34;) labels = returns.columns[clustering.cluster_centers_indices_] points = embed[clustering.cluster_centers_indices_] for i, label in enumerate(labels): plt.annotate(label, (points[i, 0], points[i, 1]))  So, here we can see each of our clusters color-coded with the representative ETF from each cluster labeled. You can obviously see the clustering: bonds in red on the left, non-US equities pink on the bottom right, and US equities in grey in the upper right. Note the overlaid text between EMB and BSV on the left-hand side. The clustering algorithm views them as being distinct, but they are right on top of each other in this 2d embedding. This most likely means there is some high-dimensional difference between them that cannot be projected downward into 2 dimensions.\nConclusion This is way a casual look into clustering applied to ETFs. There\u0026rsquo;s a lot more you can do with this. There are more sophisticated estimation methods that are potentially non-linear, and you can obviously greatly expand the number of tickers under analysis. This can also be applied to groups of stocks, which can be useful for finding pairs trades.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/etf_clustering/","summary":"Using clustering algorithms for identifying optimal subset of ETFs for portfolio construction.","title":"Clustering ETFs for Optimally Diversified Portfolio"},{"content":"Today I\u0026rsquo;ll be running through Stochastic Volatility Models! These are related to GARCH models in that they allow for time-varying volatility in the return distribution. In other words, it accounts for heteroscedasticity.\nData I\u0026rsquo;m interested in the weekly returns of the S\u0026amp;P 500 index. My intent is to trade weekly options to go short volatility, so weekly forecasts are what I need.\nimport numpy as np import pandas as pd from cmdstanpy import cmdstan_path, CmdStanModel import matplotlib.pyplot as plt import arviz as az from scipy import stats import statsmodels.api as sm from psis import psisloo from datamodel import SPX, StockData plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,10) spx = SPX() spx_wk_prices = spx.prices.resample(\u0026#34;W-FRI\u0026#34;).last() spx_wk_returns = (np.log(spx_wk_prices) - np.log(spx_wk_prices.shift(1))).dropna() Here are what the weekly returns look like for the past 20-ish years. You can see that volatility \u0026ldquo;clusters\u0026rdquo; meaning that periods of extreme returns are generally followed by periods of extreme returns.\nspx_wk_returns.plot(title=\u0026#34;S\u0026amp;P 500 Index Weekly Returns\u0026#34;, xlabel=\u0026#34;Date\u0026#34;, ylabel=\u0026#34;Return\u0026#34;)  This can be more easily seen with autocorrelation plots. Let\u0026rsquo;s look at the returns themselves first:\nfig = sm.tsa.graphics.plot_acf(spx_wk_returns, title=\u0026#34;Returns Autocorrelation\u0026#34;) plt.xlabel(\u0026#34;Lags\u0026#34;) plt.ylabel(\u0026#34;Correlation\u0026#34;)  There\u0026rsquo;s very little autocorrelation, meaning that returns at each time period are unrelated to the returns that came at past time periods. However, let\u0026rsquo;s look at the square of returns, which is a crude way to estimate their volatility.\nfig = sm.tsa.graphics.plot_acf(spx_wk_returns**2, title=\u0026#34;Squared Returns Autocorrelation\u0026#34;) plt.xlabel(\u0026#34;Lags\u0026#34;) plt.ylabel(\u0026#34;Correlation\u0026#34;)  Now there is clearly some significant autocorrelation, meaning volatility is affected by past volatility, thus the clustering effect. When there is a volatility shock, we expect to see periods of lasting higher volatility.\nModel 1 I\u0026rsquo;m going to be fitting a Stochastic Volatility Model which differs from a standard GARCH model. In a GARCH model, variance is modeled as a deterministic function of past errors and past variances:\n$$ \\sigma_{t}^2 = \\omega + \\alpha_{1} \\epsilon_{t-1}^2 + \\beta_{1} \\sigma_{t-1}^2 $$\nHowever, in a Stochastic Volatility Model, variance is modeled as a stochastic function of past variance:\n$$ \\sigma_{t}^2 = \\mu + \\phi (\\sigma_{t-1}^2 - \\mu) + \\epsilon_{t}$$\n$$ \\epsilon_{t} \\sim \\mathcal{N}(0, \\omega) $$\nThis model is what is encapsulated below in Stan model language. To use the symbols below it\u0026rsquo;s like this:\n$$ r_{t} \\sim \\mathcal{N}(\\mu_{r}, \\exp(\\frac{h_{t}}{2})) $$\n$$ h_{t} = \\mu_{h} + \\phi (h_{t-1} - \\mu_{h}) + \\epsilon_{t}$$\n$$ \\epsilon_{t} \\sim \\mathcal{N}(0, \\sigma) $$\nNote here that \\(h\\) represents the logarithm of variance. This makes its distribution far more symmetrical than in its normal form, making fitting the model easier. The gist of the model is that there exists a normal mean variance level represented by \\(\\mu_{h}\\) and when shocks occur, whose magnitude is governed by \\(\\sigma\\), variance will tend back towards that mean at a rate dictated by \\(\\phi\\).\nI generate the posterior predictive distribution in the generated quantities block, this will be useful for analysis of the model. I\u0026rsquo;m using broad uninformative priors here because I have plenty enough data points that they hardly matter.\nmodel_spec = \u0026#34;\u0026#34;\u0026#34; data { int N; // Length of data vector[N] r; // SPX returns } parameters { real mu_h; // Volatility mean term real mu_r; // Returns mean term real\u0026lt;lower=-1, upper=1\u0026gt; phi; // Persistence of volatility real\u0026lt;lower=0\u0026gt; sigma; // Volatility noise vector[N] h_std; // Log volatility } transformed parameters { vector[N] h = h_std * sigma; // h ~ normal(0, sigma); h[1] /= sqrt(1 - square(phi)); // h[1] ~ normal(0, sigma / sqrt(1 - square(phi))) h += mu_h; // h ~ normal(mu_h, sigma) for (t in 2:N) { h[t] += phi * (h[t-1] - mu_h); // h[2:N] ~ normal(mu_h + phi * (h[t-1] - mu_h), sigma) } } model { phi ~ uniform(-1, 1); sigma ~ normal(0, 10); mu_h ~ normal(0, 10); mu_r ~ normal(0, 10); h_std ~ std_normal(); r ~ normal(mu_r, exp(h / 2)); } generated quantities { real r_tilde[N]; real log_prob[N]; r_tilde = normal_rng(mu_r, exp(h / 2)); for (t in 1:N) { log_prob[t] = normal_lpdf(r[t] | mu_r, exp(h[t] / 2)); // Need log probabilities later on } } \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;./stan_model/model.stan\u0026#34;, \u0026#34;w\u0026#34;) as file: file.write(model_spec) model = CmdStanModel(stan_file=\u0026#34;./stan_model/model.stan\u0026#34;) data = {\u0026#34;N\u0026#34;: len(spx_wk_returns), \u0026#34;r\u0026#34;: spx_wk_returns.values} sample = model.sample(data=data, chains=4, parallel_chains=4, output_dir=\u0026#34;./stan_model\u0026#34;, iter_warmup=1000, iter_sampling=2500) model1_data = az.from_cmdstanpy(posterior=sample, posterior_predictive=\u0026#34;r_tilde\u0026#34;, observed_data={\u0026#34;r\u0026#34;: spx_wk_returns.values}, log_likelihood=\u0026#34;log_prob\u0026#34;) First let\u0026rsquo;s look at how our chain sampled to make sure everything looks okay.\naz.plot_trace(model1_data, compact=True, var_names=[\u0026#34;mu_h\u0026#34;, \u0026#34;mu_r\u0026#34;, \u0026#34;phi\u0026#34;, \u0026#34;sigma\u0026#34;])  Okay there is no obvious issues here. The parameter distributions from each chain look mostly the same, and there aren\u0026rsquo;t any obvious signs of autocorrelation in the samples. Next, let\u0026rsquo;s look at the summary statistics from our posterior predictive distribution versus our data. The blue histogram bars represent the posterior predictive, and the black line represents that statistic calculated from the data.\nr_tilde = model1_data.posterior_predictive.r_tilde.values.reshape(10000, -1) vol = model1_data.posterior.h.values.reshape(10000, -1) mean = np.mean(r_tilde, axis=1) std = np.std(r_tilde, axis=1) skew = stats.skew(r_tilde, axis=1) kurt = stats.kurtosis(r_tilde, axis=1) fig, axs = plt.subplots(2, 2) axs[0, 0].hist(mean, bins=50) axs[0, 0].axvline(np.mean(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 0].set_title(\u0026#34;Mean\u0026#34;) axs[0, 1].hist(std, bins=50) axs[0, 1].axvline(np.std(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 1].set_title(\u0026#34;Standard Deviation\u0026#34;) axs[1, 0].hist(skew, bins=50) axs[1, 0].axvline(stats.skew(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 0].set_title(\u0026#34;Skew\u0026#34;) axs[1, 1].hist(kurt, bins=50) axs[1, 1].axvline(stats.kurtosis(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 1].set_title(\u0026#34;Kurtosis\u0026#34;)  There are some issues here, right off the bat. Ideally, the black line should fall in a high probability region of the histogram. This would mean that the data simulated from our model closely matches the qualities of the input data. This looks true mostly only for the standard deviation and kurtosis. It seems like the model is not modeling the mean or skew very well. Next let\u0026rsquo;s look at the distribution of our input data versus the distribution of the posterior predictive.\naz.plot_ppc(model1_data, data_pairs={\u0026#34;r\u0026#34;: \u0026#34;r_tilde\u0026#34;})  This looks pretty good! The distributions look mostly the same. Next, I want to look at how well calibrated the model is. The model outputs a distributional estimate at each time point. So ideally, for instance, if we calculate the 95th percentile of that distribution, the input data should have values higher than that only 5% of the time. Likewise that data should have values smaller than the 5% percentile only 5% of the time.\n# 95% bounds exceedances np.sum(spx_wk_returns.values \u0026gt; np.percentile(r_tilde, 95, axis=0)) / len(spx_wk_returns) 0.015459723352318959 # 5% bounds exceedances np.sum(spx_wk_returns.values \u0026lt; np.percentile(r_tilde, 5, axis=0)) / len(spx_wk_returns) 0.03986981285598047 So 1.6% of the time the data values are above the 95% bounds, and 3.8% of the time the data values are below the 5% bounds. If anything then, our distribution may be too broad. However, in this case that could be considered a good thing because I\u0026rsquo;d rather predict a broader distribution of returns than a too restrictive one. It\u0026rsquo;s best to be over-prepared for extreme outcomes than under-prepared. The next plot is the 95% and 5% bounds plotted against the return data. You can see the points where the returns exceed those bounds.\nplt.plot(np.percentile(r_tilde, 95, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(np.percentile(r_tilde, 5, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(spx_wk_returns.values, color=\u0026#34;red\u0026#34;, alpha=0.5)  The next test is doing a probability integral transform. When you put a value through a CDF it gets transformed onto the range 0 to 1. Ideally, if I put the data through the CDF implied by the model, those output values should be uniformly distributed. This implies that the predicted distribution accurately predicts the probabilities of events. Unlike the exceedances test, which only looks at the tails, this test looks at the entire distribution.\nvalues = [] for t in range(len(spx_wk_returns)): ecdf = sm.distributions.empirical_distribution.ECDF(r_tilde[:, t]) values.append(ecdf(spx_wk_returns.iloc[t])) fig = sm.graphics.qqplot(np.array(values), dist=stats.uniform, line=\u0026#34;45\u0026#34;)  A QQ plot displays the transformed data against a reference distribution. If the samples match a uniform distribution, they should all fall perfectly on the 45 degree line in the figure. It\u0026rsquo;s clear there is some odd behavior at the right tail and in the center. It seems like our distributional estimate doesn\u0026rsquo;t match the data too well.\nModel 2 Okay, so issues are that the mean and skew seem off, and the distribution estimate doesn\u0026rsquo;t match too well with the data. What should I try? Well, we expect negative skew, because large negative returns happen rarely. So instead of assuming a normal error for returns, let\u0026rsquo;s try a skew normal! So everything is the same but the sampling statement for the returns looks like this now:\n$$ r_{t} \\sim Skew Normal(\\mu_{r}, \\exp(\\frac{h_{t}}{2}), \\alpha) $$\nWhere \\(\\alpha\\) is a new parameter that dictates the level of skew. In Stan, that model looks like this.\nmodel_spec = \u0026#34;\u0026#34;\u0026#34; data { int N; // Length of data vector[N] r; // SPX returns } parameters { real mu_h; // Volatility mean term real mu_r; // Returns mean term real\u0026lt;lower=-1, upper=1\u0026gt; phi; // Persistence of volatility real\u0026lt;lower=0\u0026gt; sigma; // Volatility noise vector[N] h_std; // Log volatility real alpha; // Skew Normal shape parameter } transformed parameters { vector[N] h = h_std * sigma; // h ~ normal(0, sigma); h[1] /= sqrt(1 - square(phi)); // h[1] ~ normal(0, sigma / sqrt(1 - square(phi))) h += mu_h; // h ~ normal(mu_h, sigma) for (t in 2:N) { h[t] += phi * (h[t-1] - mu_h); // h[2:N] ~ normal(mu_h + phi * (h[t-1] - mu_h), sigma) } } model { phi ~ uniform(-1, 1); sigma ~ normal(0, 10); mu_h ~ normal(0, 10); mu_r ~ normal(0, 10); alpha ~ normal(0, 10); h_std ~ std_normal(); r ~ skew_normal(mu_r, exp(h / 2), alpha); } generated quantities { real r_tilde[N]; real log_prob[N]; r_tilde = skew_normal_rng(mu_r, exp(h / 2), alpha); for (t in 1:N) { log_prob[t] = skew_normal_lpdf(r[t] | mu_r, exp(h[t] / 2), alpha); // Need log probabilities later on } } \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;./stan_model/model.stan\u0026#34;, \u0026#34;w\u0026#34;) as file: file.write(model_spec) model = CmdStanModel(stan_file=\u0026#34;./stan_model/model.stan\u0026#34;) data = {\u0026#34;N\u0026#34;: len(spx_wk_returns), \u0026#34;r\u0026#34;: spx_wk_returns.values} sample = model.sample(data=data, chains=4, parallel_chains=4, output_dir=\u0026#34;./stan_model\u0026#34;, iter_warmup=1000, iter_sampling=2500) model2_data = az.from_cmdstanpy(posterior=sample, posterior_predictive=\u0026#34;r_tilde\u0026#34;, observed_data={\u0026#34;r\u0026#34;: spx_wk_returns.values}, log_likelihood=\u0026#34;log_prob\u0026#34;) az.plot_trace(model2_data, compact=True, var_names=[\u0026#34;mu_h\u0026#34;, \u0026#34;mu_r\u0026#34;, \u0026#34;phi\u0026#34;, \u0026#34;sigma\u0026#34;, \u0026#34;alpha\u0026#34;])  Again, everything looks good here. Alpha centers around a negative value, which is a good sign, because negative skew was expected.\nr_tilde = model2_data.posterior_predictive.r_tilde.values.reshape(10000, -1) vol = model2_data.posterior.h.values.reshape(10000, -1) mean = np.mean(r_tilde, axis=1) std = np.std(r_tilde, axis=1) skew = stats.skew(r_tilde, axis=1) kurt = stats.kurtosis(r_tilde, axis=1) fig, axs = plt.subplots(2, 2) axs[0, 0].hist(mean, bins=50) axs[0, 0].axvline(np.mean(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 0].set_title(\u0026#34;Mean\u0026#34;) axs[0, 1].hist(std, bins=50) axs[0, 1].axvline(np.std(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[0, 1].set_title(\u0026#34;Standard Deviation\u0026#34;) axs[1, 0].hist(skew, bins=50) axs[1, 0].axvline(stats.skew(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 0].set_title(\u0026#34;Skew\u0026#34;) axs[1, 1].hist(kurt, bins=50) axs[1, 1].axvline(stats.kurtosis(spx_wk_returns), color=\u0026#39;black\u0026#39;) axs[1, 1].set_title(\u0026#34;Kurtosis\u0026#34;)  Now the mean value lies right in the center of the distribution and the skew value is closer to the middle then it was before. That looks like progress!\naz.plot_ppc(model2_data, data_pairs={\u0026#34;r\u0026#34;: \u0026#34;r_tilde\u0026#34;})  # 95% bounds exceedances np.sum(spx_wk_returns.values \u0026gt; np.percentile(r_tilde, 95, axis=0)) / len(spx_wk_returns) 0.025223759153783564 # 5% bounds exceedances np.sum(spx_wk_returns.values \u0026lt; np.percentile(r_tilde, 5, axis=0)) / len(spx_wk_returns) 0.025223759153783564 Our exceedances are again a bit too broad but they are more even than the first model.\nplt.plot(np.percentile(r_tilde, 95, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(np.percentile(r_tilde, 5, axis=0), color=\u0026#34;black\u0026#34;) plt.plot(spx_wk_returns.values, color=\u0026#34;red\u0026#34;, alpha=0.5)  values = [] for t in range(len(spx_wk_returns)): ecdf = sm.distributions.empirical_distribution.ECDF(r_tilde[:, t]) values.append(ecdf(spx_wk_returns.iloc[t])) fig = sm.graphics.qqplot(np.array(values), dist=stats.uniform, line=\u0026#34;45\u0026#34;)  The QQ plot here looks a little bit more funky than the one in model 1, which is concerning.\nModel Comparison So there are two models. There must be a better way to find out which is better than looking at visualizations. Turns out there is a really cool method for this called Pareto Smoothed Importance Sampling. This paper covers it very well. It sounds more complicated than it is. It allows the use of posterior samples and log probabilities to estimate the out-of-sample error of the model. It seeks to approximate the error estimated by leave-one-out (LOO) cross validation without running the model repeatedly. In this case, the model doesn\u0026rsquo;t take long to fit, and I could run fewer samples, but it would still have to be run nearly 1200 times to do true LOO cross validation\nI\u0026rsquo;ll take a brief aside to discuss leave-one-out methods on time series. This paper does some analysis on K-Fold cross validation on time series considering stationarity. They find little difference in error estimation using walk-forward versus K-Fold cross validation on stationary time series. This makes intuitive sense in that stationary time series display no time dependence, so the order in which you use the data shouldn\u0026rsquo;t matter. It\u0026rsquo;s obvious that stock market returns are not stationary. However, for the sake of this analysis, I\u0026rsquo;m going to assume that they are conditionally stationary given the volatility process. This gives some legitimacy to what I\u0026rsquo;m about to do. I will warn, however, that it\u0026rsquo;s not a perfect method and it\u0026rsquo;s applicability here can be called into question. With all of that said, let\u0026rsquo;s continue.\nThe author of the first paper linked very nicely has coded this process in python already, available here.\nmodel1_probs = model1_data.log_likelihood.log_prob.values.reshape(10000, -1) model2_probs = model2_data.log_likelihood.log_prob.values.reshape(10000, -1) loo1, loos1, ks1 = psisloo(model1_probs) loo2, loos2, ks2 = psisloo(model2_probs) diff = round(loo2 - loo1, 2) diff_se = round(np.sqrt(len(loos1) * np.var(loos2 - loos1)), 2) diff_interval = [round(diff - 2.6 * diff_se, 2), round(diff + 2.6 * diff_se, 2)] print(f\u0026#34;Model 1 ELPD: {round(loo1, 2)}\\nModel 2 ELPD: {round(loo2, 2)}\u0026#34;) Model 1 ELPD: 3042.64 Model 2 ELPD: 3050.58 print(f\u0026#34;Model 2 - Model 1: {diff}\\nStandard Error: {diff_se}\\nDifference 99% Interval: {diff_interval[0]}| {diff_interval[1]}\u0026#34;) Model 2 - Model 1: 7.94 Standard Error: 3.96 Difference 99% Interval: -2.36 | 18.24 ELPD stands for expected log predictive density. This is what we expect the out-of-sample log probability to be for the model, so we want it to be higher. Higher values imply that the probability of seeing the data given the model is higher, which means the model more closely matches the nature of the data. So, it looks like model 2 is better. Although, given the standard error of the estimate, there is some region of the sampling distribution where model 2 is similar or worse but not by very much. This method also returns a value for the shape parameter fitted to the Pareto distribution. Ideally we want this parameter to be less than 0.5 for every point, but 0.5 to 1 is okay. At these higher levels the variance of the estimator is higher and makes it less reliable. Parameter values greater than 1 are highly undesirable. At these levels, the variance of the estimator is infinite and totally unreliable.\nks1_max = round(np.max(ks1), 2) ks2_max = round(np.max(ks2), 2) ks1_gt = round(sum(ks1 \u0026gt; 0.5) / len(ks1) * 100, 2) ks2_gt = round(sum(ks2 \u0026gt; 0.5) / len(ks2) * 100, 2) print(f\u0026#34;Max k for Model 1: {ks1_max}\\nMax k for Model 2: {ks2_max}\u0026#34;) print(f\u0026#34;Percentage of values greater than 0.5 for Model 1: {ks1_gt}%\\nPercentage of values greater than 0.5 for Model 2: {ks2_gt}%\u0026#34;) Max k for Model 1: 0.87 Max k for Model 2: 0.95 Percentage of values greater than 0.5 for Model 1: 10.9% Percentage of values greater than 0.5 for Model 2: 11.64% It looks like the estimates for model 2 are slightly less reliable than model 1, which is worth considering because of how close the difference above was. The max value of k is also quite high, indicating that there are som significant data points making estimation more difficult. All-in-all I would consider model 2 to be better, but the differences are slight.\nModel Volatility versus Realized Volatility The model basically finds the value of volatility that fits the return data we give it. It\u0026rsquo;s a type of hierarchical model where volatility is a latent quantity. We cannot directly observe the volatility of a return series in the real world, we can only imply it. In the literature, there is a great deal about how to estimate that latent volatility. I\u0026rsquo;ve covered a few of those methods in a previous post. Let\u0026rsquo;s compare what our model thinks volatility is to a realized volatility estimator. I\u0026rsquo;m going to be taking the volatility from the second model.\nNote I\u0026rsquo;m taking the proper transformations to ensure both series are in standard deviation form. My volatility data is shorter than my weekly returns data, so I have to truncate some of it.\nreal_vol = np.sqrt(spx.vol.resample(\u0026#34;W-FRI\u0026#34;).sum()) model_vol = pd.Series(np.mean(arviz_data.posterior.h.values.reshape(10000, -1), axis=0), index=spx_wk_returns.index) model_vol = np.sqrt(np.exp(model_vol)) common_index = real_vol.index.intersection(model_vol.index) real_vol = real_vol.loc[common_index] model_vol = model_vol.loc[common_index] real_vol.plot() model_vol.plot(color=\u0026#34;r\u0026#34;)  With the model volatility in red and the realized measure in blue. The model pretty well captures the realized volatility! It\u0026rsquo;s a smoother estimate, which makes sense considering the linear model for it we are using. Cool!\nConclusion The model isn\u0026rsquo;t perfect, but then again no model is! The first one fails to capture the negative skew, and while the second one does better, the QQ plot looks less pleasing. This may mean that in doing better capturing skew, it fails to as effectively capture the middle of the distribution.\nThere are a lot of interesting extensions you could make to this model. The mean process of volatility could include exogenous regressors like VIX levels, or it could include past values of the returns themselves! Next the volatility of volatility, \\(\\sigma\\) in the model, could be made to have a stochastic or deterministic process of its own! Essentially, it could be made to vary with time, just like volatility of the returns.\nI\u0026rsquo;m becoming very interested in Bayesian methods for time series analysis, and there seems to be a lot less literature about that than non-time series models. I think the process for writing, fitting, and interpreting Bayesian models is much more straightforward and clear than frequentist methods. Credible intervals, the fact that parameter uncertainty is automatically accounted for in the posterior predictive distribution, and the methods for estimating out-of-sample error make life much easier.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/stoch_vol/","summary":"Fitting 2 different Stochastic Volatility Models to S\u0026amp;P 500 returns and finding out which is better","title":"Stochastic Volatility Models in Stan"},{"content":"I\u0026rsquo;ve been using a SQLite database to store my financial data locally for a while. This meant I had to have my personal computer running to do updates, I didn\u0026rsquo;t have a consistent way to access data, and ran the risk of losing my data. I decided it would be best to use Amazon Web Services (AWS) to handle data storage and updating from here on. I learned a lot along the way!\nThings that didn\u0026rsquo;t work Amazon Aurora Serverless I went in excited about Amazon Aurora Serverless. It seemed perfect for my needs. I don\u0026rsquo;t need to make database calls very often, so it would automatically shut off and cost nothing after a period of no usage. Great! However, I eventually learned that you cannot connect to an Aurora Serverless instance like you would connect to a normal SQL database to make calls. You have to use the Amazon Data API through the AWS command line interface or through their various SDKs. I frankly didn\u0026rsquo;t feel like putting the effort in to transition all of my current SQL calls and data handling to this, so I abandoned it.\nI ended up using a normal Relational Database Service instance. I\u0026rsquo;m using the smallest instance, which only costs about 15 dollars a month and is suitable for my needs.\nAWS Lambda Again, Lambda seemed perfect for my needs. It would run my python code to update my database on a schedule, charge me for that usage, then shut off and cost nothing the rest of the time. However, I found the documentation to be difficult at best and debugging to be challenging.\nYou have to upload your all of your package dependencies with your code in a zip file that ends up being 100\u0026rsquo;s of megabytes. The size means that you can\u0026rsquo;t use Amazon\u0026rsquo;s cloud IDE to update your code, so you have to re-upload it every time you need to change it, and it\u0026rsquo;s not fast. This made fixing bugs very tedious. You also have to ensure your zip file has certain read/write privileges before you upload it, something that I only found out via Stack Overflow after having errors.\nNext up was scheduling. I need this to run nightly. You have to go through Amazon CloudWatch to do this. Ugh.\nI ended up using an EC2 instance here. The smallest one is part of the free tier, so I don\u0026rsquo;t even have to pay for it!\nSetup I have found the AWS documentation to be generally hard to get through. Much of it seems to assume that you have background in AWS already, so it\u0026rsquo;s confusing to start from scratch. I want to document what I did to get this all working.\nVPC AWS creates a virtual network for you to connect all of your instances together. It\u0026rsquo;s also key to setup properly so you can access your instances from a computer connected via the internet. The easiest way to start is by using the automatic wizard from the dashboard page on the VPC console:\n Then you can select the option for a single public subnet:\n In the next screen everything can remain the default, and a name for the VPC can be entered. This process automatically creates a VPC, an internet gateway, and a subnet with that gateway attached. We do want to create another subnet under our VPC that is in a different availability zone. This is relevant for the database setup because Amazon puts the backup in a different zone than the database itself. Going to the subnets tab there should be a single subnet under the VPC you created, make note of its availability zone ID. Then you can create a new subnet under that same VPC. The IP block will need to be different. For instance the default subnet will be something like 10.0.0.0/24 so this new subnet will need to be 10.0.1.0/24. Then select an availability zone that is different than the default subnet.\nNext up is the security group that defines what connections will be accepted and from where. Create a new security group under the security heading and make the inbound rules look like this:\n The two top rules are so other instances in your subnets can connect to your database. The third can be set to accept connections from your personal computers IP by selecting \u0026ldquo;My IP\u0026rdquo; in the source box. The fourth has a type of SSH, again from your own IP, this allows you to connect to your EC2 instance via SSH to configure it. For outbound rules you can set destination to 0.0.0.0/0 and everything else to All so everything going out will be allowed.\nNow the networking and security is configured!\nRDS Subnet Group Next we have to make a subnet group for the database to use. In the RDS console, there is a subnet groups link. Create a new one, select the VPC configured earlier, and then select the two subnets. That\u0026rsquo;s it!\nRDS Instance Now moving to the database instance. Important settings to note:\n The free tier instance classes are under \u0026ldquo;Burstable classes\u0026rdquo; Make sure to deselect Multi-AZ deployment, this costs extra Select the VPC configured earlier under Connectivity, select the subnet group configured earlier, then choose the security group also configured earlier Make sure that public access is set to \u0026ldquo;yes\u0026rdquo;  Once the instance starts, on its summary page, make note of the endpoint URL and the port. This is the IP and port you\u0026rsquo;ll use when connecting to the database.\nEC2 Instance You can select a variety of machine images when creating these, I use the Ubuntu Server option. Then you can select the instance type that dictates how many resources the instance has access to. I use the free tier eligible t2.micro. On the configuration page, you can select the VPC, subnet, and other options. When you launch it, you\u0026rsquo;ll be directed to download a private key file. This is very important to keep. This file allows you to connect to your instance via SSH.\nOnce launched, on the instance summary page, there is the \u0026ldquo;Public IPv4 DNS.\u0026rdquo; This is the IP you\u0026rsquo;ll use to connect to your instance. The SSH command to connect looks like this:\nssh -i [path to .pem file] [Instance IP address] Once in, you can do whatever to get your code where it needs to be to run.\nFor scheduling, I use a cron job to run every night at midnight. Use crontab -e and put a line looking something like this:\n0 0 * * * source ~/RDSDatabase/update.sh Where update.sh is whatever you need to run. Mine looks like this:\n#!/bin/bash cd ~/RDSDatabase source venv/bin/activate python data_update.py Conclusion After all the fuss of figuring this out, it has been very well worth it. My data is there and up-to-date whenever I need it. I\u0026rsquo;ve created some data classes to fetch and hold the data the way I need it, so I have a consistent way to access it. It all just works. Most importantly, it\u0026rsquo;s not costing me that much money!\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/aws_database/","summary":"Lessons learned migrating my data to AWS","title":"Moving my Data to Amazon Web Services"},{"content":"I\u0026rsquo;ve moved my blog to Hugo for one main reason: I want to start posting work about Julia here, but Pluto.jl only exports in HTML, and Hugo supports that kind of content. In the process of moving, however, I\u0026rsquo;ve noticed that Hugo is much nicer to work with than Pelican. Hugo is written in Go, which I have no experience with, but it was very easy to install and use. Pelican, on the other hand, is written in python so I had to create a virtual environment and go through the normal annoyances with dependencies.\nHugo also includes a built in local server so you can work on your site and know what you\u0026rsquo;re getting. It works very smoothly. Writing posts and managing content is also far easier and more organized in Hugo. I can group my posts and their content together all in one folder so everything is nice and neat.\nIt\u0026rsquo;s much easier also to push my content directly to github pages. I was using a seperate plugin for Pelican, but I\u0026rsquo;ve created this shell script now that automatically builds my site, pushes the source to the master branch, and pushes only the static site content to the gh-pages branch.\n#!/usr/bin/bash hugo git add * read -p \u0026#34;Commit Message: \u0026#34; m git commit -m \u0026#34;$m\u0026#34; git push origin master git subtree push --prefix public origin gh-pages Themes are very well supported also, and there are many of them to choose from. Using the PaperMod theme, it was easy to get LaTeX support also, which is important to me.\nIt\u0026rsquo;s very usable, well documented, and I would recommend it to anyone looking for a static site generator to use for their own personal blog.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/hugo_move/","summary":"My comments on how Hugo compares to Pelican for static site generation","title":"A Note on Hugo"},{"content":"Alright, in this post I\u0026rsquo;m going to run through how to price options using Monte Carlo methods and also compute the associated greeks using automatic differentiation in PyTorch.\nBlack-Scholes  First, let\u0026rsquo;s look at implementing the Black-Scholes model in PyTorch.\nThe input variables are as follows:\n\\(K\\) : Strike price of the option\n\\(S(t)\\) : Price of the underlying asset at time \\(t\\)\n\\(t\\) : Current time in years.\n\\(T\\) : Time of option expiration\n\\(\\sigma\\) : Standard deviation of the underlying returns\n\\(r\\) : Annualized risk-free rate\n\\(N(x)\\) : Standard Normal cumulative distribution function\nThe price of a call option is given by:\n$$C(S_t, t) = N(d_1) S_t - N(d_2) K e^{-r(T-t)}$$\n$$d_1 = \\frac{1}{\\sigma\\sqrt{T-t}}[\\ln(\\frac{S_t}{K}) + (r + \\frac{\\sigma^2}{2})(T-t)]$$\n$$d_2 = d_1 - \\sigma\\sqrt{T-t}$$\nAnd by parity the price of a put option is given by:\n$$P(S_t, t) = N(-d_2) K e^{-r(T-t)} - N(-d_1) S_t$$\n Now, let\u0026rsquo;s implement that using PyTorch functions. For simplicity I replace \\(T\\) and \\(t\\) and their difference by a single term \\(T\\) specifying the total time left to expiry in years.\nimport torch from torch.distributions import Normal std_norm_cdf = Normal(0, 1).cdf std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x)) def bs_price(right, K, S, T, sigma, r): d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T) d_2 = d_1 - sigma * torch.sqrt(T) if right == \u0026#34;C\u0026#34;: C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T) return C elif right == \u0026#34;P\u0026#34;: P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S return P With this function I can calculate the price of a call option with the underyling at 100, strike price at 100, 1 year to expiration, 5% annual volatility, and a risk-free rate of 1% annually.\nright = \u0026#34;C\u0026#34; K = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) price = bs_price(right, K, S, T, sigma, r) print(price) tensor(2.5216, grad_fn=\u0026lt;SubBackward0\u0026gt;) Now, the magic of PyTorch is that it tracks all of those computations in a graph and can use its automatic differentiation feature to give us all the greeks. That\u0026rsquo;s why I told it that I needed a gradient on all of the input variables.\n# Tell PyTorch to compute gradients price.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.5890103578567505 Vega: 38.89707946777344 Theta: 1.536220908164978 Rho: 56.379390716552734 How do these compare to the greeks computed directly by differentiating the Black-Scholes formula?\nd_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T) d_2 = d_1 - sigma * torch.sqrt(T) delta = std_norm_cdf(d_1) vega = S * std_norm_pdf(d_1) * torch.sqrt(T) theta = ((S * std_norm_pdf(d_1) * sigma) / (2 * torch.sqrt(T))) + r * K * torch.exp(-r * T) * std_norm_cdf(d_2) rho = K * T * torch.exp(-r * T) * std_norm_cdf(d_2) print(f\u0026#34;Delta: {delta}\\nVega: {vega}\\nTheta: {theta}\\nRho: {rho}\u0026#34;) Delta: 0.5890103578567505 Vega: 38.89707946777344 Theta: 1.5362210273742676 Rho: 56.379390716552734 Exactly the same to a high level of precision! Amazing. It\u0026rsquo;s easy to see how much simpler the PyTorch autograd approach is. Note that it is possible to calculate second-order derivatives like Gamma, it just requires remaking the computation graph. If anyone knows of a workaround to this let me know.\nS = torch.tensor(100.0, requires_grad=True) price = bs_price(right, K, S, T, sigma, r) delta = torch.autograd.grad(price, S, create_graph=True)[0] delta.backward() print(f\u0026#34;Autograd Gamma: {S.grad}\u0026#34;) # And the direct Black-Scholes calculation gamma = std_norm_pdf(d_1) / (S * sigma * torch.sqrt(T)) print(f\u0026#34;BS Gamma: {gamma}\u0026#34;) Autograd Gamma: 0.07779412716627121 BS Gamma: 0.0777941569685936 Monte Carlo Pricing  Now that\u0026rsquo;s all fine, but nothing new except some computation tricks. Black-Scholes makes assumptions that can often violate what is observed in the real world. The problem is creating closed form pricing models under other market dynamics is usually impossible. That\u0026rsquo;s where Monte Carlo sampling comes in. It\u0026rsquo;s a trivial task to create future market paths given a model for its dynamics. You can calculate option payoffs from those paths and get a price. But how can you calculate greeks from Monte Carlo samples? Again, PyTorch and autograd can help.\nI\u0026rsquo;ll use all of the same parameters as in the example above. Let\u0026rsquo;s simulate the result of a Geometric Brownian Motion process after one year, just like Black-Scholes does.\nK = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) Z = torch.randn([1000000]) # Brownian Motion W_T = torch.sqrt(T) * Z # GBM prices = S * torch.exp((r - 0.5 * torch.square(sigma)) * T + sigma * W_T) import matplotlib.pyplot as plt plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) plt.hist(prices.detach().numpy(), bins=25) plt.xlabel(\u0026#34;Prices\u0026#34;) plt.ylabel(\u0026#34;Occurences\u0026#34;) plt.title(\u0026#34;Distribution of Underlying Price after 1 Year\u0026#34;)  Now, let\u0026rsquo;s calculate the option payoffs under each of those future prices, discount them using the risk-free rate, and then take the mean to get the option price. The price calculated with this method is close to the price calculated using Black-Scholes.\npayoffs = torch.max(prices - K, torch.zeros(1000000)) value = torch.mean(payoffs) * torch.exp(-r * T) print(value) tensor(2.5215, grad_fn=\u0026lt;MulBackward0\u0026gt;) Now, the magic comes in. The only random sampling I used above was a parameter-less standard normal. This fact allows PyTorch to keep track of gradients throughout all of the calculations above. This is called a Pathwise Derivative. This means we can use autograd just like above to get greeks.\nvalue.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.5890941023826599 Vega: 38.89133834838867 Theta: 1.536162257194519 Rho: 56.38788604736328 All the same! This means that we can simulate any Monte Carlo process we want, as long as its random component can be reparameterized, and get prices and greeks. Obviously this is a trivial example, but let\u0026rsquo;s look at a more complicated path-dependent option contract like an Asian Option. This type of option has a payoff based on the average price of the underlying over it\u0026rsquo;s duration, rather than only the price at expiration like a Vanilla Option. This means we must simulate the price movement each day instead of just at the end.\n# All the same parameters for the price process K = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) dt = torch.tensor(1 / 252) Z = torch.randn([1000000, int(T * 252)]) # Brownian Motion W_t = torch.cumsum(torch.sqrt(dt) * Z, 1) # GBM prices = S * torch.exp((r - 0.5 * torch.square(sigma)) * T + sigma * W_t) plt.plot(prices[0, :].detach().numpy()) plt.xlabel(\u0026#34;Number of Days in Future\u0026#34;) plt.ylabel(\u0026#34;Underlying Price\u0026#34;) plt.title(\u0026#34;One Possible Price path\u0026#34;) plt.axhline(y=torch.mean(prices[0, :]).detach().numpy(), color=\u0026#34;r\u0026#34;, linestyle=\u0026#34;--\u0026#34;) plt.axhline(y=100, color=\u0026#39;g\u0026#39;, linestyle=\u0026#34;--\u0026#34;)  The payoff of an Asian Option given this price path is the difference between the strike price, the green dashed line, and the daily average price over the year, shown by the dashed red line. In this case, the payoff would be zero because the average daily price is below the strike.\n# Payoff is now based on mean of underlying price, not terminal value payoffs = torch.max(torch.mean(prices, axis=1) - K, torch.zeros(1000000)) #payoffs = torch.max(prices[:, -1] - K, torch.zeros(100000)) value = torch.mean(payoffs) * torch.exp(-r * T) print(value) tensor(1.6765, grad_fn=\u0026lt;MulBackward0\u0026gt;) value.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.6314291954040527 Vega: 20.25724220275879 Theta: 0.5357358455657959 Rho: 61.46644973754883 PyTorch Autograd once again gives us greeks even though we are now pricing a totally different contract. Awesome!\nConclusion  Monte Carlo methods provide a way to price options under a much broader range of market process models. However, computing greeks can be challenging, either having to use finite difference methods or calculating pathwise derivatives symbolically. Using PyTorch can mitigate those issues and use automatic differentiation to provide greeks straight out of the box with no real overhead.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/option_pricing/","summary":"Using PyTorch to easily compute Option Greeks first using Black-Scholes and then Monte Carlo methods.","title":"Monte Carlo Methods for Option Pricing and Greeks"},{"content":"Okay, today we are moving up in the world and I\u0026rsquo;m going to use the magic of neural networks to forecast volatility.\nThe Data import pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) pd.plotting.register_matplotlib_converters() # Here\u0026#34;s my minute data for the S\u0026amp;P 500 spx_minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#34;datetime\u0026#34;, \u0026#34;open\u0026#34;, \u0026#34;high\u0026#34;, \u0026#34;low\u0026#34;, \u0026#34;close\u0026#34;], index_col=\u0026#34;datetime\u0026#34;, parse_dates=True) # Here\u0026#34;s the function for calculating the 1-min RV, as discussed in my last post def rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#34;close\u0026#34;]) - np.log(data[\u0026#34;close\u0026#34;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) spx_variance = rv_calc(spx_minute) conn = sqlite3.Connection(\u0026#34;data.db\u0026#34;) spx_data = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^GSPC\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=\u0026#34;date\u0026#34;) spx_returns = np.log(spx_data[\u0026#34;close\u0026#34;]) - np.log(spx_data[\u0026#34;close\u0026#34;].shift(1)) spx_returns = spx_returns.dropna() vix_data = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^VIX\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=\u0026#34;date\u0026#34;) # This puts it into units of daily standard deviation vix = vix_data[\u0026#34;close\u0026#34;] / np.sqrt(252) / 100 def create_lags(series, lags, name=\u0026#34;x\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Creates a dataframe with lagged values of the given series. Generates columns named x_t-n which means the value of each row is the value of the original series lagged n times \u0026#34;\u0026#34;\u0026#34; result = pd.DataFrame(index=series.index) result[f\u0026#34;{name}_t\u0026#34;] = series for n in range(lags): result[f\u0026#34;{name}_t-{n+1}\u0026#34;] = series.shift((n+1)) return result The predictive variables are the VIX, returns of the index, and our calculated realized variance. I include the 21 past values of these variables.\nvix_lags = create_lags(np.log(vix), 21, name=\u0026#34;vix\u0026#34;) return_lags = create_lags(spx_returns, 21, name=\u0026#34;returns\u0026#34;) rv_lags = create_lags(np.log(spx_variance), 21, name=\u0026#34;rv\u0026#34;) x = pd.concat([vix_lags, return_lags, rv_lags], axis=1).dropna() # We want to predict log of variance y = np.log(spx_variance.rolling(5).sum().shift(-5)).dropna() common_index = x.index.intersection(y.index) x = x.loc[common_index] y = y.loc[common_index] The Model I\u0026rsquo;m using a mixture density network to model future volatility. This is because I want an estimate of the future distribution of volatility, not just a point estimate. A mixture density network outputs the parameters for making a mixture of normal distributions. This is useful because you can approximate any arbitrary distribution with a large enough mixture of only normal distributions.\nimport torch import torch.nn as nn from torch.distributions import Categorical, Normal, Independent, MixtureSameFamily from torch.optim.swa_utils import AveragedModel, SWALR torch.set_default_dtype(torch.float64) class MDN(nn.Module): def __init__(self, in_dim, out_dim, hidden_dim, n_components): super().__init__() self.n_components = n_components # Last layer output dimension rationale: # Need two parameters for each distributionm thus 2 * n_components. # Need each of those for each output dimension, thus that multiplication self.norm_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, 2 * n_components * out_dim) ) self.cat_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, n_components * out_dim) ) def forward(self, x): norm_params = self.norm_network(x) # Split so we get parameters for mean and standard deviation mean, std = torch.split(norm_params, norm_params.shape[1] // 2, dim=1) # We need rightmost dimension to be n_components for mixture mean = mean.view(mean.shape[0], -1, self.n_components) std = std.view(std.shape[0], -1, self.n_components) normal = Normal(mean, torch.exp(std)) cat_params = self.cat_network(x) # Again, rightmost dimension must be n_components cat = Categorical(logits=cat_params.view(cat_params.shape[0], -1, self.n_components)) return MixtureSameFamily(cat, normal) test_index = int(len(x) * .75) train_x = torch.Tensor(x.iloc[:test_index].values) train_y = torch.Tensor(y.iloc[:test_index].values) test_x = torch.Tensor(x.iloc[test_index:].values) test_y = torch.Tensor(y.iloc[test_index:].values) in_dim = len(x.columns) out_dim = 1 n_components = 5 hidden_dim = 250 Below here is the training loop. I\u0026rsquo;m using a cosine annealing learning rate schedule to better explore the parameter space, as well as using model averaging over the last 500 iterations so the model generalizes better.\nmodel = MDN(in_dim, out_dim, hidden_dim, n_components) optimizer = torch.optim.AdamW(model.parameters(), lr=.001) scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 100, 2) swa_model = AveragedModel(model) swa_start = 400 swa_scheduler = SWALR(optimizer, swa_lr=0.001, anneal_epochs=10, anneal_strategy=\u0026#34;cos\u0026#34;) train_losses = [] validation_losses = [] model.train() swa_model.train() for epoch in range(500): optimizer.zero_grad() output = model(train_x) train_loss = -output.log_prob(train_y.view(-1, 1)).sum() train_losses.append(train_loss.detach()) test_loss = -model(test_x).log_prob(test_y.view(-1, 1)).sum() validation_losses.append(test_loss.detach()) train_loss.backward() optimizer.step() if epoch \u0026gt; swa_start: swa_model.update_parameters(model) swa_scheduler.step() else: scheduler.step() plt.plot(train_losses) plt.plot(validation_losses) plt.xlabel(\u0026#34;Training Epochs\u0026#34;) plt.ylabel(\u0026#34;Model Loss\u0026#34;) plt.title(\u0026#34;Training \u0026amp; Validation Losses\u0026#34;) plt.legend([\u0026#34;Training\u0026#34;, \u0026#34;Validation\u0026#34;])  swa_model.eval() output_mean = np.sqrt(np.exp(swa_model(test_x).mean.detach().numpy().squeeze())) y_trans = np.sqrt(np.exp(test_y.numpy().squeeze())) output_sample = np.sqrt(np.exp(swa_model(test_x).sample([5000]).numpy().squeeze())) Our out-of-sample R-squared is excellent, much higher than my previous simple linear model.\nregress = stats.linregress(output_mean, y_trans) print(f\u0026#34;R-squared: {regress.rvalue**2}\u0026#34;) R-squared: 0.7128714654332561 plt.plot(output_mean) plt.plot(y_trans) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Volatility\u0026#34;) plt.title(\u0026#34;Predicted and Actual Volatility\u0026#34;) plt.legend([\u0026#34;Model\u0026#34;, \u0026#34;Actual\u0026#34;])  Our distributional assumption also does well. We expect 5% of cases to be outside what the model distribution forecasts, and we find that to be the case.\npercent = np.percentile(output_sample, 95, axis=0) print(f\u0026#34;Number of exceedences: {(y_trans \u0026gt; percent).sum() / len(y_trans)}\u0026#34;) Number of exceedences: 0.04477611940298507 Further testing the distribution accuracy, let\u0026rsquo;s see if doing a probability integral transform yields a uniform.\nclass ECDF: def __init__(self, data): self.sorted = data self.sorted.sort() self.y = np.arange(1, len(self.sorted) + 1) / len(self.sorted) def __call__(self, x): ind = np.searchsorted(self.sorted, x) - 1 return self.y[ind] values = [] for x in range(len(y_trans)): ecdf = ECDF(output_sample[x]) values.append(ecdf(y_trans[x])) plt.hist(values, bins=10)  stats.kstest(values, \u0026#34;uniform\u0026#34;) KstestResult(statistic=0.028702640642939155, pvalue=0.46125545362008036) We can\u0026rsquo;t reject the null hypothesis that the transformed values come from a uniform distribution! That means our distributions accurately models the data\u0026rsquo;s real distribution.\nConclusion This model seems quite excellent. I\u0026rsquo;m going to use this model for my future posts about how to make an effective trading strategy. Next time I\u0026rsquo;m going to discuss Kelly Bet Sizing and its application to continuous distributions.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/vol_mdn/","summary":"Using a mixture density neural network implemented in PyTorch to forecast the distribution of future realized volatility.","title":"Mixture Density Network for Forecasting Realized Volatility"},{"content":"So now that I\u0026rsquo;ve decided that I\u0026rsquo;m going to use 1-min RV as my volatility proxy, I can move on to the juicy part: forecasting.\nThe Data  import pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) pd.plotting.register_matplotlib_converters() # Here\u0026#39;s my minute data for the S\u0026amp;P 500 spx_minute = minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#39;datetime\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;], index_col=\u0026#39;datetime\u0026#39;, parse_dates=True) # Here\u0026#39;s the function for calculating the 1-min RV, as discussed in my last post def rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) spx_rv = rv_calc(spx_minute) The Model My goal is to predict the volatility over the next week, or 5 trading days, with the past 5 days of daily volatility. This means my independent variables will be the last 5 days of volatility, and my dependent variable is the realized volatility over the next 5 days. For the sake of increased samples, I\u0026rsquo;m going to create a rolling 5-day window of volatility and shift it 5 periods backwards and use that as the dependent variable. This means I can create a 5-day volatility forecast for each day, rather than each week.\ndef create_lags(series, lags): \u0026#34;\u0026#34;\u0026#34; Creates a dataframe with lagged values of the given series. Generates columns named x_{n}which means the value of each row is the value of the original series lagged n times \u0026#34;\u0026#34;\u0026#34; result = pd.DataFrame(index=series.index) result[\u0026#34;x\u0026#34;] = series \u0026#34;\u0026#34; for n in range(lags): result[f\u0026#34;x_{n+1}\u0026#34;] = series.shift((n+1)) return result dep_var = spx_rv.rolling(5).sum().shift(-5).dropna() indep_var = create_lags(spx_rv, 5).dropna() # This ensures that we only keep rows that occur in each set. This means their length is the same and # rows match up properly common_index = dep_var.index.intersection(indep_var.index) dep_var = dep_var.loc[common_index] indep_var = indep_var.loc[common_index] # I\u0026#39;m going to take the log of the variance because it has better distributional qualities dep_var = np.log(dep_var) indep_var = np.log(indep_var) I\u0026rsquo;m going to use a very simple Bayesian linear regression for this model. It assumes the data is distributed according to\n$$y \\sim normal(\\mu + X\\beta, \\sigma)$$\nimport pystan as stan import arviz model_spec = \u0026#39;\u0026#39;\u0026#39; data { int len; int vars; vector[len] dep_var; matrix[len, vars] indep_var; } parameters { real mu; vector[vars] beta; real\u0026lt;lower=0\u0026gt; sigma; } model { mu ~ cauchy(0, 10); beta ~ cauchy(0, 10); sigma ~ cauchy(0, 5); dep_var ~ normal(mu + (indep_var * beta), sigma); } \u0026#39;\u0026#39;\u0026#39; model = stan.StanModel(model_code=model_spec) Model Testing and Verification Okay, let\u0026rsquo;s do some out of sample testing to see how our model does! Below, I\u0026rsquo;m defining the training and testing sets. I\u0026rsquo;m going to use 75% of the data for in-sample fitting and the remaining 25% for out-of-sample testing.\ntest_index = int(len(indep_var) * .75) train_x = indep_var.iloc[:test_index] train_y = dep_var[:test_index] test_x = indep_var.iloc[test_index:] test_y = dep_var[test_index:] Now, I fit the model to the data.\nparams = {\u0026#39;len\u0026#39;: len(train_x), \u0026#39;vars\u0026#39;: len(train_x.columns), \u0026#39;dep_var\u0026#39;: train_y, \u0026#39;indep_var\u0026#39;: train_x} sample = model.sampling(data=params, chains=4, warmup=250, iter=1500) Let\u0026rsquo;s check our sampling statistics to ensure the sampler converged. R-hats all look very good and our effective samples also look good.\nprint(sample.stansummary(pars=[\u0026#39;mu\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;])) Inference for Stan model: anon_model_842ef31b1beae12ccaeb1a8773757520. 4 chains, each with iter=1500; warmup=250; thin=1; post-warmup draws per chain=1250, total post-warmup draws=5000. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat mu 0.59 1.2e-3 0.09 0.42 0.53 0.59 0.65 0.77 5682 1.0 beta[1] 0.46 3.4e-4 0.02 0.42 0.45 0.46 0.47 0.5 3219 1.0 beta[2] 0.14 4.5e-4 0.02 0.1 0.13 0.14 0.16 0.18 2408 1.0 beta[3] 0.09 3.9e-4 0.02 0.04 0.07 0.09 0.1 0.13 3317 1.0 beta[4] 0.08 3.7e-4 0.02 0.03 0.06 0.08 0.09 0.12 3753 1.0 beta[5] 0.06 4.1e-4 0.02 0.01 0.04 0.06 0.07 0.1 2966 1.0 beta[6] 0.07 3.0e-4 0.02 0.03 0.06 0.07 0.08 0.11 4026 1.0 sigma 0.49 9.5e-5 6.9e-3 0.48 0.49 0.49 0.5 0.51 5295 1.0 Samples were drawn using NUTS at Wed Mar 17 19:28:01 2021. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). arviz_data = arviz.from_pystan( posterior=sample ) We can look at trace plots for our samples. Good samples should look like fuzzy caterpillars, which is what we see here. The distributions also match across sampling chains. The variables also match our intuition: $\\mu$ and $\\beta$ are positive, and the regression coefficients are all positive.\narviz.plot_trace(arviz_data, var_names=[\u0026#39;mu\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;])  The code below creates the posterior predictive distribution for the in-sample and out-of-sample data. These represent what the model predicts the distribution of the data is. The job now is to compare this predicted distribution to the reality.\nmu = sample[\u0026#39;mu\u0026#39;] beta = sample[\u0026#39;beta\u0026#39;] sigma = sample[\u0026#39;sigma\u0026#39;] # This is some tensordot sorcery that works, but that I don\u0026#39;t frankly understand. It takes the matrix product of train_x # and beta over each row of beta. Essentially a higher-dimensional version of what the model does. train_post = np.random.normal(mu + (np.tensordot(train_x, beta, axes=(1,1))), sigma) test_post = np.random.normal(mu + (np.tensordot(test_x, beta, axes=(1,1))), sigma) train_post_mean = np.mean(train_post, axis=1) test_post_mean = np.mean(test_post, axis=1) Let\u0026rsquo;s take a look at the in-sample and out-of-sample residuals. In this case, I\u0026rsquo;m making a point estimate by taking the mean of the posterior predictive distribution. It\u0026rsquo;s obvious that the model has problems predicting volatility jumps, signified by unexpected jumps in the residuals.\nplt.plot(np.exp(train_y) - np.exp(train_post_mean)) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Residual\u0026#39;) plt.title(\u0026#39;In-Sample Residuals\u0026#39;)  plt.plot(np.exp(test_y) - np.exp(test_post_mean)) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Residual\u0026#39;) plt.title(\u0026#39;Out-of-Sample Residuals\u0026#39;)  Now, let\u0026rsquo;s look at the root mean square error of our model. Looks like our out-of-sample RMSE, using exponentiated values, is around 7% higher, not bad!\ntrain_rmse = np.sqrt(np.mean((np.exp(train_y) - np.exp(train_post_mean))**2)) test_rmse = np.sqrt(np.mean((np.exp(test_y) - np.exp(test_post_mean))**2)) print(f\u0026#39;In-Sample RMSE: {train_rmse}\\nOut-of-Sample RMSE: {test_rmse}\u0026#39;) print(f\u0026#39;Percent Increase: {(test_rmse / train_rmse) - 1}\u0026#39;) In-Sample RMSE: 0.0006314456099670146 Out-of-Sample RMSE: 0.0006745751839390536 Percent Increase: 0.06830291206600037 I like to do a Mincer-Zarnowitz regression to analyze out-of-sample forests. In this case, the out-of-sample predictions are treated as the independent variable and the true values are the dependent variable. The R-Squared for out model is about 64%, which means our out-of-sample predictions explain 64% of the variance of the true values. Not bad! The intercept is also very close to zero, which means our prediction isn\u0026rsquo;t biased.\nregress = stats.linregress(np.exp(test_post_mean), np.exp(test_y)) print(f\u0026#39;Intercept: {regress.intercept}\\nSlope: {regress.slope}\\nR-Squared: {regress.rvalue**2}\u0026#39;) Intercept: 1.7250208362578126e-05 Slope: 1.183989352654772 R-Squared: 0.6438180914963003 Next, I want to check the distributional assumptions. Specifically, I want to know how many times real volatility exceeds what our distribution predicts. To do this, I\u0026rsquo;m going to look at the posterior predictive distribution, which should, if our model is correct, accurately predict the distribution of the real data. I\u0026rsquo;ll figure out the 95th percentile of the posterior predictive, and see how many times real volatility exceeded that. We should expect exceedances to happen about 5% of the time.\nupper_bound_train = np.percentile(np.exp(train_post), 95, axis=1) num_exceeds_train = (np.exp(train_y) \u0026gt; upper_bound_train).sum() upper_bound_test = np.percentile(np.exp(test_post), 95, axis=1) num_exceeds_test = (np.exp(test_y) \u0026gt; upper_bound_test).sum() print(f\u0026#39;In-Sample Exceedances: {num_exceeds_train / len(upper_bound_train)}\u0026#39;) print(f\u0026#39;Out-of-Sample Exceedances: {num_exceeds_test / len(upper_bound_test)}\u0026#39;) In-Sample Exceedances: 0.0481139337952271 Out-of-Sample Exceedances: 0.09815242494226328 In-sample we are within 5%, and out-of-sample we are above 5% by about double, which isn\u0026rsquo;t a good sign. Next up is testing the empirical distribution of the data. If our posterior predictive distribution is a good representation of the underlying distribution, doing a probability integral transform should transform the data into a uniform distribution.\nclass ECDF: def __init__(self, data): self.sorted = data self.sorted.sort() self.y = np.arange(1, len(self.sorted) + 1) / len(self.sorted) def __call__(self, x): ind = np.searchsorted(self.sorted, x) - 1 return self.y[ind] values = [] for x in range(len(test_post)): ecdf = ECDF(np.exp(test_post[x])) values.append(ecdf(np.exp(test_y[x]))) plt.hist(values) plt.title(\u0026#39;Transformed Data\u0026#39;)  We can see an obvious deviation from the expected uniform distribution here. It looks like our distribution most significantly under-predicts large volatiltiy values. This makes sense when looking back to the residual graph, large jumps aren\u0026rsquo;t handled well.\nstats.kstest(values, \u0026#39;uniform\u0026#39;) KstestResult(statistic=0.0760443418013857, pvalue=8.408548699568476e-05) This Kolmogorov-Smirnov test takes the null hypothesis that the data matches the specified distribution, in this case a uniform. It looks like we can handedly reject that hypothesis. This means that the posterior predictive is not fully capable of representing the real distribution.\nConclusion and Extensions It seems like this very simple model does pretty well providing a point-forecast of future volatility, however it fails at accurately describing the distribution of future volatility. This could be fixed in several ways. First is assuming a different distributional form in the model, such as something with fatter tails like a Student\u0026rsquo;s T. Another possibility is allowing the standard deviation of the normal to vary with time. That is more in line with models like traditional stochastic volatility.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/vol_linear_model/","summary":"Using a simple bayesian autoregressive model to forecast future volatility","title":"Bayesian Autoregressive Volatility Forecasting"},{"content":"Today, I\u0026rsquo;m going to be discussing the difference between two volatility estimators.\nThe Data  I\u0026rsquo;m going to be using daily-resolution SPX data from Sharadar as well as minute-resolution SPX data from First Rate Data.\nimport pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) conn = sqlite3.Connection(\u0026#34;data.db\u0026#34;) spx_daily = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^GSPC\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=[\u0026#34;date\u0026#34;]) spx_minute = minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#39;datetime\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;], index_col=\u0026#39;datetime\u0026#39;, parse_dates=True) # A quick look at the data spx_daily.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ticker open high low close volume dividends closeunadj lastupdated   date              1997-12-31 ^GSPC 970.84 975.02 967.41 970.43 467280000 0 970.43 2019-02-03   1998-01-02 ^GSPC 970.43 975.04 965.73 975.04 366730000 0 975.04 2019-02-03   1998-01-05 ^GSPC 975.04 982.63 969.00 977.07 628070000 0 977.07 2019-02-03   1998-01-06 ^GSPC 977.07 977.07 962.68 966.58 618360000 0 966.58 2019-02-03   1998-01-07 ^GSPC 966.58 966.58 952.67 964.00 667390000 0 964.00 2019-02-03     spx_minute.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  open high low close   datetime         2007-04-27 12:25:00 1492.39 1492.54 1492.39 1492.54   2007-04-27 12:26:00 1492.57 1492.57 1492.52 1492.56   2007-04-27 12:27:00 1492.58 1492.64 1492.58 1492.63   2007-04-27 12:28:00 1492.63 1492.73 1492.63 1492.73   2007-04-27 12:29:00 1492.91 1492.91 1492.87 1492.87     The Estimators  Now, what I want to do is compare volatility estimates from these two data sets. I would prefer to use the daily data if possible, because in my case it\u0026rsquo;s easier to get and updates more frequently.\nGarman-Klass Estimator This estimator has been around for a while and is deemed to be far more effcient than a traditional close-to-close volatility estimator (Garman and Klass, 1980).\nFrom equation 20 in the paper, a jump adjusted volatility estimator:\n$$f = 0.73$$ percentage of the day trading is closed based on NYSE hours of 9:30 to 4\n$$a = 0.12$$ as they suggest in the paper\n$$\\sigma^2_{unadj} = 0.511(u - d)^2 - 0.019(c(u+d) - 2ud) - 0.383c^2$$\n$$\\sigma^2_{adj} = 0.12\\frac{(O_{1} - C_{0})^2}{0.73} + 0.12\\frac{\\sigma^2_{unadj}}{0.27}$$\nWhere\n$$u = H_{1} - O_{1}$$ the normalized high\n$$d = L_{1} - O_{1}$$ the normalized low\n$$c = C_{1} - O_{1}$$ the normalized close and subscripts indicating time. They also indicate in the paper that these equations expect the log of the price series.\ndef gk_vol_calc(data): u = np.log(data[\u0026#39;high\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) d = np.log(data[\u0026#39;low\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) c = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) vol_unadj = 0.511 * (u - d)**2 - 0.019 * (c * (u + d) - 2 * u * d) - 0.283 * c**2 jumps = np.log(data[\u0026#39;open\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) vol_adj = 0.12 * (jumps**2 / 0.73) + 0.12 * (vol_unadj / 0.27) return vol_adj # Let\u0026#39;s take a look gk_vol = np.sqrt(gk_vol_calc(spx_daily)) gk_vol.plot()  As an aside, opening jumps have become more common and larger in recent years, maybe something to investigate. This is as a percentage, so it\u0026rsquo;s not a simple case of the index values becoming larger.\n(spx_daily[\u0026#39;open\u0026#39;] / spx_daily[\u0026#39;close\u0026#39;].shift(1) - 1).plot()  Realized Volatility Estimator This estimator is very simply and has become more prominent in the literature in the last few years because of increasing availability of higher-frequency data. Based on (Liu, Patton, and Sheppard, 2012), it\u0026rsquo;s hard to beat a 5-minute RV. Here, I\u0026rsquo;m going to use a 1-minute estimator, which is also shown to be effective.\n$$RV_{t} = \\sum_{k=1}^n r_{t,k}^2$$ where the t index is each day, and the k index represents each intraday return\nFor daily volatility, it\u0026rsquo;s simply a sum of squared returns from within that day. So in this case we calculate returns for each 1 minute period, square them, and they sum them for each day.\ndef rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) # Let\u0026#39;s take a look at this one rv = np.sqrt(rv_calc(spx_minute)) rv.plot()  Comparisons  # Because the minute data has a shorter history, let\u0026#39;s match them up gk_vol = gk_vol.reindex(rv.index) rv.plot() gk_vol.plot()  Here\u0026rsquo;s a plot of our two different volatility estimators with RV in blue and Garman-Klass in orange. The RV estimator is far less noisy, looking at each of their graphs above. The Garman-Klass estimator also seems to persistently return a lower result than RV. This is backed up by looking at a graph of their difference.\n(gk_vol - rv).plot()  Netx, let\u0026rsquo;s analyze how they do at normalizing the returns of the S\u0026amp;P 500. According to (Molnr, 2015) normalizing a number of equity returns by their Garman-Klass estimated volatility does indeed make their distributions normal. Let\u0026rsquo;s see if we can replicate that result with either of our esimates on the S\u0026amp;P 500.\n# Daily close-to-close returns of the S\u0026amp;P 500 spx_returns = np.log(spx_daily[\u0026#39;close\u0026#39;]) - np.log(spx_daily[\u0026#39;close\u0026#39;].shift(1)) spx_returns = spx_returns.reindex(rv.index) # Normalizing by our estimated volatilties gk_vol_norm = (spx_returns / gk_vol).dropna() rv_norm = (spx_returns / rv).dropna() # Here are the unadjusted returns _, _, _ = plt.hist(spx_returns, bins=50)  # Here\u0026#39;s normalized by the Garman-Klass Estimator _, _, _ = plt.hist(gk_vol_norm, bins=50)  # And this is by the RV estimator _, _, _ = plt.hist(rv_norm, bins=50)  At first glance, the RV adjusted returns seem most like normal to me, let\u0026rsquo;s run some tests. These Scipy tests set the null hypothesis that the data comes from a corresponding normal distribution. So if the p-value is small we can reject that hypothesis and conclude the distribution is non-normal.\nprint(stats.skewtest(gk_vol_norm)) print(stats.skewtest(rv_norm)) Garman-Klass Skew: SkewtestResult(statistic=-0.3767923327324783, pvalue=0.7063279391177064) RV-5min Skew: SkewtestResult(statistic=5.251294175425576, pvalue=1.5103423951480544e-07) print(stats.kurtosistest(gk_vol_norm)) print(stats.kurtosistest(rv_norm)) KurtosistestResult(statistic=-13.088609427904334, pvalue=3.825472809774632e-39) KurtosistestResult(statistic=0.315320709120601, pvalue=0.7525181628202805) Looks like the Garman-Klass-normalized returns have normal skew, but non-normal kurtosis. The RV-normalized returns have non-normal skew but normal kurtosis! There\u0026rsquo;s no winning here! Both are non-normal in different ways. Either normalization does do better than the unadjusted returns though.\nprint(stats.skewtest(spx_returns.dropna())) print(stats.kurtosistest(spx_returns.dropna())) SkewtestResult(statistic=-12.386230904806132, pvalue=3.1028724633560147e-35) KurtosistestResult(statistic=26.470418979318143, pvalue=2.124045513612033e-154) Conclusion  While from a statistical point of view, neither option seems particularly favorable, my personal choice is going to be the RV estimator. I think the literature is clear on its efficacy and its less noisy and conceptually easier. It\u0026rsquo;s been said that when there are a bunch of competing theories, none of them are very good. So I\u0026rsquo;ll pick the simplest option and go with RV.\nReferences    Garman, M., \u0026amp; Klass, M. (1980). On the Estimation of Security Price Volatilities from Historical Data. The Journal of Business, 53(1), 67-78. Retrieved February 14, 2021, from http://www.jstor.org/stable/2352358\n  Liu, L., Patton, A., \u0026amp; Sheppard, K. (2012). Does Anything Beat 5-Minute RV? A Comparison of Realized Measures Across Multiple Asset Classes. SSRN. http://dx.doi.org/10.2139/ssrn.2214997\n  Molnr, P. (2015). Properties of Range-Based Volatility Estimators. SSRN. Retrieved from https://ssrn.com/abstract=2691435\n  ","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/vol_estimators/","summary":"Comparing Garman-Klass estimator to 5-minute Realized Volatility estimator.","title":"Comparison of Volatility Estimators"},{"content":"Alright, with this post I\u0026rsquo;m going to start a series on portfolio optimization techniques! This is one of my favorite topics in finance. This post is going to construct a portfolio based on the diversification ratio, which is outlined in the papers linked below. The basic idea is to maximize the Diversification ratio, which is defined as the weighted average volatilities of assets in the portfolio divided by the total portfolio volatility. This makes intuitive sense, by increasing diversification we lower portfolio volatility compared to the average volatility of the assets that make it up.\nChoueifaty, Y., \u0026amp; Coignard, Y. (2008). Toward Maximum Diversification. The Journal of Portfolio Management, 40-51. doi:https://doi.org/10.3905/JPM.2008.35.1.40\nChoueifaty, Y., Reynier, J., \u0026amp; Froidure, T. (2013). Properties of the Most Diversified Portfolio. Journal of Investment Strategies, 49-70. doi:http://doi.org/10.2139/ssrn.1895459\nThe assets  I\u0026rsquo;m going to mainly focus on Vanguard ETFs as they have the lowest fees. For anything they don\u0026rsquo;t offer, I\u0026rsquo;m using iShares. I\u0026rsquo;m also limiting myself to funds with inception dates \u0026gt;10 years ago for stability.\nHere\u0026rsquo;s the list:    Symbol Description     VGSH Short-term Treasury   VGIT Mid-term Treasury   VGLT Long-term Treasury   TIP TIPS Treasury Bonds   VMBS Agency MBS   SUB Municipal Bonds   VCSH Short-term Investment Grade Corporate Bonds   VCIT Mid-term Investment Grade Corporate Bonds   VCLT Long-term Investment Grade Corporate Bonds   HYG High-yield Corporate Bonds   EMB Emerging Markets Bonds   IGOV International Treasuries   VV Large Cap US Stocks   VO Mid-Cap US Stocks   VB Small-Cap US Stocks   VWO Emerging Markets Stocks   VEA Non-US Developed Markets Stocks   IYR US Real Estate   IFGL Non-US Real Estate    Data  All of this is the code to fetch historical data from QuantConnect and calculate returns.\nimport numpy as np import pandas as pd symbols = [\u0026#39;VGSH\u0026#39;, \u0026#39;VGIT\u0026#39;, \u0026#39;VGLT\u0026#39;, \u0026#39;TIP\u0026#39;, \u0026#39;VMBS\u0026#39;, \u0026#39;SUB\u0026#39;, \u0026#39;VCSH\u0026#39;, \u0026#39;VCIT\u0026#39;, \u0026#39;VCLT\u0026#39;, \u0026#39;HYG\u0026#39;, \u0026#39;EMB\u0026#39;, \u0026#39;IGOV\u0026#39;, \u0026#39;VV\u0026#39;, \u0026#39;VO\u0026#39;, \u0026#39;VB\u0026#39;, \u0026#39;VWO\u0026#39;, \u0026#39;VEA\u0026#39;, \u0026#39;IYR\u0026#39;, \u0026#39;IFGL\u0026#39;] qb = QuantBook() symbols_data = {symbol: qb.AddEquity(symbol) for symbol in symbols} from datetime import datetime # This is QuantConnect API code to get price history history = qb.History(qb.Securities.Keys, datetime(2009, 1, 1), datetime(2020, 12, 31), Resolution.Daily) history = history[\u0026#39;close\u0026#39;].unstack(level=0).dropna() I\u0026rsquo;m using arithmetic returns here so I can easily weight the returns across assets when computing portfolio returns.\nreturns = (history / history.shift(1)) - 1 returns = returns.dropna() # Let\u0026#39;s define some helper functions to get cumulative return series and the total return def get_cum_returns(returns): return (returns + 1).cumprod() - 1 def get_total_return(returns): return np.product(returns + 1) - 1 The Optimization  This function calculates the diversification ratio for a portfolio given asset weights and their covariance matrix. This is from equation (1) (Choueifaty \u0026amp; Coignard, 2008).\ndef diverse_ratio(weights, covariance): # Standard deviation vector stds = np.sqrt(np.diagonal(covariance)) # Asset-weighted standard deviation num = np.dot(weights, stds) # Portfolio standard deviation denom = np.sqrt(weights @ covariance @ weights) return num / denom Now, to confirm that scipy minimize works as we expect for this problem, I\u0026rsquo;m going to test a bunch of randomized starting weights to confirm that the final weights end up the same. I increase the level of precision using the \u0026lsquo;ftol\u0026rsquo; option because returns are fairly small decimal quantities and I want to ensure the optimization converges completely.\nfrom scipy.optimize import minimize cov = np.cov(returns.values.T) # Long-only constraint bounds = [(0, 1) for x in range(len(cov))] # Portfolio weights must sum to 1 constraints = ( {\u0026#39;type\u0026#39;: \u0026#39;eq\u0026#39;, \u0026#39;fun\u0026#39;: lambda x: np.sum(x) - 1} ) results = [] for x in range(100): # Set initial weights randomly initial = np.random.random(len(cov)) # Use negative of objective function to maximize result = minimize(lambda x, y: -1 * diverse_ratio(x, y), initial, method=\u0026#39;SLSQP\u0026#39;, args=(cov), bounds=bounds, constraints=constraints, options={\u0026#39;ftol\u0026#39;: 1e-10}) results.append(result.x) # Stack all optimized weight vectors, and round to 4 digits after the decimal results_array = np.round(np.stack(results), 4) # Let\u0026#39;s look at the standard deviation of the asset weights accross the different optimizations stds = np.std(results_array, axis=0) # Looks like they\u0026#39;re all zero or nearly zero! print(stds) [0.00000000e+00 3.12250226e-17 0.00000000e+00 1.24900090e-16 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.16333634e-17 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.08166817e-17 1.66533454e-16 1.38777878e-16 1.66533454e-16 0.00000000e+00 0.00000000e+00 1.24900090e-16 2.42861287e-17] Looks like the optimization converges to the same values every time regardless of starting point! This means it\u0026rsquo;s finding the true minimum (or maximum in this case). Let\u0026rsquo;s look the at the weights for each symbol.\n# I\u0026#39;ll just grab the last optimization result this way weights_series = pd.Series(data=np.round(result.x, 4), index=returns.columns) print(weights_series) EMB 0.0000 HYG 0.0215 IFGL 0.0000 IGOV 0.0586 IYR 0.0000 SUB 0.2759 TIP 0.0000 VB 0.0336 VCIT 0.0000 VCLT 0.0000 VCSH 0.0000 VEA 0.0265 VGIT 0.1150 VGLT 0.1868 VGSH 0.1825 VMBS 0.0000 VO 0.0000 VV 0.0817 VWO 0.0179 # Let\u0026#39;s drop everything with a zero weight final_weights = weights_series[weights_series \u0026gt; 0] # Sort by weight for viewing ease print(final_weights.sort_values(ascending=False)) SUB 0.2759 VGLT 0.1868 VGSH 0.1825 VGIT 0.1150 VV 0.0817 IGOV 0.0586 VB 0.0336 VEA 0.0265 HYG 0.0215 VWO 0.0179 # Confirm everything sums to 1. Looks good! final_weights.sum() 0.9999999999999999 Backtest Results  After doing some offscreen magic to implement this in QuantConnect, we get a dataframe tracking portfolio weights over each month. The algorithm starts at the beginning of 2011 and runs to the end of 2020. On the first trading day of each month it computes the portfolio using the code above using the past year of returns data for each ticker. Note that this is slightly different than the above that uses the entire return history in the optimization. Let\u0026rsquo;s take a closer look at a particular ETFs portfolio allocation over time. I\u0026rsquo;m going to use VGSH for this because it\u0026rsquo;s the least risky, most cash-like instrument under consideration.\nYou can see that the weight changes quite drastically over time, from near zero to nearly 90% in the later parts of 2020. This reflects the nature of how we are calculating asset variances and correlations using only the last 252 days of data. When volatilities or correlations change it causes changes in the allocation.\nweights_frame[\u0026#39;VGSH\u0026#39;].plot(figsize=(15,10))  In this case, it\u0026rsquo;s caused by a large change in correlation for certain assets in early 2020. Shown in the graph below is the correlation between VGSH and the other ETFs over time. Note the large downward jump on the right side. This shows the weakness of using a rolling data approach like in the backtest. You get big market jumps that dramatically shift your allocation and then when they eventually fall out the backward-looking window, you get big jumps again. I want to come back to this topic some time in the future.\nrolling_corr = returns.rolling(252).corr() rolling_corr[\u0026#39;VGSH\u0026#39;].unstack().plot(figsize=(15,10))  # let\u0026#39;s reindex the monthly weights frame to daily with forward fill to match returns arra weights_frame = weights_frame.reindex(returns.index, method=\u0026#39;ffill\u0026#39;) # Now we can calculate portfolio returns by weight the returns and summing port_returns = (weights_frame.values * returns).sum(axis=1, skipna=False).dropna() # We can also calculate cumulative returns this way because we\u0026#39;re working with logarithmic returns cum_port_returns = get_cum_returns(port_returns) Alright, plotted below are the cumulative returns for the strategy! Note this is without transaction costs factored in.\ncum_port_returns.plot(figsize=(15, 10))  Now let\u0026rsquo;s assemble some backtest statistics. We\u0026rsquo;re going to be using mlfinlab for this task.\nfrom mlfinlab import backtest_statistics total_return = get_total_return(port_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(port_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(cum_port_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 29.77% CAGR 2.94% Sharpe Ratio 1.15 Maximum Drawdown 7.07% MAR Ratio 0.42 Let\u0026rsquo;s compare that to just US large cap stocks over the same period.\nvv_returns = returns[\u0026#39;VV\u0026#39;][\u0026#39;2011\u0026#39;:] vv_cum_returns = get_cum_returns(vv_returns) total_return = get_total_return(vv_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(vv_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(vv_cum_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 267.74% CAGR 15.57% Sharpe Ratio 0.84 Maximum Drawdown 34.28% MAR Ratio 0.45 Looks like the maximum diversification portfolio achieves a higher sharpe ratio! Although it comes at the cost of signficantly lower total returns. More interesting is the MAR ratio, defined as the CAGR over the maximum drawdown. This is a useful ratio because it gauges how much extra return you are getting for taking on heavier drawdown risk. It looks like large cap US stocks win out on this metric.\nIt gives a different perspective than the Sharpe ratio. The Sharpe ratio uses only standard deviation as a metric for risk. This can be very unrealistic because radically different equity curves can actually have the same Sharpe ratio and total return. That can be interestingly illustrated by reordering returns.\n# Okay, let\u0026#39;s sort VV returns from least to greatest. Note that these are the same returns, just reordered. sorted_returns = pd.Series(sorted(vv_returns.values), index=vv_returns.index) cum_sorted_returns = get_cum_returns(sorted_returns) # Here you can see the cumulative return graphs. The sorted one looks very unusual, but in fact, the total return ends # up exactly the same! cum_sorted_returns.plot(figsize=(15, 10)) vv_cum_returns.plot()  total_return = get_total_return(sorted_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(sorted_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(cum_sorted_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 267.74% CAGR 15.57% Sharpe Ratio 0.84 Maximum Drawdown 99.97% MAR Ratio 0.16 As you can see the total return, CAGR, and Sharpe ratio are all the same as the original return series! But the maximum drawdown is significantly higher. Obviously this is a worst case scenario, but it shows how drawdowns can drastically affect your portfolio performance over time. Volatility by itself doesn\u0026rsquo;t reflect all kinds of risk because it ignores path dependency. This again is a topic worth covering in more detail at a later date.\nConclusions  Even considering the backtest and attributes of this simple strategy shows deep complexity. In future, I want to compare this optimization strategy to others like the traditional mean-variance approach, hierarchical risk parity, minimum variance, and others. Along with that is discussing extensions like using models to provide forecasts for asset volatility and correlation.\nSo with all those things to think about, see you next time!\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/mdp_etf_portfolio/","summary":"Constructing a portfolio from a selection of ETFs to maximize the diversification ratio","title":"A Most Diversified ETF Portfolio"}]
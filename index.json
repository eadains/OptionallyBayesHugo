[{"content":"I\u0026rsquo;ve move my blog to Hugo for one main reason: I want to start posting work about Julia here, and Pluto.jl only exports in HTML, and Hugo supports that kind of content. In the process of moving, however, I\u0026rsquo;ve noticed that Hugo is much nicer to work with than Pelican. Hugo is written in Go, which I have no experience with, but it was very easy to install and use. Pelican, on the other hand, is written in python so I had to create a virtual environment and go through the normal annoyances with dependencies.\nHugo also includes a built in local server so you can work on your site and know what you\u0026rsquo;re getting. It works very smoothly. Writing posts and managing content is also far easier and more organized in Hugo. I can group my posts and their content together all in one folder so everything is nice and neat.\nIt\u0026rsquo;s much easier also to push my content directly to github pages. I was using a seperate plugin for Pelican, but I\u0026rsquo;ve created this shell script now that automatically builds my site, pushes the source to the master branch, and pushes only the static site content to the gh-pages branch.\n#!/usr/bin/bash hugo git add * read -p \u0026#34;Commit Message: \u0026#34; m git commit -m \u0026#34;$m\u0026#34; git push origin master git subtree push --prefix public origin gh-pages Themes are very well supported also, and there are many of them to choose from. Using the PaperMod theme, it was easy to get LaTeX support also, which is important to me.\nIt\u0026rsquo;s very usable, well documented, and I would recommend it to anyone looking for a static site generator to use for their own personal blog.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/hugo_move/","summary":"My comments on how Hugo compares to Pelican for static site generation","title":"A Note on Hugo"},{"content":"Alright, in this post I\u0026rsquo;m going to run through how to price options using Monte Carlo methods and also compute the associated greeks using automatic differentiation in PyTorch.\nBlack-Scholes  First, let\u0026rsquo;s look at implementing the Black-Scholes model in PyTorch.\nThe input variables are as follows:\n\\(K\\) : Strike price of the option\n\\(S(t)\\) : Price of the underlying asset at time \\(t\\)\n\\(t\\) : Current time in years.\n\\(T\\) : Time of option expiration\n\\(\\sigma\\) : Standard deviation of the underlying returns\n\\(r\\) : Annualized risk-free rate\n\\(N(x)\\) : Standard Normal cumulative distribution function\nThe price of a call option is given by:\n$$C(S_t, t) = N(d_1) S_t - N(d_2) K e^{-r(T-t)}$$\n$$d_1 = \\frac{1}{\\sigma\\sqrt{T-t}}[\\ln(\\frac{S_t}{K}) + (r + \\frac{\\sigma^2}{2})(T-t)]$$\n$$d_2 = d_1 - \\sigma\\sqrt{T-t}$$\nAnd by parity the price of a put option is given by:\n$$P(S_t, t) = N(-d_2) K e^{-r(T-t)} - N(-d_1) S_t$$\n Now, let\u0026rsquo;s implement that using PyTorch functions. For simplicity I replace \\(T\\) and \\(t\\) and their difference by a single term \\(T\\) specifying the total time left to expiry in years.\nimport torch from torch.distributions import Normal std_norm_cdf = Normal(0, 1).cdf std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x)) def bs_price(right, K, S, T, sigma, r): d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T) d_2 = d_1 - sigma * torch.sqrt(T) if right == \u0026#34;C\u0026#34;: C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T) return C elif right == \u0026#34;P\u0026#34;: P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S return P With this function I can calculate the price of a call option with the underyling at 100, strike price at 100, 1 year to expiration, 5% annual volatility, and a risk-free rate of 1% annually.\nright = \u0026#34;C\u0026#34; K = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) price = bs_price(right, K, S, T, sigma, r) print(price) tensor(2.5216, grad_fn=\u0026lt;SubBackward0\u0026gt;) Now, the magic of PyTorch is that it tracks all of those computations in a graph and can use its automatic differentiation feature to give us all the greeks. That\u0026rsquo;s why I told it that I needed a gradient on all of the input variables.\n# Tell PyTorch to compute gradients price.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.5890103578567505 Vega: 38.89707946777344 Theta: 1.536220908164978 Rho: 56.379390716552734 How do these compare to the greeks computed directly by differentiating the Black-Scholes formula?\nd_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T) d_2 = d_1 - sigma * torch.sqrt(T) delta = std_norm_cdf(d_1) vega = S * std_norm_pdf(d_1) * torch.sqrt(T) theta = ((S * std_norm_pdf(d_1) * sigma) / (2 * torch.sqrt(T))) + r * K * torch.exp(-r * T) * std_norm_cdf(d_2) rho = K * T * torch.exp(-r * T) * std_norm_cdf(d_2) print(f\u0026#34;Delta: {delta}\\nVega: {vega}\\nTheta: {theta}\\nRho: {rho}\u0026#34;) Delta: 0.5890103578567505 Vega: 38.89707946777344 Theta: 1.5362210273742676 Rho: 56.379390716552734 Exactly the same to a high level of precision! Amazing. It\u0026rsquo;s easy to see how much simpler the PyTorch autograd approach is. Note that it is possible to calculate second-order derivatives like Gamma, it just requires remaking the computation graph. If anyone knows of a workaround to this let me know.\nS = torch.tensor(100.0, requires_grad=True) price = bs_price(right, K, S, T, sigma, r) delta = torch.autograd.grad(price, S, create_graph=True)[0] delta.backward() print(f\u0026#34;Autograd Gamma: {S.grad}\u0026#34;) # And the direct Black-Scholes calculation gamma = std_norm_pdf(d_1) / (S * sigma * torch.sqrt(T)) print(f\u0026#34;BS Gamma: {gamma}\u0026#34;) Autograd Gamma: 0.07779412716627121 BS Gamma: 0.0777941569685936 Monte Carlo Pricing  Now that\u0026rsquo;s all fine, but nothing new except some computation tricks. Black-Scholes makes assumptions that can often violate what is observed in the real world. The problem is creating closed form pricing models under other market dynamics is usually impossible. That\u0026rsquo;s where Monte Carlo sampling comes in. It\u0026rsquo;s a trivial task to create future market paths given a model for its dynamics. You can calculate option payoffs from those paths and get a price. But how can you calculate greeks from Monte Carlo samples? Again, PyTorch and autograd can help.\nI\u0026rsquo;ll use all of the same parameters as in the example above. Let\u0026rsquo;s simulate the result of a Geometric Brownian Motion process after one year, just like Black-Scholes does.\nK = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) Z = torch.randn([1000000]) # Brownian Motion W_T = torch.sqrt(T) * Z # GBM prices = S * torch.exp((r - 0.5 * torch.square(sigma)) * T + sigma * W_T) import matplotlib.pyplot as plt plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) plt.hist(prices.detach().numpy(), bins=25) plt.xlabel(\u0026#34;Prices\u0026#34;) plt.ylabel(\u0026#34;Occurences\u0026#34;) plt.title(\u0026#34;Distribution of Underlying Price after 1 Year\u0026#34;)   Now, let\u0026rsquo;s calculate the option payoffs under each of those future prices, discount them using the risk-free rate, and then take the mean to get the option price. The price calculated with this method is close to the price calculated using Black-Scholes.\npayoffs = torch.max(prices - K, torch.zeros(1000000)) value = torch.mean(payoffs) * torch.exp(-r * T) print(value) tensor(2.5215, grad_fn=\u0026lt;MulBackward0\u0026gt;) Now, the magic comes in. The only random sampling I used above was a parameter-less standard normal. This fact allows PyTorch to keep track of gradients throughout all of the calculations above. This is called a Pathwise Derivative. This means we can use autograd just like above to get greeks.\nvalue.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.5890941023826599 Vega: 38.89133834838867 Theta: 1.536162257194519 Rho: 56.38788604736328 All the same! This means that we can simulate any Monte Carlo process we want, as long as its random component can be reparameterized, and get prices and greeks. Obviously this is a trivial example, but let\u0026rsquo;s look at a more complicated path-dependent option contract like an Asian Option. This type of option has a payoff based on the average price of the underlying over it\u0026rsquo;s duration, rather than only the price at expiration like a Vanilla Option. This means we must simulate the price movement each day instead of just at the end.\n# All the same parameters for the price process K = torch.tensor(100.0, requires_grad=True) S = torch.tensor(100.0, requires_grad=True) T = torch.tensor(1.0, requires_grad=True) sigma = torch.tensor(0.05, requires_grad=True) r = torch.tensor(0.01, requires_grad=True) dt = torch.tensor(1 / 252) Z = torch.randn([1000000, int(T * 252)]) # Brownian Motion W_t = torch.cumsum(torch.sqrt(dt) * Z, 1) # GBM prices = S * torch.exp((r - 0.5 * torch.square(sigma)) * T + sigma * W_t) plt.plot(prices[0, :].detach().numpy()) plt.xlabel(\u0026#34;Number of Days in Future\u0026#34;) plt.ylabel(\u0026#34;Underlying Price\u0026#34;) plt.title(\u0026#34;One Possible Price path\u0026#34;) plt.axhline(y=torch.mean(prices[0, :]).detach().numpy(), color=\u0026#34;r\u0026#34;, linestyle=\u0026#34;--\u0026#34;) plt.axhline(y=100, color=\u0026#39;g\u0026#39;, linestyle=\u0026#34;--\u0026#34;)   The payoff of an Asian Option given this price path is the difference between the strike price, the green dashed line, and the daily average price over the year, shown by the dashed red line. In this case, the payoff would be zero because the average daily price is below the strike.\n# Payoff is now based on mean of underlying price, not terminal value payoffs = torch.max(torch.mean(prices, axis=1) - K, torch.zeros(1000000)) #payoffs = torch.max(prices[:, -1] - K, torch.zeros(100000)) value = torch.mean(payoffs) * torch.exp(-r * T) print(value) tensor(1.6765, grad_fn=\u0026lt;MulBackward0\u0026gt;) value.backward() print(f\u0026#34;Delta: {S.grad}\\nVega: {sigma.grad}\\nTheta: {T.grad}\\nRho: {r.grad}\u0026#34;) Delta: 0.6314291954040527 Vega: 20.25724220275879 Theta: 0.5357358455657959 Rho: 61.46644973754883 PyTorch Autograd once again gives us greeks even though we are now pricing a totally different contract. Awesome!\nConclusion  Monte Carlo methods provide a way to price options under a much broader range of market process models. However, computing greeks can be challenging, either having to use finite difference methods or calculating pathwise derivatives symbolically. Using PyTorch can mitigate those issues and use automatic differentiation to provide greeks straight out of the box with no real overhead.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/option_pricing/","summary":"Using PyTorch to easily compute Option Greeks first using Black-Scholes and then Monte Carlo methods.","title":"Monte Carlo Methods for Option Pricing and Greeks"},{"content":"Okay, today we are moving up in the world and I\u0026rsquo;m going to use the magic of neural networks to forecast volatility.\nThe Data import pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) pd.plotting.register_matplotlib_converters() # Here\u0026#34;s my minute data for the S\u0026amp;P 500 spx_minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#34;datetime\u0026#34;, \u0026#34;open\u0026#34;, \u0026#34;high\u0026#34;, \u0026#34;low\u0026#34;, \u0026#34;close\u0026#34;], index_col=\u0026#34;datetime\u0026#34;, parse_dates=True) # Here\u0026#34;s the function for calculating the 1-min RV, as discussed in my last post def rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#34;close\u0026#34;]) - np.log(data[\u0026#34;close\u0026#34;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) spx_variance = rv_calc(spx_minute) conn = sqlite3.Connection(\u0026#34;data.db\u0026#34;) spx_data = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^GSPC\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=\u0026#34;date\u0026#34;) spx_returns = np.log(spx_data[\u0026#34;close\u0026#34;]) - np.log(spx_data[\u0026#34;close\u0026#34;].shift(1)) spx_returns = spx_returns.dropna() vix_data = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^VIX\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=\u0026#34;date\u0026#34;) # This puts it into units of daily standard deviation vix = vix_data[\u0026#34;close\u0026#34;] / np.sqrt(252) / 100 def create_lags(series, lags, name=\u0026#34;x\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Creates a dataframe with lagged values of the given series. Generates columns named x_t-n which means the value of each row is the value of the original series lagged n times \u0026#34;\u0026#34;\u0026#34; result = pd.DataFrame(index=series.index) result[f\u0026#34;{name}_t\u0026#34;] = series for n in range(lags): result[f\u0026#34;{name}_t-{n+1}\u0026#34;] = series.shift((n+1)) return result The predictive variables are the VIX, returns of the index, and our calculated realized variance. I include the 21 past values of these variables.\nvix_lags = create_lags(np.log(vix), 21, name=\u0026#34;vix\u0026#34;) return_lags = create_lags(spx_returns, 21, name=\u0026#34;returns\u0026#34;) rv_lags = create_lags(np.log(spx_variance), 21, name=\u0026#34;rv\u0026#34;) x = pd.concat([vix_lags, return_lags, rv_lags], axis=1).dropna() # We want to predict log of variance y = np.log(spx_variance.rolling(5).sum().shift(-5)).dropna() common_index = x.index.intersection(y.index) x = x.loc[common_index] y = y.loc[common_index] The Model I\u0026rsquo;m using a mixture density network to model future volatility. This is because I want an estimate of the future distribution of volatility, not just a point estimate. A mixture density network outputs the parameters for making a mixture of normal distributions. This is useful because you can approximate any arbitrary distribution with a large enough mixture of only normal distributions.\nimport torch import torch.nn as nn from torch.distributions import Categorical, Normal, Independent, MixtureSameFamily from torch.optim.swa_utils import AveragedModel, SWALR torch.set_default_dtype(torch.float64) class MDN(nn.Module): def __init__(self, in_dim, out_dim, hidden_dim, n_components): super().__init__() self.n_components = n_components # Last layer output dimension rationale: # Need two parameters for each distributionm thus 2 * n_components. # Need each of those for each output dimension, thus that multiplication self.norm_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, 2 * n_components * out_dim) ) self.cat_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, n_components * out_dim) ) def forward(self, x): norm_params = self.norm_network(x) # Split so we get parameters for mean and standard deviation mean, std = torch.split(norm_params, norm_params.shape[1] // 2, dim=1) # We need rightmost dimension to be n_components for mixture mean = mean.view(mean.shape[0], -1, self.n_components) std = std.view(std.shape[0], -1, self.n_components) normal = Normal(mean, torch.exp(std)) cat_params = self.cat_network(x) # Again, rightmost dimension must be n_components cat = Categorical(logits=cat_params.view(cat_params.shape[0], -1, self.n_components)) return MixtureSameFamily(cat, normal) test_index = int(len(x) * .75) train_x = torch.Tensor(x.iloc[:test_index].values) train_y = torch.Tensor(y.iloc[:test_index].values) test_x = torch.Tensor(x.iloc[test_index:].values) test_y = torch.Tensor(y.iloc[test_index:].values) in_dim = len(x.columns) out_dim = 1 n_components = 5 hidden_dim = 250 Below here is the training loop. I\u0026rsquo;m using a cosine annealing learning rate schedule to better explore the parameter space, as well as using model averaging over the last 500 iterations so the model generalizes better.\nmodel = MDN(in_dim, out_dim, hidden_dim, n_components) optimizer = torch.optim.AdamW(model.parameters(), lr=.001) scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 100, 2) swa_model = AveragedModel(model) swa_start = 400 swa_scheduler = SWALR(optimizer, swa_lr=0.001, anneal_epochs=10, anneal_strategy=\u0026#34;cos\u0026#34;) train_losses = [] validation_losses = [] model.train() swa_model.train() for epoch in range(500): optimizer.zero_grad() output = model(train_x) train_loss = -output.log_prob(train_y.view(-1, 1)).sum() train_losses.append(train_loss.detach()) test_loss = -model(test_x).log_prob(test_y.view(-1, 1)).sum() validation_losses.append(test_loss.detach()) train_loss.backward() optimizer.step() if epoch \u0026gt; swa_start: swa_model.update_parameters(model) swa_scheduler.step() else: scheduler.step() plt.plot(train_losses) plt.plot(validation_losses) plt.xlabel(\u0026#34;Training Epochs\u0026#34;) plt.ylabel(\u0026#34;Model Loss\u0026#34;) plt.title(\u0026#34;Training \u0026amp; Validation Losses\u0026#34;) plt.legend([\u0026#34;Training\u0026#34;, \u0026#34;Validation\u0026#34;])   swa_model.eval() output_mean = np.sqrt(np.exp(swa_model(test_x).mean.detach().numpy().squeeze())) y_trans = np.sqrt(np.exp(test_y.numpy().squeeze())) output_sample = np.sqrt(np.exp(swa_model(test_x).sample([5000]).numpy().squeeze())) Our out-of-sample R-squared is excellent, much higher than my previous simple linear model.\nregress = stats.linregress(output_mean, y_trans) print(f\u0026#34;R-squared: {regress.rvalue**2}\u0026#34;) R-squared: 0.7128714654332561 plt.plot(output_mean) plt.plot(y_trans) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Volatility\u0026#34;) plt.title(\u0026#34;Predicted and Actual Volatility\u0026#34;) plt.legend([\u0026#34;Model\u0026#34;, \u0026#34;Actual\u0026#34;])   Our distributional assumption also does well. We expect 5% of cases to be outside what the model distribution forecasts, and we find that to be the case.\npercent = np.percentile(output_sample, 95, axis=0) print(f\u0026#34;Number of exceedences: {(y_trans \u0026gt; percent).sum() / len(y_trans)}\u0026#34;) Number of exceedences: 0.04477611940298507 Further testing the distribution accuracy, let\u0026rsquo;s see if doing a probability integral transform yields a uniform.\nclass ECDF: def __init__(self, data): self.sorted = data self.sorted.sort() self.y = np.arange(1, len(self.sorted) + 1) / len(self.sorted) def __call__(self, x): ind = np.searchsorted(self.sorted, x) - 1 return self.y[ind] values = [] for x in range(len(y_trans)): ecdf = ECDF(output_sample[x]) values.append(ecdf(y_trans[x])) plt.hist(values, bins=10)   stats.kstest(values, \u0026#34;uniform\u0026#34;) KstestResult(statistic=0.028702640642939155, pvalue=0.46125545362008036) We can\u0026rsquo;t reject the null hypothesis that the transformed values come from a uniform distribution! That means our distributions accurately models the data\u0026rsquo;s real distribution.\nConclusion This model seems quite excellent. I\u0026rsquo;m going to use this model for my future posts about how to make an effective trading strategy. Next time I\u0026rsquo;m going to discuss Kelly Bet Sizing and its application to continuous distributions.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/vol_mdn/","summary":"Using a mixture density neural network implemented in PyTorch to forecast the distribution of future realized volatility.","title":"Mixture Density Network for Forecasting Realized Volatility"},{"content":"So now that I\u0026rsquo;ve decided that I\u0026rsquo;m going to use 1-min RV as my volatility proxy, I can move on to the juicy part: forecasting.\nThe Data  import pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) pd.plotting.register_matplotlib_converters() # Here\u0026#39;s my minute data for the S\u0026amp;P 500 spx_minute = minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#39;datetime\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;], index_col=\u0026#39;datetime\u0026#39;, parse_dates=True) # Here\u0026#39;s the function for calculating the 1-min RV, as discussed in my last post def rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) spx_rv = rv_calc(spx_minute) The Model My goal is to predict the volatility over the next week, or 5 trading days, with the past 5 days of daily volatility. This means my independent variables will be the last 5 days of volatility, and my dependent variable is the realized volatility over the next 5 days. For the sake of increased samples, I\u0026rsquo;m going to create a rolling 5-day window of volatility and shift it 5 periods backwards and use that as the dependent variable. This means I can create a 5-day volatility forecast for each day, rather than each week.\ndef create_lags(series, lags): \u0026#34;\u0026#34;\u0026#34; Creates a dataframe with lagged values of the given series. Generates columns named x_{n}which means the value of each row is the value of the original series lagged n times \u0026#34;\u0026#34;\u0026#34; result = pd.DataFrame(index=series.index) result[\u0026#34;x\u0026#34;] = series \u0026#34;\u0026#34; for n in range(lags): result[f\u0026#34;x_{n+1}\u0026#34;] = series.shift((n+1)) return result dep_var = spx_rv.rolling(5).sum().shift(-5).dropna() indep_var = create_lags(spx_rv, 5).dropna() # This ensures that we only keep rows that occur in each set. This means their length is the same and # rows match up properly common_index = dep_var.index.intersection(indep_var.index) dep_var = dep_var.loc[common_index] indep_var = indep_var.loc[common_index] # I\u0026#39;m going to take the log of the variance because it has better distributional qualities dep_var = np.log(dep_var) indep_var = np.log(indep_var) I\u0026rsquo;m going to use a very simple Bayesian linear regression for this model. It assumes the data is distributed according to\n$$y \\sim normal(\\mu + X\\beta, \\sigma)$$\nimport pystan as stan import arviz model_spec = \u0026#39;\u0026#39;\u0026#39; data { int len; int vars; vector[len] dep_var; matrix[len, vars] indep_var; } parameters { real mu; vector[vars] beta; real\u0026lt;lower=0\u0026gt; sigma; } model { mu ~ cauchy(0, 10); beta ~ cauchy(0, 10); sigma ~ cauchy(0, 5); dep_var ~ normal(mu + (indep_var * beta), sigma); } \u0026#39;\u0026#39;\u0026#39; model = stan.StanModel(model_code=model_spec) Model Testing and Verification Okay, let\u0026rsquo;s do some out of sample testing to see how our model does! Below, I\u0026rsquo;m defining the training and testing sets. I\u0026rsquo;m going to use 75% of the data for in-sample fitting and the remaining 25% for out-of-sample testing.\ntest_index = int(len(indep_var) * .75) train_x = indep_var.iloc[:test_index] train_y = dep_var[:test_index] test_x = indep_var.iloc[test_index:] test_y = dep_var[test_index:] Now, I fit the model to the data.\nparams = {\u0026#39;len\u0026#39;: len(train_x), \u0026#39;vars\u0026#39;: len(train_x.columns), \u0026#39;dep_var\u0026#39;: train_y, \u0026#39;indep_var\u0026#39;: train_x} sample = model.sampling(data=params, chains=4, warmup=250, iter=1500) Let\u0026rsquo;s check our sampling statistics to ensure the sampler converged. R-hats all look very good and our effective samples also look good.\nprint(sample.stansummary(pars=[\u0026#39;mu\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;])) Inference for Stan model: anon_model_842ef31b1beae12ccaeb1a8773757520. 4 chains, each with iter=1500; warmup=250; thin=1; post-warmup draws per chain=1250, total post-warmup draws=5000. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat mu 0.59 1.2e-3 0.09 0.42 0.53 0.59 0.65 0.77 5682 1.0 beta[1] 0.46 3.4e-4 0.02 0.42 0.45 0.46 0.47 0.5 3219 1.0 beta[2] 0.14 4.5e-4 0.02 0.1 0.13 0.14 0.16 0.18 2408 1.0 beta[3] 0.09 3.9e-4 0.02 0.04 0.07 0.09 0.1 0.13 3317 1.0 beta[4] 0.08 3.7e-4 0.02 0.03 0.06 0.08 0.09 0.12 3753 1.0 beta[5] 0.06 4.1e-4 0.02 0.01 0.04 0.06 0.07 0.1 2966 1.0 beta[6] 0.07 3.0e-4 0.02 0.03 0.06 0.07 0.08 0.11 4026 1.0 sigma 0.49 9.5e-5 6.9e-3 0.48 0.49 0.49 0.5 0.51 5295 1.0 Samples were drawn using NUTS at Wed Mar 17 19:28:01 2021. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). arviz_data = arviz.from_pystan( posterior=sample ) We can look at trace plots for our samples. Good samples should look like fuzzy caterpillars, which is what we see here. The distributions also match across sampling chains. The variables also match our intuition: $\\mu$ and $\\beta$ are positive, and the regression coefficients are all positive.\narviz.plot_trace(arviz_data, var_names=[\u0026#39;mu\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;])   The code below creates the posterior predictive distribution for the in-sample and out-of-sample data. These represent what the model predicts the distribution of the data is. The job now is to compare this predicted distribution to the reality.\nmu = sample[\u0026#39;mu\u0026#39;] beta = sample[\u0026#39;beta\u0026#39;] sigma = sample[\u0026#39;sigma\u0026#39;] # This is some tensordot sorcery that works, but that I don\u0026#39;t frankly understand. It takes the matrix product of train_x # and beta over each row of beta. Essentially a higher-dimensional version of what the model does. train_post = np.random.normal(mu + (np.tensordot(train_x, beta, axes=(1,1))), sigma) test_post = np.random.normal(mu + (np.tensordot(test_x, beta, axes=(1,1))), sigma) train_post_mean = np.mean(train_post, axis=1) test_post_mean = np.mean(test_post, axis=1) Let\u0026rsquo;s take a look at the in-sample and out-of-sample residuals. In this case, I\u0026rsquo;m making a point estimate by taking the mean of the posterior predictive distribution. It\u0026rsquo;s obvious that the model has problems predicting volatility jumps, signified by unexpected jumps in the residuals.\nplt.plot(np.exp(train_y) - np.exp(train_post_mean)) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Residual\u0026#39;) plt.title(\u0026#39;In-Sample Residuals\u0026#39;)   plt.plot(np.exp(test_y) - np.exp(test_post_mean)) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Residual\u0026#39;) plt.title(\u0026#39;Out-of-Sample Residuals\u0026#39;)   Now, let\u0026rsquo;s look at the root mean square error of our model. Looks like our out-of-sample RMSE, using exponentiated values, is around 7% higher, not bad!\ntrain_rmse = np.sqrt(np.mean((np.exp(train_y) - np.exp(train_post_mean))**2)) test_rmse = np.sqrt(np.mean((np.exp(test_y) - np.exp(test_post_mean))**2)) print(f\u0026#39;In-Sample RMSE: {train_rmse}\\nOut-of-Sample RMSE: {test_rmse}\u0026#39;) print(f\u0026#39;Percent Increase: {(test_rmse / train_rmse) - 1}\u0026#39;) In-Sample RMSE: 0.0006314456099670146 Out-of-Sample RMSE: 0.0006745751839390536 Percent Increase: 0.06830291206600037 I like to do a Mincer-Zarnowitz regression to analyze out-of-sample forests. In this case, the out-of-sample predictions are treated as the independent variable and the true values are the dependent variable. The R-Squared for out model is about 64%, which means our out-of-sample predictions explain 64% of the variance of the true values. Not bad! The intercept is also very close to zero, which means our prediction isn\u0026rsquo;t biased.\nregress = stats.linregress(np.exp(test_post_mean), np.exp(test_y)) print(f\u0026#39;Intercept: {regress.intercept}\\nSlope: {regress.slope}\\nR-Squared: {regress.rvalue**2}\u0026#39;) Intercept: 1.7250208362578126e-05 Slope: 1.183989352654772 R-Squared: 0.6438180914963003 Next, I want to check the distributional assumptions. Specifically, I want to know how many times real volatility exceeds what our distribution predicts. To do this, I\u0026rsquo;m going to look at the posterior predictive distribution, which should, if our model is correct, accurately predict the distribution of the real data. I\u0026rsquo;ll figure out the 95th percentile of the posterior predictive, and see how many times real volatility exceeded that. We should expect exceedances to happen about 5% of the time.\nupper_bound_train = np.percentile(np.exp(train_post), 95, axis=1) num_exceeds_train = (np.exp(train_y) \u0026gt; upper_bound_train).sum() upper_bound_test = np.percentile(np.exp(test_post), 95, axis=1) num_exceeds_test = (np.exp(test_y) \u0026gt; upper_bound_test).sum() print(f\u0026#39;In-Sample Exceedances: {num_exceeds_train / len(upper_bound_train)}\u0026#39;) print(f\u0026#39;Out-of-Sample Exceedances: {num_exceeds_test / len(upper_bound_test)}\u0026#39;) In-Sample Exceedances: 0.0481139337952271 Out-of-Sample Exceedances: 0.09815242494226328 In-sample we are within 5%, and out-of-sample we are above 5% by about double, which isn\u0026rsquo;t a good sign. Next up is testing the empirical distribution of the data. If our posterior predictive distribution is a good representation of the underlying distribution, doing a probability integral transform should transform the data into a uniform distribution.\nclass ECDF: def __init__(self, data): self.sorted = data self.sorted.sort() self.y = np.arange(1, len(self.sorted) + 1) / len(self.sorted) def __call__(self, x): ind = np.searchsorted(self.sorted, x) - 1 return self.y[ind] values = [] for x in range(len(test_post)): ecdf = ECDF(np.exp(test_post[x])) values.append(ecdf(np.exp(test_y[x]))) plt.hist(values) plt.title(\u0026#39;Transformed Data\u0026#39;)   We can see an obvious deviation from the expected uniform distribution here. It looks like our distribution most significantly under-predicts large volatiltiy values. This makes sense when looking back to the residual graph, large jumps aren\u0026rsquo;t handled well.\nstats.kstest(values, \u0026#39;uniform\u0026#39;) KstestResult(statistic=0.0760443418013857, pvalue=8.408548699568476e-05) This Kolmogorov-Smirnov test takes the null hypothesis that the data matches the specified distribution, in this case a uniform. It looks like we can handedly reject that hypothesis. This means that the posterior predictive is not fully capable of representing the real distribution.\nConclusion and Extensions It seems like this very simple model does pretty well providing a point-forecast of future volatility, however it fails at accurately describing the distribution of future volatility. This could be fixed in several ways. First is assuming a different distributional form in the model, such as something with fatter tails like a Student\u0026rsquo;s T. Another possibility is allowing the standard deviation of the normal to vary with time. That is more in line with models like traditional stochastic volatility.\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/vol_linear_model/","summary":"Using a simple bayesian autoregressive model to forecast future volatility","title":"Bayesian Autoregressive Volatility Forecasting"},{"content":"Today, I\u0026rsquo;m going to be discussing the difference between two volatility estimators.\nThe Data  I\u0026rsquo;m going to be using daily-resolution SPX data from Sharadar as well as minute-resolution SPX data from First Rate Data.\nimport pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15, 10) conn = sqlite3.Connection(\u0026#34;data.db\u0026#34;) spx_daily = pd.read_sql(\u0026#34;SELECT * FROM prices WHERE ticker=\u0026#39;^GSPC\u0026#39;\u0026#34;, conn, index_col=\u0026#34;date\u0026#34;, parse_dates=[\u0026#34;date\u0026#34;]) spx_minute = minute = pd.read_csv(\u0026#34;SPX_1min.csv\u0026#34;, header=0,names=[\u0026#39;datetime\u0026#39;, \u0026#39;open\u0026#39;, \u0026#39;high\u0026#39;, \u0026#39;low\u0026#39;, \u0026#39;close\u0026#39;], index_col=\u0026#39;datetime\u0026#39;, parse_dates=True) # A quick look at the data spx_daily.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ticker open high low close volume dividends closeunadj lastupdated   date              1997-12-31 ^GSPC 970.84 975.02 967.41 970.43 467280000 0 970.43 2019-02-03   1998-01-02 ^GSPC 970.43 975.04 965.73 975.04 366730000 0 975.04 2019-02-03   1998-01-05 ^GSPC 975.04 982.63 969.00 977.07 628070000 0 977.07 2019-02-03   1998-01-06 ^GSPC 977.07 977.07 962.68 966.58 618360000 0 966.58 2019-02-03   1998-01-07 ^GSPC 966.58 966.58 952.67 964.00 667390000 0 964.00 2019-02-03     spx_minute.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  open high low close   datetime         2007-04-27 12:25:00 1492.39 1492.54 1492.39 1492.54   2007-04-27 12:26:00 1492.57 1492.57 1492.52 1492.56   2007-04-27 12:27:00 1492.58 1492.64 1492.58 1492.63   2007-04-27 12:28:00 1492.63 1492.73 1492.63 1492.73   2007-04-27 12:29:00 1492.91 1492.91 1492.87 1492.87     The Estimators  Now, what I want to do is compare volatility estimates from these two data sets. I would prefer to use the daily data if possible, because in my case it\u0026rsquo;s easier to get and updates more frequently.\nGarman-Klass Estimator This estimator has been around for a while and is deemed to be far more effcient than a traditional close-to-close volatility estimator (Garman and Klass, 1980).\nFrom equation 20 in the paper, a jump adjusted volatility estimator:\n$$f = 0.73$$ percentage of the day trading is closed based on NYSE hours of 9:30 to 4\n$$a = 0.12$$ as they suggest in the paper\n$$\\sigma^2_{unadj} = 0.511(u - d)^2 - 0.019(c(u+d) - 2ud) - 0.383c^2$$\n$$\\sigma^2_{adj} = 0.12\\frac{(O_{1} - C_{0})^2}{0.73} + 0.12\\frac{\\sigma^2_{unadj}}{0.27}$$\nWhere\n$$u = H_{1} - O_{1}$$ the normalized high\n$$d = L_{1} - O_{1}$$ the normalized low\n$$c = C_{1} - O_{1}$$ the normalized close and subscripts indicating time. They also indicate in the paper that these equations expect the log of the price series.\ndef gk_vol_calc(data): u = np.log(data[\u0026#39;high\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) d = np.log(data[\u0026#39;low\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) c = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;open\u0026#39;]) vol_unadj = 0.511 * (u - d)**2 - 0.019 * (c * (u + d) - 2 * u * d) - 0.283 * c**2 jumps = np.log(data[\u0026#39;open\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) vol_adj = 0.12 * (jumps**2 / 0.73) + 0.12 * (vol_unadj / 0.27) return vol_adj # Let\u0026#39;s take a look gk_vol = np.sqrt(gk_vol_calc(spx_daily)) gk_vol.plot()   As an aside, opening jumps have become more common and larger in recent years, maybe something to investigate. This is as a percentage, so it\u0026rsquo;s not a simple case of the index values becoming larger.\n(spx_daily[\u0026#39;open\u0026#39;] / spx_daily[\u0026#39;close\u0026#39;].shift(1) - 1).plot()   Realized Volatility Estimator This estimator is very simply and has become more prominent in the literature in the last few years because of increasing availability of higher-frequency data. Based on (Liu, Patton, and Sheppard, 2012), it\u0026rsquo;s hard to beat a 5-minute RV. Here, I\u0026rsquo;m going to use a 1-minute estimator, which is also shown to be effective.\n$$RV_{t} = \\sum_{k=1}^n r_{t,k}^2$$ where the t index is each day, and the k index represents each intraday return\nFor daily volatility, it\u0026rsquo;s simply a sum of squared returns from within that day. So in this case we calculate returns for each 1 minute period, square them, and they sum them for each day.\ndef rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\u0026#39;close\u0026#39;]) - np.log(data[\u0026#39;close\u0026#39;].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) # Let\u0026#39;s take a look at this one rv = np.sqrt(rv_calc(spx_minute)) rv.plot()   Comparisons  # Because the minute data has a shorter history, let\u0026#39;s match them up gk_vol = gk_vol.reindex(rv.index) rv.plot() gk_vol.plot()   Here\u0026rsquo;s a plot of our two different volatility estimators with RV in blue and Garman-Klass in orange. The RV estimator is far less noisy, looking at each of their graphs above. The Garman-Klass estimator also seems to persistently return a lower result than RV. This is backed up by looking at a graph of their difference.\n(gk_vol - rv).plot()   Netx, let\u0026rsquo;s analyze how they do at normalizing the returns of the S\u0026amp;P 500. According to (Molnár, 2015) normalizing a number of equity returns by their Garman-Klass estimated volatility does indeed make their distributions normal. Let\u0026rsquo;s see if we can replicate that result with either of our esimates on the S\u0026amp;P 500.\n# Daily close-to-close returns of the S\u0026amp;P 500 spx_returns = np.log(spx_daily[\u0026#39;close\u0026#39;]) - np.log(spx_daily[\u0026#39;close\u0026#39;].shift(1)) spx_returns = spx_returns.reindex(rv.index) # Normalizing by our estimated volatilties gk_vol_norm = (spx_returns / gk_vol).dropna() rv_norm = (spx_returns / rv).dropna() # Here are the unadjusted returns _, _, _ = plt.hist(spx_returns, bins=50)   # Here\u0026#39;s normalized by the Garman-Klass Estimator _, _, _ = plt.hist(gk_vol_norm, bins=50)   # And this is by the RV estimator _, _, _ = plt.hist(rv_norm, bins=50)   At first glance, the RV adjusted returns seem most like normal to me, let\u0026rsquo;s run some tests. These Scipy tests set the null hypothesis that the data comes from a corresponding normal distribution. So if the p-value is small we can reject that hypothesis and conclude the distribution is non-normal.\nprint(stats.skewtest(gk_vol_norm)) print(stats.skewtest(rv_norm)) Garman-Klass Skew: SkewtestResult(statistic=-0.3767923327324783, pvalue=0.7063279391177064) RV-5min Skew: SkewtestResult(statistic=5.251294175425576, pvalue=1.5103423951480544e-07) print(stats.kurtosistest(gk_vol_norm)) print(stats.kurtosistest(rv_norm)) KurtosistestResult(statistic=-13.088609427904334, pvalue=3.825472809774632e-39) KurtosistestResult(statistic=0.315320709120601, pvalue=0.7525181628202805) Looks like the Garman-Klass-normalized returns have normal skew, but non-normal kurtosis. The RV-normalized returns have non-normal skew but normal kurtosis! There\u0026rsquo;s no winning here! Both are non-normal in different ways. Either normalization does do better than the unadjusted returns though.\nprint(stats.skewtest(spx_returns.dropna())) print(stats.kurtosistest(spx_returns.dropna())) SkewtestResult(statistic=-12.386230904806132, pvalue=3.1028724633560147e-35) KurtosistestResult(statistic=26.470418979318143, pvalue=2.124045513612033e-154) Conclusion  While from a statistical point of view, neither option seems particularly favorable, my personal choice is going to be the RV estimator. I think the literature is clear on its efficacy and its less noisy and conceptually easier. It\u0026rsquo;s been said that when there are a bunch of competing theories, none of them are very good. So I\u0026rsquo;ll pick the simplest option and go with RV.\nReferences    Garman, M., \u0026amp; Klass, M. (1980). On the Estimation of Security Price Volatilities from Historical Data. The Journal of Business, 53(1), 67-78. Retrieved February 14, 2021, from http://www.jstor.org/stable/2352358\n  Liu, L., Patton, A., \u0026amp; Sheppard, K. (2012). Does Anything Beat 5-Minute RV? A Comparison of Realized Measures Across Multiple Asset Classes. SSRN. http://dx.doi.org/10.2139/ssrn.2214997\n  Molnár, P. (2015). Properties of Range-Based Volatility Estimators. SSRN. Retrieved from https://ssrn.com/abstract=2691435\n  ","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/vol_estimators/","summary":"Comparing Garman-Klass estimator to 5-minute Realized Volatility estimator.","title":"Comparison of Volatility Estimators"},{"content":"Alright, with this post I\u0026rsquo;m going to start a series on portfolio optimization techniques! This is one of my favorite topics in finance. This post is going to construct a portfolio based on the diversification ratio, which is outlined in the papers linked below. The basic idea is to maximize the Diversification ratio, which is defined as the weighted average volatilities of assets in the portfolio divided by the total portfolio volatility. This makes intuitive sense, by increasing diversification we lower portfolio volatility compared to the average volatility of the assets that make it up.\nChoueifaty, Y., \u0026amp; Coignard, Y. (2008). Toward Maximum Diversification. The Journal of Portfolio Management, 40-51. doi:https://doi.org/10.3905/JPM.2008.35.1.40\nChoueifaty, Y., Reynier, J., \u0026amp; Froidure, T. (2013). Properties of the Most Diversified Portfolio. Journal of Investment Strategies, 49-70. doi:http://doi.org/10.2139/ssrn.1895459\nThe assets  I\u0026rsquo;m going to mainly focus on Vanguard ETFs as they have the lowest fees. For anything they don\u0026rsquo;t offer, I\u0026rsquo;m using iShares. I\u0026rsquo;m also limiting myself to funds with inception dates \u0026gt;10 years ago for stability.\nHere\u0026rsquo;s the list:    Symbol Description     VGSH Short-term Treasury   VGIT Mid-term Treasury   VGLT Long-term Treasury   TIP TIPS Treasury Bonds   VMBS Agency MBS   SUB Municipal Bonds   VCSH Short-term Investment Grade Corporate Bonds   VCIT Mid-term Investment Grade Corporate Bonds   VCLT Long-term Investment Grade Corporate Bonds   HYG High-yield Corporate Bonds   EMB Emerging Markets Bonds   IGOV International Treasuries   VV Large Cap US Stocks   VO Mid-Cap US Stocks   VB Small-Cap US Stocks   VWO Emerging Markets Stocks   VEA Non-US Developed Markets Stocks   IYR US Real Estate   IFGL Non-US Real Estate    Data  All of this is the code to fetch historical data from QuantConnect and calculate returns.\nimport numpy as np import pandas as pd symbols = [\u0026#39;VGSH\u0026#39;, \u0026#39;VGIT\u0026#39;, \u0026#39;VGLT\u0026#39;, \u0026#39;TIP\u0026#39;, \u0026#39;VMBS\u0026#39;, \u0026#39;SUB\u0026#39;, \u0026#39;VCSH\u0026#39;, \u0026#39;VCIT\u0026#39;, \u0026#39;VCLT\u0026#39;, \u0026#39;HYG\u0026#39;, \u0026#39;EMB\u0026#39;, \u0026#39;IGOV\u0026#39;, \u0026#39;VV\u0026#39;, \u0026#39;VO\u0026#39;, \u0026#39;VB\u0026#39;, \u0026#39;VWO\u0026#39;, \u0026#39;VEA\u0026#39;, \u0026#39;IYR\u0026#39;, \u0026#39;IFGL\u0026#39;] qb = QuantBook() symbols_data = {symbol: qb.AddEquity(symbol) for symbol in symbols} from datetime import datetime # This is QuantConnect API code to get price history history = qb.History(qb.Securities.Keys, datetime(2009, 1, 1), datetime(2020, 12, 31), Resolution.Daily) history = history[\u0026#39;close\u0026#39;].unstack(level=0).dropna() I\u0026rsquo;m using arithmetic returns here so I can easily weight the returns across assets when computing portfolio returns.\nreturns = (history / history.shift(1)) - 1 returns = returns.dropna() # Let\u0026#39;s define some helper functions to get cumulative return series and the total return def get_cum_returns(returns): return (returns + 1).cumprod() - 1 def get_total_return(returns): return np.product(returns + 1) - 1 The Optimization  This function calculates the diversification ratio for a portfolio given asset weights and their covariance matrix. This is from equation (1) (Choueifaty \u0026amp; Coignard, 2008).\ndef diverse_ratio(weights, covariance): # Standard deviation vector stds = np.sqrt(np.diagonal(covariance)) # Asset-weighted standard deviation num = np.dot(weights, stds) # Portfolio standard deviation denom = np.sqrt(weights @ covariance @ weights) return num / denom Now, to confirm that scipy minimize works as we expect for this problem, I\u0026rsquo;m going to test a bunch of randomized starting weights to confirm that the final weights end up the same. I increase the level of precision using the \u0026lsquo;ftol\u0026rsquo; option because returns are fairly small decimal quantities and I want to ensure the optimization converges completely.\nfrom scipy.optimize import minimize cov = np.cov(returns.values.T) # Long-only constraint bounds = [(0, 1) for x in range(len(cov))] # Portfolio weights must sum to 1 constraints = ( {\u0026#39;type\u0026#39;: \u0026#39;eq\u0026#39;, \u0026#39;fun\u0026#39;: lambda x: np.sum(x) - 1} ) results = [] for x in range(100): # Set initial weights randomly initial = np.random.random(len(cov)) # Use negative of objective function to maximize result = minimize(lambda x, y: -1 * diverse_ratio(x, y), initial, method=\u0026#39;SLSQP\u0026#39;, args=(cov), bounds=bounds, constraints=constraints, options={\u0026#39;ftol\u0026#39;: 1e-10}) results.append(result.x) # Stack all optimized weight vectors, and round to 4 digits after the decimal results_array = np.round(np.stack(results), 4) # Let\u0026#39;s look at the standard deviation of the asset weights accross the different optimizations stds = np.std(results_array, axis=0) # Looks like they\u0026#39;re all zero or nearly zero! print(stds) [0.00000000e+00 3.12250226e-17 0.00000000e+00 1.24900090e-16 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.16333634e-17 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.08166817e-17 1.66533454e-16 1.38777878e-16 1.66533454e-16 0.00000000e+00 0.00000000e+00 1.24900090e-16 2.42861287e-17] Looks like the optimization converges to the same values every time regardless of starting point! This means it\u0026rsquo;s finding the true minimum (or maximum in this case). Let\u0026rsquo;s look the at the weights for each symbol.\n# I\u0026#39;ll just grab the last optimization result this way weights_series = pd.Series(data=np.round(result.x, 4), index=returns.columns) print(weights_series) EMB 0.0000 HYG 0.0215 IFGL 0.0000 IGOV 0.0586 IYR 0.0000 SUB 0.2759 TIP 0.0000 VB 0.0336 VCIT 0.0000 VCLT 0.0000 VCSH 0.0000 VEA 0.0265 VGIT 0.1150 VGLT 0.1868 VGSH 0.1825 VMBS 0.0000 VO 0.0000 VV 0.0817 VWO 0.0179 # Let\u0026#39;s drop everything with a zero weight final_weights = weights_series[weights_series \u0026gt; 0] # Sort by weight for viewing ease print(final_weights.sort_values(ascending=False)) SUB 0.2759 VGLT 0.1868 VGSH 0.1825 VGIT 0.1150 VV 0.0817 IGOV 0.0586 VB 0.0336 VEA 0.0265 HYG 0.0215 VWO 0.0179 # Confirm everything sums to 1. Looks good! final_weights.sum() 0.9999999999999999 Backtest Results  After doing some offscreen magic to implement this in QuantConnect, we get a dataframe tracking portfolio weights over each month. The algorithm starts at the beginning of 2011 and runs to the end of 2020. On the first trading day of each month it computes the portfolio using the code above using the past year of returns data for each ticker. Note that this is slightly different than the above that uses the entire return history in the optimization. Let\u0026rsquo;s take a closer look at a particular ETFs portfolio allocation over time. I\u0026rsquo;m going to use VGSH for this because it\u0026rsquo;s the least risky, most cash-like instrument under consideration.\nYou can see that the weight changes quite drastically over time, from near zero to nearly 90% in the later parts of 2020. This reflects the nature of how we are calculating asset variances and correlations using only the last 252 days of data. When volatilities or correlations change it causes changes in the allocation.\nweights_frame[\u0026#39;VGSH\u0026#39;].plot(figsize=(15,10))   In this case, it\u0026rsquo;s caused by a large change in correlation for certain assets in early 2020. Shown in the graph below is the correlation between VGSH and the other ETFs over time. Note the large downward jump on the right side. This shows the weakness of using a rolling data approach like in the backtest. You get big market jumps that dramatically shift your allocation and then when they eventually fall out the backward-looking window, you get big jumps again. I want to come back to this topic some time in the future.\nrolling_corr = returns.rolling(252).corr() rolling_corr[\u0026#39;VGSH\u0026#39;].unstack().plot(figsize=(15,10))   # let\u0026#39;s reindex the monthly weights frame to daily with forward fill to match returns arra weights_frame = weights_frame.reindex(returns.index, method=\u0026#39;ffill\u0026#39;) # Now we can calculate portfolio returns by weight the returns and summing port_returns = (weights_frame.values * returns).sum(axis=1, skipna=False).dropna() # We can also calculate cumulative returns this way because we\u0026#39;re working with logarithmic returns cum_port_returns = get_cum_returns(port_returns) Alright, plotted below are the cumulative returns for the strategy! Note this is without transaction costs factored in.\ncum_port_returns.plot(figsize=(15, 10))   Now let\u0026rsquo;s assemble some backtest statistics. We\u0026rsquo;re going to be using mlfinlab for this task.\nfrom mlfinlab import backtest_statistics total_return = get_total_return(port_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(port_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(cum_port_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 29.77% CAGR 2.94% Sharpe Ratio 1.15 Maximum Drawdown 7.07% MAR Ratio 0.42 Let\u0026rsquo;s compare that to just US large cap stocks over the same period.\nvv_returns = returns[\u0026#39;VV\u0026#39;][\u0026#39;2011\u0026#39;:] vv_cum_returns = get_cum_returns(vv_returns) total_return = get_total_return(vv_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(vv_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(vv_cum_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 267.74% CAGR 15.57% Sharpe Ratio 0.84 Maximum Drawdown 34.28% MAR Ratio 0.45 Looks like the maximum diversification portfolio achieves a higher sharpe ratio! Although it comes at the cost of signficantly lower total returns. More interesting is the MAR ratio, defined as the CAGR over the maximum drawdown. This is a useful ratio because it gauges how much extra return you are getting for taking on heavier drawdown risk. It looks like large cap US stocks win out on this metric.\nIt gives a different perspective than the Sharpe ratio. The Sharpe ratio uses only standard deviation as a metric for risk. This can be very unrealistic because radically different equity curves can actually have the same Sharpe ratio and total return. That can be interestingly illustrated by reordering returns.\n# Okay, let\u0026#39;s sort VV returns from least to greatest. Note that these are the same returns, just reordered. sorted_returns = pd.Series(sorted(vv_returns.values), index=vv_returns.index) cum_sorted_returns = get_cum_returns(sorted_returns) # Here you can see the cumulative return graphs. The sorted one looks very unusual, but in fact, the total return ends # up exactly the same! cum_sorted_returns.plot(figsize=(15, 10)) vv_cum_returns.plot()   total_return = get_total_return(sorted_returns) cagr = (total_return + 1)**(1 / 9) - 1 sharpe = backtest_statistics.sharpe_ratio(sorted_returns) drawdown, _ = backtest_statistics.drawdown_and_time_under_water(cum_sorted_returns + 1) mar_ratio = cagr / drawdown.max() pd.Series({\u0026#39;Total Return\u0026#39;: f\u0026#39;{round(total_return * 100, 2)}%\u0026#39;,\u0026#39;CAGR\u0026#39;: f\u0026#39;{round(cagr * 100, 2)}%\u0026#39;, \u0026#39;Sharpe Ratio\u0026#39;: round(sharpe, 2), \u0026#39;Maximum Drawdown\u0026#39;: f\u0026#39;{round(drawdown.max() * 100, 2)}%\u0026#39;, \u0026#39;MAR Ratio\u0026#39;: round(mar_ratio, 2)}) Total Return 267.74% CAGR 15.57% Sharpe Ratio 0.84 Maximum Drawdown 99.97% MAR Ratio 0.16 As you can see the total return, CAGR, and Sharpe ratio are all the same as the original return series! But the maximum drawdown is significantly higher. Obviously this is a worst case scenario, but it shows how drawdowns can drastically affect your portfolio performance over time. Volatility by itself doesn\u0026rsquo;t reflect all kinds of risk because it ignores path dependency. This again is a topic worth covering in more detail at a later date.\nConclusions  Even considering the backtest and attributes of this simple strategy shows deep complexity. In future, I want to compare this optimization strategy to others like the traditional mean-variance approach, hierarchical risk parity, minimum variance, and others. Along with that is discussing extensions like using models to provide forecasts for asset volatility and correlation.\nSo with all those things to think about, see you next time!\n","permalink":"http://eadains.github.io/OptionallyBayesHugo/posts/mdp_etf_portfolio/","summary":"Constructing a portfolio from a selection of ETFs to maximize the diversification ratio","title":"A Most Diversified ETF Portfolio"}]
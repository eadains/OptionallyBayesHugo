<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Mixture Density Network for Forecasting Realized Volatility | Optionally Bayes</title>
<meta name="keywords" content="volatility, forecasting, mixture-density-network, machine-learning, pytorch, model-building" />
<meta name="description" content="Using a mixture density neural network implemented in PyTorch to forecast the distribution of future realized volatility.">
<meta name="author" content="Erik Dains">
<link rel="canonical" href="http://eadains.github.io/OptionallyBayesHugo/posts/vol_mdn/" />
<link crossorigin="anonymous" href="/OptionallyBayesHugo/assets/css/stylesheet.min.22565d513b14d8ddaed3ac5691392cfbd9911aeb414bfcf2511a41766f93b9b9.css" integrity="sha256-IlZdUTsU2N2u06xWkTks&#43;9mRGutBS/zyURpBdm&#43;Tubk=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/OptionallyBayesHugo/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://eadains.github.io/OptionallyBayesHugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://eadains.github.io/OptionallyBayesHugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://eadains.github.io/OptionallyBayesHugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://eadains.github.io/OptionallyBayesHugo/apple-touch-icon.png">
<link rel="mask-icon" href="http://eadains.github.io/OptionallyBayesHugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.84.0" />

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
    integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js"
    integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js"
    integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<meta property="og:title" content="Mixture Density Network for Forecasting Realized Volatility" />
<meta property="og:description" content="Using a mixture density neural network implemented in PyTorch to forecast the distribution of future realized volatility." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://eadains.github.io/OptionallyBayesHugo/posts/vol_mdn/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-07T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-04-07T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Mixture Density Network for Forecasting Realized Volatility"/>
<meta name="twitter:description" content="Using a mixture density neural network implemented in PyTorch to forecast the distribution of future realized volatility."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://eadains.github.io/OptionallyBayesHugo/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Mixture Density Network for Forecasting Realized Volatility",
      "item": "http://eadains.github.io/OptionallyBayesHugo/posts/vol_mdn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Mixture Density Network for Forecasting Realized Volatility",
  "name": "Mixture Density Network for Forecasting Realized Volatility",
  "description": "Using a mixture density neural network implemented in PyTorch to forecast the distribution of future realized volatility.",
  "keywords": [
    "volatility", "forecasting", "mixture-density-network", "machine-learning", "pytorch", "model-building"
  ],
  "articleBody": "Okay, today we are moving up in the world and I’m going to use the magic of neural networks to forecast volatility.\nThe Data import pandas as pd import numpy as np import sqlite3 from matplotlib import pyplot as plt from scipy import stats # Set default figure size plt.rcParams[\"figure.figsize\"] = (15, 10) pd.plotting.register_matplotlib_converters() # Here\"s my minute data for the S\u0026P 500 spx_minute = pd.read_csv(\"SPX_1min.csv\", header=0,names=[\"datetime\", \"open\", \"high\", \"low\", \"close\"], index_col=\"datetime\", parse_dates=True) # Here\"s the function for calculating the 1-min RV, as discussed in my last post def rv_calc(data): results = {} for idx, data in data.groupby(data.index.date): returns = np.log(data[\"close\"]) - np.log(data[\"close\"].shift(1)) results[idx] = np.sum(returns**2) return pd.Series(results) spx_variance = rv_calc(spx_minute) conn = sqlite3.Connection(\"data.db\") spx_data = pd.read_sql(\"SELECT * FROM prices WHERE ticker='^GSPC'\", conn, index_col=\"date\", parse_dates=\"date\") spx_returns = np.log(spx_data[\"close\"]) - np.log(spx_data[\"close\"].shift(1)) spx_returns = spx_returns.dropna() vix_data = pd.read_sql(\"SELECT * FROM prices WHERE ticker='^VIX'\", conn, index_col=\"date\", parse_dates=\"date\") # This puts it into units of daily standard deviation vix = vix_data[\"close\"] / np.sqrt(252) / 100 def create_lags(series, lags, name=\"x\"): \"\"\" Creates a dataframe with lagged values of the given series. Generates columns named x_t-n which means the value of each row is the value of the original series lagged n times \"\"\" result = pd.DataFrame(index=series.index) result[f\"{name}_t\"] = series for n in range(lags): result[f\"{name}_t-{n+1}\"] = series.shift((n+1)) return result The predictive variables are the VIX, returns of the index, and our calculated realized variance. I include the 21 past values of these variables.\nvix_lags = create_lags(np.log(vix), 21, name=\"vix\") return_lags = create_lags(spx_returns, 21, name=\"returns\") rv_lags = create_lags(np.log(spx_variance), 21, name=\"rv\") x = pd.concat([vix_lags, return_lags, rv_lags], axis=1).dropna() # We want to predict log of variance y = np.log(spx_variance.rolling(5).sum().shift(-5)).dropna() common_index = x.index.intersection(y.index) x = x.loc[common_index] y = y.loc[common_index] The Model I’m using a mixture density network to model future volatility. This is because I want an estimate of the future distribution of volatility, not just a point estimate. A mixture density network outputs the parameters for making a mixture of normal distributions. This is useful because you can approximate any arbitrary distribution with a large enough mixture of only normal distributions.\nimport torch import torch.nn as nn from torch.distributions import Categorical, Normal, Independent, MixtureSameFamily from torch.optim.swa_utils import AveragedModel, SWALR torch.set_default_dtype(torch.float64) class MDN(nn.Module): def __init__(self, in_dim, out_dim, hidden_dim, n_components): super().__init__() self.n_components = n_components # Last layer output dimension rationale: # Need two parameters for each distributionm thus 2 * n_components. # Need each of those for each output dimension, thus that multiplication self.norm_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, 2 * n_components * out_dim) ) self.cat_network = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ELU(), nn.Dropout(), nn.Linear(hidden_dim, n_components * out_dim) ) def forward(self, x): norm_params = self.norm_network(x) # Split so we get parameters for mean and standard deviation mean, std = torch.split(norm_params, norm_params.shape[1] // 2, dim=1) # We need rightmost dimension to be n_components for mixture mean = mean.view(mean.shape[0], -1, self.n_components) std = std.view(std.shape[0], -1, self.n_components) normal = Normal(mean, torch.exp(std)) cat_params = self.cat_network(x) # Again, rightmost dimension must be n_components cat = Categorical(logits=cat_params.view(cat_params.shape[0], -1, self.n_components)) return MixtureSameFamily(cat, normal) test_index = int(len(x) * .75) train_x = torch.Tensor(x.iloc[:test_index].values) train_y = torch.Tensor(y.iloc[:test_index].values) test_x = torch.Tensor(x.iloc[test_index:].values) test_y = torch.Tensor(y.iloc[test_index:].values) in_dim = len(x.columns) out_dim = 1 n_components = 5 hidden_dim = 250 Below here is the training loop. I’m using a cosine annealing learning rate schedule to better explore the parameter space, as well as using model averaging over the last 500 iterations so the model generalizes better.\nmodel = MDN(in_dim, out_dim, hidden_dim, n_components) optimizer = torch.optim.AdamW(model.parameters(), lr=.001) scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 100, 2) swa_model = AveragedModel(model) swa_start = 400 swa_scheduler = SWALR(optimizer, swa_lr=0.001, anneal_epochs=10, anneal_strategy=\"cos\") train_losses = [] validation_losses = [] model.train() swa_model.train() for epoch in range(500): optimizer.zero_grad() output = model(train_x) train_loss = -output.log_prob(train_y.view(-1, 1)).sum() train_losses.append(train_loss.detach()) test_loss = -model(test_x).log_prob(test_y.view(-1, 1)).sum() validation_losses.append(test_loss.detach()) train_loss.backward() optimizer.step() if epoch  swa_start: swa_model.update_parameters(model) swa_scheduler.step() else: scheduler.step() plt.plot(train_losses) plt.plot(validation_losses) plt.xlabel(\"Training Epochs\") plt.ylabel(\"Model Loss\") plt.title(\"Training \u0026 Validation Losses\") plt.legend([\"Training\", \"Validation\"])  swa_model.eval() output_mean = np.sqrt(np.exp(swa_model(test_x).mean.detach().numpy().squeeze())) y_trans = np.sqrt(np.exp(test_y.numpy().squeeze())) output_sample = np.sqrt(np.exp(swa_model(test_x).sample([5000]).numpy().squeeze())) Our out-of-sample R-squared is excellent, much higher than my previous simple linear model.\nregress = stats.linregress(output_mean, y_trans) print(f\"R-squared: {regress.rvalue**2}\") R-squared: 0.7128714654332561 plt.plot(output_mean) plt.plot(y_trans) plt.xlabel(\"Time\") plt.ylabel(\"Volatility\") plt.title(\"Predicted and Actual Volatility\") plt.legend([\"Model\", \"Actual\"])  Our distributional assumption also does well. We expect 5% of cases to be outside what the model distribution forecasts, and we find that to be the case.\npercent = np.percentile(output_sample, 95, axis=0) print(f\"Number of exceedences: {(y_trans  percent).sum() / len(y_trans)}\") Number of exceedences: 0.04477611940298507 Further testing the distribution accuracy, let’s see if doing a probability integral transform yields a uniform.\nclass ECDF: def __init__(self, data): self.sorted = data self.sorted.sort() self.y = np.arange(1, len(self.sorted) + 1) / len(self.sorted) def __call__(self, x): ind = np.searchsorted(self.sorted, x) - 1 return self.y[ind] values = [] for x in range(len(y_trans)): ecdf = ECDF(output_sample[x]) values.append(ecdf(y_trans[x])) plt.hist(values, bins=10)  stats.kstest(values, \"uniform\") KstestResult(statistic=0.028702640642939155, pvalue=0.46125545362008036) We can’t reject the null hypothesis that the transformed values come from a uniform distribution! That means our distributions accurately models the data’s real distribution.\nConclusion This model seems quite excellent. I’m going to use this model for my future posts about how to make an effective trading strategy. Next time I’m going to discuss Kelly Bet Sizing and its application to continuous distributions.\n",
  "wordCount" : "844",
  "inLanguage": "en",
  "datePublished": "2021-04-07T00:00:00Z",
  "dateModified": "2021-04-07T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Erik Dains"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://eadains.github.io/OptionallyBayesHugo/posts/vol_mdn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Optionally Bayes",
    "logo": {
      "@type": "ImageObject",
      "url": "http://eadains.github.io/OptionallyBayesHugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://eadains.github.io/OptionallyBayesHugo/" accesskey="h" title="Optionally Bayes (Alt + H)">Optionally Bayes</a>
            <span class="logo-switches">
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://eadains.github.io/OptionallyBayesHugo/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://eadains.github.io/OptionallyBayesHugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://eadains.github.io/OptionallyBayesHugo/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://eadains.github.io/OptionallyBayesHugo/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Mixture Density Network for Forecasting Realized Volatility
    </h1>
    <div class="post-meta">April 7, 2021&nbsp;·&nbsp;Erik Dains
</div>
  </header> 
  <div class="post-content"><p>Okay, today we are moving up in the world and I&rsquo;m going to use the magic of neural networks to forecast volatility.</p>
<h1 id="the-data">The Data<a hidden class="anchor" aria-hidden="true" href="#the-data">#</a></h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
<span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
<span style="color:#f92672">import</span> sqlite3
<span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
<span style="color:#f92672">from</span> scipy <span style="color:#f92672">import</span> stats

<span style="color:#75715e"># Set default figure size</span>
plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;figure.figsize&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">10</span>)
pd<span style="color:#f92672">.</span>plotting<span style="color:#f92672">.</span>register_matplotlib_converters()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Here&#34;s my minute data for the S&amp;P 500</span>
spx_minute <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;SPX_1min.csv&#34;</span>, header<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;datetime&#34;</span>, <span style="color:#e6db74">&#34;open&#34;</span>, <span style="color:#e6db74">&#34;high&#34;</span>, <span style="color:#e6db74">&#34;low&#34;</span>, <span style="color:#e6db74">&#34;close&#34;</span>],
                                  index_col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;datetime&#34;</span>, parse_dates<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Here&#34;s the function for calculating the 1-min RV, as discussed in my last post</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rv_calc</span>(data):
    results <span style="color:#f92672">=</span> {}
    
    <span style="color:#66d9ef">for</span> idx, data <span style="color:#f92672">in</span> data<span style="color:#f92672">.</span>groupby(data<span style="color:#f92672">.</span>index<span style="color:#f92672">.</span>date):
        returns <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(data[<span style="color:#e6db74">&#34;close&#34;</span>]) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>log(data[<span style="color:#e6db74">&#34;close&#34;</span>]<span style="color:#f92672">.</span>shift(<span style="color:#ae81ff">1</span>))
        results[idx] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(returns<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
        
    <span style="color:#66d9ef">return</span> pd<span style="color:#f92672">.</span>Series(results)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">spx_variance <span style="color:#f92672">=</span> rv_calc(spx_minute)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">conn <span style="color:#f92672">=</span> sqlite3<span style="color:#f92672">.</span>Connection(<span style="color:#e6db74">&#34;data.db&#34;</span>)
spx_data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_sql(<span style="color:#e6db74">&#34;SELECT * FROM prices WHERE ticker=&#39;^GSPC&#39;&#34;</span>, conn, index_col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;date&#34;</span>, parse_dates<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;date&#34;</span>)
spx_returns <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(spx_data[<span style="color:#e6db74">&#34;close&#34;</span>]) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>log(spx_data[<span style="color:#e6db74">&#34;close&#34;</span>]<span style="color:#f92672">.</span>shift(<span style="color:#ae81ff">1</span>))
spx_returns <span style="color:#f92672">=</span> spx_returns<span style="color:#f92672">.</span>dropna()

vix_data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_sql(<span style="color:#e6db74">&#34;SELECT * FROM prices WHERE ticker=&#39;^VIX&#39;&#34;</span>, conn, index_col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;date&#34;</span>, parse_dates<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;date&#34;</span>)
<span style="color:#75715e"># This puts it into units of daily standard deviation</span>
vix <span style="color:#f92672">=</span> vix_data[<span style="color:#e6db74">&#34;close&#34;</span>] <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">252</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">100</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_lags</span>(series, lags, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x&#34;</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Creates a dataframe with lagged values of the given series.
</span><span style="color:#e6db74">    Generates columns named x_t-n which means the value of each row is the value of the original series lagged n times
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    result <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(index<span style="color:#f92672">=</span>series<span style="color:#f92672">.</span>index)
    result[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">_t&#34;</span>] <span style="color:#f92672">=</span> series
    
    <span style="color:#66d9ef">for</span> n <span style="color:#f92672">in</span> range(lags):
        result[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">_t-</span><span style="color:#e6db74">{</span>n<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>] <span style="color:#f92672">=</span> series<span style="color:#f92672">.</span>shift((n<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>))
        
    <span style="color:#66d9ef">return</span> result
</code></pre></div><p>The predictive variables are the VIX, returns of the index, and our calculated realized variance. I include the 21 past values of these variables.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">vix_lags <span style="color:#f92672">=</span> create_lags(np<span style="color:#f92672">.</span>log(vix), <span style="color:#ae81ff">21</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;vix&#34;</span>)
return_lags <span style="color:#f92672">=</span> create_lags(spx_returns, <span style="color:#ae81ff">21</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;returns&#34;</span>)
rv_lags <span style="color:#f92672">=</span> create_lags(np<span style="color:#f92672">.</span>log(spx_variance), <span style="color:#ae81ff">21</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;rv&#34;</span>)

x <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([vix_lags, return_lags, rv_lags], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>dropna()
<span style="color:#75715e"># We want to predict log of variance</span>
y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(spx_variance<span style="color:#f92672">.</span>rolling(<span style="color:#ae81ff">5</span>)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>shift(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>))<span style="color:#f92672">.</span>dropna()

common_index <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>index<span style="color:#f92672">.</span>intersection(y<span style="color:#f92672">.</span>index)
x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>loc[common_index]
y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>loc[common_index]
</code></pre></div><h1 id="the-model">The Model<a hidden class="anchor" aria-hidden="true" href="#the-model">#</a></h1>
<p>I&rsquo;m using a mixture density network to model future volatility. This is because I want an estimate of the future <em>distribution</em> of volatility, not just a point estimate. A mixture density network outputs the parameters for making a mixture of normal distributions. This is useful because you can approximate any arbitrary distribution with a large enough mixture of only normal distributions.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
<span style="color:#f92672">from</span> torch.distributions <span style="color:#f92672">import</span> Categorical, Normal, Independent, MixtureSameFamily
<span style="color:#f92672">from</span> torch.optim.swa_utils <span style="color:#f92672">import</span> AveragedModel, SWALR

torch<span style="color:#f92672">.</span>set_default_dtype(torch<span style="color:#f92672">.</span>float64)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MDN</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, in_dim, out_dim, hidden_dim, n_components):
        super()<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>n_components <span style="color:#f92672">=</span> n_components
        <span style="color:#75715e"># Last layer output dimension rationale:</span>
        <span style="color:#75715e"># Need two parameters for each distributionm thus 2 * n_components.</span>
        <span style="color:#75715e"># Need each of those for each output dimension, thus that multiplication</span>
        self<span style="color:#f92672">.</span>norm_network <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Linear(in_dim, hidden_dim),
            nn<span style="color:#f92672">.</span>ELU(),
            nn<span style="color:#f92672">.</span>Dropout(),
            nn<span style="color:#f92672">.</span>Linear(hidden_dim, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> n_components <span style="color:#f92672">*</span> out_dim)
        )
        self<span style="color:#f92672">.</span>cat_network <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            nn<span style="color:#f92672">.</span>Linear(in_dim, hidden_dim),
            nn<span style="color:#f92672">.</span>ELU(),
            nn<span style="color:#f92672">.</span>Dropout(),
            nn<span style="color:#f92672">.</span>Linear(hidden_dim, n_components <span style="color:#f92672">*</span> out_dim)
        )
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        norm_params <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm_network(x)
        <span style="color:#75715e"># Split so we get parameters for mean and standard deviation</span>
        mean, std <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>split(norm_params, norm_params<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        <span style="color:#75715e"># We need rightmost dimension to be n_components for mixture</span>
        mean <span style="color:#f92672">=</span> mean<span style="color:#f92672">.</span>view(mean<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>n_components)
        std <span style="color:#f92672">=</span> std<span style="color:#f92672">.</span>view(std<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>n_components)
        normal <span style="color:#f92672">=</span> Normal(mean, torch<span style="color:#f92672">.</span>exp(std))
        
        cat_params <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>cat_network(x)
        <span style="color:#75715e"># Again, rightmost dimension must be n_components</span>
        cat <span style="color:#f92672">=</span> Categorical(logits<span style="color:#f92672">=</span>cat_params<span style="color:#f92672">.</span>view(cat_params<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>n_components))
        
        <span style="color:#66d9ef">return</span> MixtureSameFamily(cat, normal)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">test_index <span style="color:#f92672">=</span> int(len(x) <span style="color:#f92672">*</span> <span style="color:#ae81ff">.75</span>)
train_x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(x<span style="color:#f92672">.</span>iloc[:test_index]<span style="color:#f92672">.</span>values)
train_y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(y<span style="color:#f92672">.</span>iloc[:test_index]<span style="color:#f92672">.</span>values)
test_x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(x<span style="color:#f92672">.</span>iloc[test_index:]<span style="color:#f92672">.</span>values)
test_y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(y<span style="color:#f92672">.</span>iloc[test_index:]<span style="color:#f92672">.</span>values)

in_dim <span style="color:#f92672">=</span> len(x<span style="color:#f92672">.</span>columns)
out_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
n_components <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">250</span>
</code></pre></div><p>Below here is the training loop. I&rsquo;m using a cosine annealing learning rate schedule to better explore the parameter space, as well as using model averaging over the last 500 iterations so the model generalizes better.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> MDN(in_dim, out_dim, hidden_dim, n_components)
optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">.001</span>)
scheduler <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>lr_scheduler<span style="color:#f92672">.</span>CosineAnnealingWarmRestarts(optimizer, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">2</span>)

swa_model <span style="color:#f92672">=</span> AveragedModel(model)
swa_start <span style="color:#f92672">=</span> <span style="color:#ae81ff">400</span>
swa_scheduler <span style="color:#f92672">=</span> SWALR(optimizer, swa_lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>, anneal_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, anneal_strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cos&#34;</span>)

train_losses <span style="color:#f92672">=</span> []
validation_losses <span style="color:#f92672">=</span> []
model<span style="color:#f92672">.</span>train()
swa_model<span style="color:#f92672">.</span>train()
<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">500</span>):
    optimizer<span style="color:#f92672">.</span>zero_grad()
    output <span style="color:#f92672">=</span> model(train_x)

    train_loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>output<span style="color:#f92672">.</span>log_prob(train_y<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>sum()
    train_losses<span style="color:#f92672">.</span>append(train_loss<span style="color:#f92672">.</span>detach())
    
    test_loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>model(test_x)<span style="color:#f92672">.</span>log_prob(test_y<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>sum()
    validation_losses<span style="color:#f92672">.</span>append(test_loss<span style="color:#f92672">.</span>detach())
    
    train_loss<span style="color:#f92672">.</span>backward()
    optimizer<span style="color:#f92672">.</span>step()
    
    <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">&gt;</span> swa_start:
        swa_model<span style="color:#f92672">.</span>update_parameters(model)
        swa_scheduler<span style="color:#f92672">.</span>step()
    <span style="color:#66d9ef">else</span>:
        scheduler<span style="color:#f92672">.</span>step()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>plot(train_losses)
plt<span style="color:#f92672">.</span>plot(validation_losses)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Training Epochs&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Model Loss&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Training &amp; Validation Losses&#34;</span>)
plt<span style="color:#f92672">.</span>legend([<span style="color:#e6db74">&#34;Training&#34;</span>, <span style="color:#e6db74">&#34;Validation&#34;</span>])
</code></pre></div><figure class="align-center ">
    <img loading="lazy" src="./output_17_1.png#center"/> 
</figure>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">swa_model<span style="color:#f92672">.</span>eval()
output_mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(np<span style="color:#f92672">.</span>exp(swa_model(test_x)<span style="color:#f92672">.</span>mean<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>squeeze()))
y_trans <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(np<span style="color:#f92672">.</span>exp(test_y<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>squeeze()))

output_sample <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(np<span style="color:#f92672">.</span>exp(swa_model(test_x)<span style="color:#f92672">.</span>sample([<span style="color:#ae81ff">5000</span>])<span style="color:#f92672">.</span>numpy()<span style="color:#f92672">.</span>squeeze()))
</code></pre></div><p>Our out-of-sample R-squared is excellent, much higher than my previous simple linear model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">regress <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>linregress(output_mean, y_trans)
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;R-squared: </span><span style="color:#e6db74">{</span>regress<span style="color:#f92672">.</span>rvalue<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">    R-squared: 0.7128714654332561
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>plot(output_mean)
plt<span style="color:#f92672">.</span>plot(y_trans)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Time&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Volatility&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Predicted and Actual Volatility&#34;</span>)
plt<span style="color:#f92672">.</span>legend([<span style="color:#e6db74">&#34;Model&#34;</span>, <span style="color:#e6db74">&#34;Actual&#34;</span>])
</code></pre></div><figure class="align-center ">
    <img loading="lazy" src="./output_21_1.png#center"/> 
</figure>

<p>Our distributional assumption also does well. We expect 5% of cases to be outside what the model distribution forecasts, and we find that to be the case.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">percent <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>percentile(output_sample, <span style="color:#ae81ff">95</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Number of exceedences: </span><span style="color:#e6db74">{</span>(y_trans <span style="color:#f92672">&gt;</span> percent)<span style="color:#f92672">.</span>sum() <span style="color:#f92672">/</span> len(y_trans)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">    Number of exceedences: 0.04477611940298507
</code></pre></div><p>Further testing the distribution accuracy, let&rsquo;s see if doing a probability integral transform yields a uniform.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ECDF</span>:
    <span style="color:#66d9ef">def</span> __init__(self, data):
        self<span style="color:#f92672">.</span>sorted <span style="color:#f92672">=</span> data
        self<span style="color:#f92672">.</span>sorted<span style="color:#f92672">.</span>sort()
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, len(self<span style="color:#f92672">.</span>sorted) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> len(self<span style="color:#f92672">.</span>sorted)
        
    <span style="color:#66d9ef">def</span> __call__(self, x):
        ind <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>searchsorted(self<span style="color:#f92672">.</span>sorted, x) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>y[ind]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">values <span style="color:#f92672">=</span> []

<span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(len(y_trans)):
    ecdf <span style="color:#f92672">=</span> ECDF(output_sample[x])
    values<span style="color:#f92672">.</span>append(ecdf(y_trans[x]))
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>hist(values, bins<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</code></pre></div><figure class="align-center ">
    <img loading="lazy" src="./output_27_1.png#center"/> 
</figure>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">stats<span style="color:#f92672">.</span>kstest(values, <span style="color:#e6db74">&#34;uniform&#34;</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">    KstestResult(statistic=0.028702640642939155, pvalue=0.46125545362008036)
</code></pre></div><p>We can&rsquo;t reject the null hypothesis that the transformed values come from a uniform distribution! That means our distributions accurately models the data&rsquo;s real distribution.</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>This model seems quite excellent. I&rsquo;m going to use this model for my future posts about how to make an effective trading strategy. Next time I&rsquo;m going to discuss Kelly Bet Sizing and its application to continuous distributions.</p>


  </div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://eadains.github.io/OptionallyBayesHugo/tags/volatility/">volatility</a></li>
      <li><a href="http://eadains.github.io/OptionallyBayesHugo/tags/forecasting/">forecasting</a></li>
      <li><a href="http://eadains.github.io/OptionallyBayesHugo/tags/mixture-density-network/">mixture-density-network</a></li>
      <li><a href="http://eadains.github.io/OptionallyBayesHugo/tags/machine-learning/">machine-learning</a></li>
      <li><a href="http://eadains.github.io/OptionallyBayesHugo/tags/pytorch/">pytorch</a></li>
      <li><a href="http://eadains.github.io/OptionallyBayesHugo/tags/model-building/">model-building</a></li>
    </ul>
  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="http://eadains.github.io/OptionallyBayesHugo/">Optionally Bayes</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>
